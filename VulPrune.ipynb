{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwe92DANPFrn"
      },
      "outputs": [],
      "source": [
        "!pip install typing groq openai -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJCu9Wj3AbSL"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/7MMA7/Vulprune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFAWioiGPqkB"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import threading\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from abc import ABC, abstractmethod\n",
        "from enum import Enum\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import difflib\n",
        "import subprocess\n",
        "import csv\n",
        "from groq import Groq\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umJSA9rcLetZ"
      },
      "outputs": [],
      "source": [
        "def get_20_random_deterministic_pair_indices() -> List[int]:\n",
        "    file_path = Path(\"/content/VulnerabilitiesDetectionProject/PrimeVul/primevul_filtered.jsonl\")\n",
        "    MAX_PAIRS_TO_LOAD = 20\n",
        "    RANDOM_SEED = 42\n",
        "\n",
        "    if not file_path.exists():\n",
        "        print(f\"Error: Data file not found at {file_path}\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            all_lines = f.readlines()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return []\n",
        "\n",
        "    total_lines = len(all_lines) - 2\n",
        "    total_pairs = total_lines // 2\n",
        "\n",
        "    all_pair_ranks = list(range(total_pairs))\n",
        "\n",
        "    random.seed(RANDOM_SEED)\n",
        "    random.shuffle(all_pair_ranks)\n",
        "\n",
        "    selected_indices = all_pair_ranks[:MAX_PAIRS_TO_LOAD]\n",
        "\n",
        "    print(f\"Total available pairs: {total_pairs}\")\n",
        "    print(f\"Number of pairs selected: {len(selected_indices)}\")\n",
        "    return all_pair_ranks, selected_indices\n",
        "\n",
        "all_pair_ranks, selected_pair_ranks = get_20_random_deterministic_pair_indices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMCo6mRNPuGU"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "class Config:\n",
        "    BASE_DIR = Path(\"/content/vulprune\")\n",
        "    DATA_DIR = BASE_DIR / \"data\"\n",
        "    RESULTS_DIR = BASE_DIR / \"results\"\n",
        "    CACHE_DIR = BASE_DIR / \"cache\"\n",
        "    CHECKPOINT_DIR = BASE_DIR / \"checkpoints\"\n",
        "\n",
        "    PRIMEVUL_DIR = BASE_DIR / \"primevul\"\n",
        "    PRIMEVUL_DATA_DIR = PRIMEVUL_DIR / \"data\"\n",
        "    PRIMEVUL_RESULTS_DIR = PRIMEVUL_DIR / \"results\"\n",
        "\n",
        "    CWE_EXAMPLES_PATH = Path(\"/content/VulnerabilitiesDetectionProject/cwe_example/cwe_commit_diff_vuln_examples.json\")\n",
        "\n",
        "    FLAWFINDER_DIR = BASE_DIR / \"flawfinder\" / \"reports\"\n",
        "    CPPCHECK_DIR = BASE_DIR / \"cppcheck\" / \"reports\"\n",
        "    SONARQUBE_DIR = BASE_DIR / \"sonarqube\" / \"reports\"\n",
        "\n",
        "    LLM_TEMPERATURE = 0.0\n",
        "    LLM_MAX_TOKENS = 4096\n",
        "\n",
        "    MAX_CONCURRENT_AGENTS = 10\n",
        "    ENABLE_CACHING = True\n",
        "    BATCH_SIZE = 10\n",
        "    CHECKPOINT_INTERVAL = 3\n",
        "\n",
        "    @classmethod\n",
        "    def setup_directories(cls):\n",
        "        for dir_path in [cls.BASE_DIR, cls.DATA_DIR, cls.RESULTS_DIR,\n",
        "                         cls.CACHE_DIR, cls.CHECKPOINT_DIR]:\n",
        "            dir_path.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--6ImAjEPxau"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATA STRUCTURES\n",
        "# =============================================================================\n",
        "\n",
        "@dataclass\n",
        "class CodeInput:\n",
        "    id: str\n",
        "    code: str\n",
        "    project: str\n",
        "    commit_id: Optional[str] = None\n",
        "    commit_message: Optional[str] = None\n",
        "    file_name: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class PairedCheckpoint:\n",
        "    last_pair_index: int\n",
        "    vuln_predictions: List[Dict[str, Any]]\n",
        "    clean_predictions: List[Dict[str, Any]]\n",
        "    pair_results: List[Dict[str, Any]]\n",
        "    timestamp: float\n",
        "    llm_stats: Dict[str, Any]\n",
        "    metadata: Dict[str, Any]\n",
        "\n",
        "@dataclass\n",
        "class GroundTruth:\n",
        "  id: str\n",
        "  cwe: str\n",
        "  label: int # target in primevul_filtered.jsonl\n",
        "  commit_id: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class PredictionResult:\n",
        "  id: str\n",
        "  is_vulnerable: bool\n",
        "  detected_cwes: List[str]\n",
        "  confidence: float\n",
        "  phases: Dict[str, Any]\n",
        "\n",
        "@dataclass\n",
        "class VulnerabilityReport:\n",
        "    cwe: str\n",
        "    code_lines: List[int]\n",
        "    description: str\n",
        "    source_agent: str\n",
        "    confidence: float = 0.0\n",
        "    evidence: List[str] = field(default_factory=list)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return asdict(self)\n",
        "\n",
        "@dataclass\n",
        "class VulnerabilityHypothesis:\n",
        "    cwe: str\n",
        "    code_lines: List[int]\n",
        "    description: str\n",
        "    source_agent: str\n",
        "    confidence: float\n",
        "    trigger_lines: List[int]\n",
        "    evidence: List[str]\n",
        "    assumptions: List[str] = field(default_factory=list)\n",
        "    trigger_path: str = \"\"\n",
        "\n",
        "    def __init__(self, cwe: str, code_lines: List[int], description: str,\n",
        "                 source_agent: str,\n",
        "                 confidence: float,\n",
        "                 trigger_lines: List[int],\n",
        "                 evidence: List[str],\n",
        "                 assumptions: List[str] = None,\n",
        "                 trigger_path: str = \"\"):\n",
        "\n",
        "        self.cwe = cwe\n",
        "        self.code_lines = code_lines\n",
        "        self.description = description\n",
        "        self.source_agent = source_agent\n",
        "        self.confidence = confidence\n",
        "        self.trigger_lines = trigger_lines\n",
        "        self.evidence = evidence if evidence is not None else []\n",
        "        self.assumptions = assumptions if assumptions is not None else []\n",
        "        self.trigger_path = trigger_path\n",
        "\n",
        "    def to_dict(self):\n",
        "        return asdict(self)\n",
        "\n",
        "@dataclass\n",
        "class ValidationResult:\n",
        "    hypothesis: VulnerabilityHypothesis\n",
        "    assumption_status: Dict[str, str]\n",
        "    path_valid: bool\n",
        "    reasoning: str\n",
        "    final_verdict: bool\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            \"hypothesis\": self.hypothesis.to_dict(),\n",
        "            \"assumption_status\": self.assumption_status,\n",
        "            \"path_valid\": self.path_valid,\n",
        "            \"reasoning\": self.reasoning,\n",
        "            \"final_verdict\": self.final_verdict\n",
        "        }\n",
        "\n",
        "@dataclass\n",
        "class ProgramContext:\n",
        "    cfg: Optional[Dict] = None\n",
        "    dfg: Optional[Dict] = None\n",
        "    call_graph: Optional[Dict] = None\n",
        "    functions: List[str] = field(default_factory=list)\n",
        "    imports: List[str] = field(default_factory=list)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return asdict(self)\n",
        "\n",
        "@dataclass\n",
        "class CWEExampleLoader:\n",
        "    @staticmethod\n",
        "    def load_examples(path: Path) -> Dict[str, str]:\n",
        "        if not path.exists():\n",
        "            print(f\"Warning: Examples file not found at {path}\")\n",
        "            return {}\n",
        "\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            formatted_examples = {}\n",
        "            for cwe, content in data.items():\n",
        "                diff_lines = content.get(\"diff\", [])\n",
        "                if isinstance(diff_lines, list):\n",
        "                    code_snippet = \"\\n\".join(diff_lines)\n",
        "                else:\n",
        "                    code_snippet = str(diff_lines)\n",
        "\n",
        "                formatted_examples[cwe] = (\n",
        "                    f\"Example of {cwe} ({content.get('cve', 'N/A')}):\\n\"\n",
        "                    f\"```match\\n{code_snippet}\\n```\"\n",
        "                )\n",
        "            return formatted_examples\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading CWE examples: {e}\")\n",
        "            return {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBX0WshK419J"
      },
      "outputs": [],
      "source": [
        "class PairedCheckpointManager:\n",
        "\n",
        "    def __init__(self, checkpoint_dir: Path = None):\n",
        "        self.checkpoint_dir = checkpoint_dir or Config.CHECKPOINT_DIR\n",
        "        self.checkpoint_path = self.checkpoint_dir / \"paired_evaluation_checkpoint.json\"\n",
        "\n",
        "    def save(self,\n",
        "             last_pair_index: int,\n",
        "             vuln_predictions: List[Dict],\n",
        "             clean_predictions: List[Dict],\n",
        "             pair_results: List[Dict],\n",
        "             llm_stats: Dict,\n",
        "             metadata: Dict = None):\n",
        "\n",
        "        checkpoint = {\n",
        "            \"last_pair_index\": last_pair_index,\n",
        "            \"vuln_predictions\": vuln_predictions,\n",
        "            \"clean_predictions\": clean_predictions,\n",
        "            \"pair_results\": pair_results,\n",
        "            \"timestamp\": time.time(),\n",
        "            \"llm_stats\": llm_stats,\n",
        "            \"metadata\": metadata or {}\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            with open(self.checkpoint_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(checkpoint, f, indent=2)\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to save checkpoint: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load(self) -> Optional[Dict]:\n",
        "        if not self.checkpoint_path.exists():\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            with open(self.checkpoint_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                return json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load checkpoint: {e}\")\n",
        "            return None\n",
        "\n",
        "    def exists(self) -> bool:\n",
        "        return self.checkpoint_path.exists()\n",
        "\n",
        "    def delete(self):\n",
        "        if self.checkpoint_path.exists():\n",
        "            try:\n",
        "                self.checkpoint_path.unlink()\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to delete checkpoint: {e}\")\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def get_info(self) -> Optional[Dict]:\n",
        "        if not self.checkpoint_path.exists():\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            with open(self.checkpoint_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            return {\n",
        "                \"last_pair_index\": data.get(\"last_pair_index\", 0),\n",
        "                \"num_vuln_predictions\": len(data.get(\"vuln_predictions\", [])),\n",
        "                \"num_clean_predictions\": len(data.get(\"clean_predictions\", [])),\n",
        "                \"num_pair_results\": len(data.get(\"pair_results\", [])),\n",
        "                \"timestamp\": data.get(\"timestamp\", 0),\n",
        "                \"metadata\": data.get(\"metadata\", {})\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to read checkpoint info: {e}\")\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McmruzWvP0Kx"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MULTI-MODELS CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "class APIConfig:\n",
        "  GROQ_API_KEY = \"YOUR_GROQ_API_KEY\"\n",
        "  OPENAI_API_KEY = \"YOUR_OPENAI_API_KEY\"\n",
        "  DEEPSEEK_API_KEY = \"YOUR_DEEPSEEK_API_KEY\"\n",
        "\n",
        "  OPENAI_BASE_URL = \"https://api.openai.com/v1\"\n",
        "  DEEPSEEK_BASE_URL = \"https://api.deepseek.com/v1\"\n",
        "\n",
        "  GROQ_MODELS = {\n",
        "        \"llama-3.3-70b\": {\n",
        "            \"name\": \"llama-3.3-70b-versatile\",\n",
        "            \"priority\": 1\n",
        "        },\n",
        "        \"llama-3.1-8b\": {\n",
        "            \"name\": \"llama-3.1-8b-instant\",\n",
        "            \"priority\": 2\n",
        "        }\n",
        "    }\n",
        "\n",
        "  OPENAI_MODELS = {\n",
        "      \"gpt-4o\": {\n",
        "          \"name\": \"gpt-4o\",\n",
        "          \"priority\": 1\n",
        "      }\n",
        "  }\n",
        "\n",
        "  DEEPSEEK_MODELS = {\n",
        "      \"deepseek-chat\": {\n",
        "          \"name\": \"deepseek-chat\",\n",
        "          \"priority\": 1\n",
        "      }\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjZ01EZair1z"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MULTI-PROVIDERS DATA STRUCTURE\n",
        "# =============================================================================\n",
        "\n",
        "class LLMProvider(Enum):\n",
        "    GROQ = \"groq\"\n",
        "    OPENAI = \"openai\"\n",
        "    DEEPSEEK = \"deepseek\"\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    name: str\n",
        "    provider: LLMProvider\n",
        "    priority: int\n",
        "    enabled: bool = True\n",
        "    api_key: str = \"\"\n",
        "    base_url: Optional[str] = None\n",
        "    call_count: int = 0\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"name\": self.name,\n",
        "            \"provider\": self.provider.value,\n",
        "            \"priority\": self.priority,\n",
        "            \"enabled\": self.enabled,\n",
        "            \"call_count\": self.call_count\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nC_ioLDgi5Tn"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PROVIDERS ABSTRACT INTERFACE\n",
        "# =============================================================================\n",
        "\n",
        "class BaseLLMProvider(ABC):\n",
        "\n",
        "    def __init__(self, api_key: str, models: List[ModelConfig], auto_rotate_on_error: bool = False):\n",
        "        self.api_key = api_key\n",
        "        self.available_models = models\n",
        "        self.current_model_index = 0\n",
        "        self.total_calls = 0\n",
        "        self.failed_calls = 0\n",
        "        self.auto_rotate_on_error = auto_rotate_on_error  # True only for Groq\n",
        "        self.model_stats = {m.name: {\"calls\": 0, \"errors\": 0} for m in models}\n",
        "        self.cache = {}\n",
        "\n",
        "    @abstractmethod\n",
        "    def _make_api_call(self, model: ModelConfig, messages: List[Dict],\n",
        "                       temperature: float, max_tokens: int,\n",
        "                       response_format: str) -> str:\n",
        "        pass\n",
        "\n",
        "    def _get_next_model(self) -> ModelConfig:\n",
        "\n",
        "        if self.auto_rotate_on_error:\n",
        "            sorted_models = sorted(\n",
        "                [m for m in self.available_models if m.enabled],\n",
        "                key=lambda m: (m.call_count, m.priority)\n",
        "            )\n",
        "            if sorted_models:\n",
        "                return sorted_models[0]\n",
        "\n",
        "        enabled_models = [m for m in self.available_models if m.enabled]\n",
        "        if enabled_models:\n",
        "            return min(enabled_models, key=lambda m: m.priority)\n",
        "\n",
        "        return self.available_models[0]\n",
        "\n",
        "    def call(self, prompt: str, temperature: float = 0.0,\n",
        "             max_tokens: int = 4096, response_format: str = \"json\",\n",
        "             max_retries: int = 5) -> str:\n",
        "\n",
        "        cache_key = f\"{hash(prompt)}_{temperature}\"\n",
        "        if Config.ENABLE_CACHING and cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        self.total_calls += 1\n",
        "        retries = 0\n",
        "        last_error = None\n",
        "\n",
        "        while retries < max_retries:\n",
        "            current_model = self._get_next_model()\n",
        "\n",
        "            try:\n",
        "                messages = [\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"You are an expert security researcher specialized in software vulnerability detection. Always respond with valid JSON when requested.\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": prompt\n",
        "                    }\n",
        "                ]\n",
        "\n",
        "                if response_format == \"json\" and \"Return JSON:\" not in prompt and \"Return STRICT JSON:\" not in prompt:\n",
        "                    messages[-1][\"content\"] += \"\\n\\nIMPORTANT: Respond with valid JSON only.\"\n",
        "\n",
        "                result = self._make_api_call(\n",
        "                    current_model, messages, temperature, max_tokens, response_format\n",
        "                )\n",
        "\n",
        "                current_model.call_count += 1\n",
        "                self.model_stats[current_model.name][\"calls\"] += 1\n",
        "\n",
        "                if Config.ENABLE_CACHING:\n",
        "                    self.cache[cache_key] = result\n",
        "\n",
        "                return result\n",
        "\n",
        "            except Exception as e:\n",
        "                error_str = str(e)\n",
        "                last_error = e\n",
        "                self.failed_calls += 1\n",
        "                self.model_stats[current_model.name][\"errors\"] += 1\n",
        "\n",
        "                if self.auto_rotate_on_error and (\"rate_limit\" in error_str.lower() or \"429\" in error_str):\n",
        "                    print(f\"{current_model.name} rate limited, rotating to next model...\")\n",
        "                    current_model.enabled = False\n",
        "\n",
        "                    def reactivate():\n",
        "                        time.sleep(60)\n",
        "                        current_model.enabled = True\n",
        "                        print(f\"{current_model.name} reactivated\")\n",
        "\n",
        "                    threading.Thread(target=reactivate, daemon=True).start()\n",
        "\n",
        "                    if all(not m.enabled for m in self.available_models):\n",
        "                        print(f\"Waiting 60 seconds...\")\n",
        "                        time.sleep(60)\n",
        "                        for m in self.available_models:\n",
        "                            m.enabled = True\n",
        "\n",
        "                    retries += 1\n",
        "                    continue\n",
        "\n",
        "                print(f\"Error with {current_model.name}: {error_str[:100]}\")\n",
        "                retries += 1\n",
        "                time.sleep(2)\n",
        "\n",
        "        print(f\"All retries exhausted. Last error: {last_error}\")\n",
        "        return json.dumps({\n",
        "            \"error\": \"LLM call failed after all retries\",\n",
        "            \"details\": str(last_error)\n",
        "        })\n",
        "\n",
        "    def get_stats(self) -> dict:\n",
        "        return {\n",
        "            \"provider\": self.__class__.__name__,\n",
        "            \"total_calls\": self.total_calls,\n",
        "            \"failed_calls\": self.failed_calls,\n",
        "            \"success_rate\": (self.total_calls - self.failed_calls) / self.total_calls if self.total_calls > 0 else 0,\n",
        "            \"cached_entries\": len(self.cache),\n",
        "            \"model_stats\": self.model_stats,\n",
        "            \"current_model_usage\": {\n",
        "                m.name: {\n",
        "                    \"call_count\": m.call_count,\n",
        "                    \"enabled\": m.enabled\n",
        "                }\n",
        "                for m in self.available_models\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def reset_stats(self):\n",
        "        for model in self.available_models:\n",
        "            model.call_count = 0\n",
        "            model.enabled = True\n",
        "        print(f\"Stats reset for all {self.__class__.__name__} models\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfyDB3ZqjiKf"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# GROQ IMPLEMENTATION\n",
        "# =============================================================================\n",
        "\n",
        "class GroqProvider(BaseLLMProvider):\n",
        "\n",
        "    def __init__(self, api_key: str, model_names: Optional[List[str]] = None):\n",
        "        models = []\n",
        "        for key, config in APIConfig.GROQ_MODELS.items():\n",
        "            if model_names is None or key in model_names:\n",
        "                models.append(ModelConfig(\n",
        "                    name=config[\"name\"],\n",
        "                    provider=LLMProvider.GROQ,\n",
        "                    priority=config[\"priority\"],\n",
        "                    api_key=api_key\n",
        "                ))\n",
        "\n",
        "        super().__init__(api_key, models, auto_rotate_on_error=True)\n",
        "        self.client = Groq(api_key=api_key)\n",
        "\n",
        "        print(f\"Groq Provider initialized with {len(models)} models (auto-rotation enabled)\")\n",
        "        for model in models:\n",
        "            print(f\"   {model.name} (Priority: {model.priority})\")\n",
        "\n",
        "    def _make_api_call(self, model: ModelConfig, messages: List[Dict],\n",
        "                       temperature: float, max_tokens: int,\n",
        "                       response_format: str) -> str:\n",
        "\n",
        "        if response_format == \"json\":\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=model.name,\n",
        "                messages=messages,\n",
        "                temperature=temperature,\n",
        "                max_tokens=max_tokens,\n",
        "                response_format={\"type\": \"json_object\"}\n",
        "            )\n",
        "        else:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=model.name,\n",
        "                messages=messages,\n",
        "                temperature=temperature,\n",
        "                max_tokens=max_tokens\n",
        "            )\n",
        "\n",
        "        return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhJU_TRLjyyl"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# OPENAI (GPT-4o) IMPLEMENTATION\n",
        "# =============================================================================\n",
        "\n",
        "class OpenAIProvider(BaseLLMProvider):\n",
        "\n",
        "    def __init__(self, api_key: str, model_names: Optional[List[str]] = None):\n",
        "        models = []\n",
        "        for key, config in APIConfig.OPENAI_MODELS.items():\n",
        "            if model_names is None or key in model_names:\n",
        "                models.append(ModelConfig(\n",
        "                    name=config[\"name\"],\n",
        "                    provider=LLMProvider.OPENAI,\n",
        "                    priority=config[\"priority\"],\n",
        "                    api_key=api_key,\n",
        "                    base_url=APIConfig.OPENAI_BASE_URL\n",
        "                ))\n",
        "\n",
        "        super().__init__(api_key, models, auto_rotate_on_error=False)\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "\n",
        "        print(f\"OpenAI Provider initialized with {len(models)} model(s) (sufficient quotas)\")\n",
        "        for model in models:\n",
        "            print(f\"{model.name} (Priority: {model.priority})\")\n",
        "\n",
        "    def _make_api_call(self, model: ModelConfig, messages: List[Dict],\n",
        "                       temperature: float, max_tokens: int,\n",
        "                       response_format: str) -> str:\n",
        "\n",
        "        call_params = {\n",
        "            \"model\": model.name,\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": temperature,\n",
        "            \"max_tokens\": max_tokens\n",
        "        }\n",
        "\n",
        "        if response_format == \"json\":\n",
        "            call_params[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "        response = self.client.chat.completions.create(**call_params)\n",
        "\n",
        "        return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-3o-audkFXq"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DEEPSEEK IMPLEMENTATION\n",
        "# =============================================================================\n",
        "\n",
        "# Deepseek uses a compatible API with OpenAI\n",
        "\n",
        "class DeepSeekProvider(BaseLLMProvider):\n",
        "\n",
        "    def __init__(self, api_key: str, model_names: Optional[List[str]] = None):\n",
        "        models = []\n",
        "        for key, config in APIConfig.DEEPSEEK_MODELS.items():\n",
        "            if model_names is None or key in model_names:\n",
        "                models.append(ModelConfig(\n",
        "                    name=config[\"name\"],\n",
        "                    provider=LLMProvider.DEEPSEEK,\n",
        "                    priority=config[\"priority\"],\n",
        "                    api_key=api_key,\n",
        "                    base_url=APIConfig.DEEPSEEK_BASE_URL\n",
        "                ))\n",
        "\n",
        "        super().__init__(api_key, models, auto_rotate_on_error=False)\n",
        "        self.client = OpenAI(\n",
        "            api_key=api_key,\n",
        "            base_url=APIConfig.DEEPSEEK_BASE_URL\n",
        "        )\n",
        "\n",
        "        print(f\"DeepSeek Provider initialized with {len(models)} model(s) (sufficient quotas)\")\n",
        "        for model in models:\n",
        "            print(f\"   • {model.name} (Priority: {model.priority})\")\n",
        "\n",
        "    def _make_api_call(self, model: ModelConfig, messages: List[Dict],\n",
        "                       temperature: float, max_tokens: int,\n",
        "                       response_format: str) -> str:\n",
        "\n",
        "        call_params = {\n",
        "            \"model\": model.name,\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": temperature,\n",
        "            \"max_tokens\": max_tokens\n",
        "        }\n",
        "\n",
        "        if response_format == \"json\":\n",
        "            call_params[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "        response = self.client.chat.completions.create(**call_params)\n",
        "\n",
        "        return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R92jtOR3lynz"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# UNIFIED MULTI-PROVIDER MANAGER\n",
        "# =============================================================================\n",
        "\n",
        "class UnifiedLLMInterface:\n",
        "\n",
        "    def __init__(self, providers: List[BaseLLMProvider],\n",
        "                 rotation_strategy: str = \"priority\"):\n",
        "\n",
        "        self.providers = providers\n",
        "        self.rotation_strategy = rotation_strategy\n",
        "        self.current_provider_index = 0\n",
        "        self.total_calls = 0\n",
        "        self.failed_calls = 0\n",
        "        self.provider_stats = {\n",
        "            p.__class__.__name__: {\"calls\": 0, \"errors\": 0, \"success_rate\": 0.0}\n",
        "            for p in providers\n",
        "        }\n",
        "\n",
        "        print(f\"\\nUnified LLM Interface initialized\")\n",
        "        print(f\"   Strategy: {rotation_strategy}\")\n",
        "        print(f\"   Providers: {len(providers)}\")\n",
        "        for p in providers:\n",
        "            print(f\"      {p.__class__.__name__} ({len(p.available_models)} model(s))\")\n",
        "\n",
        "    def _get_next_provider(self) -> BaseLLMProvider:\n",
        "\n",
        "        if self.rotation_strategy == \"round_robin\":\n",
        "            provider = self.providers[self.current_provider_index]\n",
        "            self.current_provider_index = (self.current_provider_index + 1) % len(self.providers)\n",
        "            return provider\n",
        "\n",
        "        elif self.rotation_strategy == \"least_used\":\n",
        "            return min(self.providers,\n",
        "                      key=lambda p: self.provider_stats[p.__class__.__name__][\"calls\"])\n",
        "\n",
        "        elif self.rotation_strategy == \"priority\":\n",
        "            for provider in sorted(self.providers,\n",
        "                                  key=lambda p: min(m.priority for m in p.available_models\n",
        "                                                   if m.enabled)):\n",
        "                if any(m.enabled for m in provider.available_models):\n",
        "                    return provider\n",
        "            return self.providers[0]\n",
        "\n",
        "        return self.providers[0]\n",
        "\n",
        "    def call(self, prompt: str, temperature: float = 0.0,\n",
        "             max_tokens: int = 4096, response_format: str = \"json\",\n",
        "             max_retries: int = 3, preferred_provider: Optional[str] = None) -> str:\n",
        "\n",
        "        self.total_calls += 1\n",
        "\n",
        "        if preferred_provider:\n",
        "            for provider in self.providers:\n",
        "                if provider.__class__.__name__.lower().startswith(preferred_provider.lower()):\n",
        "                    try:\n",
        "                        result = provider.call(prompt, temperature, max_tokens,\n",
        "                                             response_format, max_retries)\n",
        "\n",
        "                        if \"error\" not in result:\n",
        "                            self.provider_stats[provider.__class__.__name__][\"calls\"] += 1\n",
        "                            return result\n",
        "                    except Exception as e:\n",
        "                        print(f\"Preferred provider {preferred_provider} failed: {e}\")\n",
        "                        self.provider_stats[provider.__class__.__name__][\"errors\"] += 1\n",
        "\n",
        "        providers_to_try = self.providers.copy()\n",
        "\n",
        "        for provider in providers_to_try:\n",
        "            try:\n",
        "                result = provider.call(prompt, temperature, max_tokens,\n",
        "                                     response_format, max_retries)\n",
        "\n",
        "                if \"error\" not in result:\n",
        "                    provider_name = provider.__class__.__name__\n",
        "                    self.provider_stats[provider_name][\"calls\"] += 1\n",
        "\n",
        "                    total = self.provider_stats[provider_name][\"calls\"]\n",
        "                    errors = self.provider_stats[provider_name][\"errors\"]\n",
        "                    self.provider_stats[provider_name][\"success_rate\"] = \\\n",
        "                        (total - errors) / total if total > 0 else 0\n",
        "\n",
        "                    return result\n",
        "\n",
        "            except Exception as e:\n",
        "                provider_name = provider.__class__.__name__\n",
        "                self.provider_stats[provider_name][\"errors\"] += 1\n",
        "                print(f\"{provider_name} failed, trying next provider...\")\n",
        "                continue\n",
        "\n",
        "        self.failed_calls += 1\n",
        "        return json.dumps({\n",
        "            \"error\": \"All providers failed\",\n",
        "            \"details\": \"No provider could complete the request\"\n",
        "        })\n",
        "\n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "\n",
        "        stats = {\n",
        "            \"total_calls\": self.total_calls,\n",
        "            \"failed_calls\": self.failed_calls,\n",
        "            \"success_rate\": (self.total_calls - self.failed_calls) / self.total_calls\n",
        "                           if self.total_calls > 0 else 0,\n",
        "            \"providers\": {}\n",
        "        }\n",
        "\n",
        "        for provider in self.providers:\n",
        "            provider_name = provider.__class__.__name__\n",
        "            provider_stats = provider.get_stats()\n",
        "\n",
        "            stats[\"providers\"][provider_name] = {\n",
        "                \"calls\": self.provider_stats[provider_name][\"calls\"],\n",
        "                \"errors\": self.provider_stats[provider_name][\"errors\"],\n",
        "                \"success_rate\": self.provider_stats[provider_name][\"success_rate\"],\n",
        "                \"models\": provider_stats[\"current_model_usage\"],\n",
        "                \"cache_size\": provider_stats[\"cached_entries\"]\n",
        "            }\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def reset_stats(self):\n",
        "        for provider in self.providers:\n",
        "            provider.reset_stats()\n",
        "\n",
        "        for key in self.provider_stats:\n",
        "            self.provider_stats[key] = {\"calls\": 0, \"errors\": 0, \"success_rate\": 0.0}\n",
        "\n",
        "        self.total_calls = 0\n",
        "        self.failed_calls = 0\n",
        "\n",
        "        print(\"All providers stats reset\")\n",
        "\n",
        "    def print_stats(self):\n",
        "        stats = self.get_stats()\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"UNIFIED LLM INTERFACE STATISTICS\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Total Calls: {stats['total_calls']}\")\n",
        "        print(f\"Failed Calls: {stats['failed_calls']}\")\n",
        "        print(f\"Overall Success Rate: {stats['success_rate']*100:.2f}%\")\n",
        "        print(f\"\\n{'Provider Stats':-^70}\")\n",
        "\n",
        "        for provider_name, provider_stats in stats[\"providers\"].items():\n",
        "            print(f\"\\n{provider_name}:\")\n",
        "            print(f\"  Calls: {provider_stats['calls']}\")\n",
        "            print(f\"  Errors: {provider_stats['errors']}\")\n",
        "            print(f\"  Success Rate: {provider_stats['success_rate']*100:.2f}%\")\n",
        "            print(f\"  Cache Size: {provider_stats['cache_size']}\")\n",
        "            print(f\"  Models:\")\n",
        "\n",
        "            for model_name, model_stats in provider_stats[\"models\"].items():\n",
        "                print(f\"    {model_name}: {model_stats['call_count']} calls - \"\n",
        "                      f\"{'✅' if model_stats['enabled'] else '❌'}\")\n",
        "\n",
        "        print(f\"\\n{'='*70}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW2F4E1gP8zV"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PREPROCESSING\n",
        "# =============================================================================\n",
        "\n",
        "class CodePreprocessor:\n",
        "\n",
        "    @staticmethod\n",
        "    def add_line_numbers(code: str) -> str:\n",
        "        lines = code.split('\\n')\n",
        "        return '\\n'.join([f\"L{i+1}: {line}\" for i, line in enumerate(lines)])\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_line_numbers(code: str) -> str:\n",
        "        lines = code.split('\\n')\n",
        "        return '\\n'.join([re.sub(r'^L\\d+:\\s*', '', line) for line in lines])\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_line_numbers(text: str) -> List[int]:\n",
        "        matches = re.findall(r'L(\\d+)', text)\n",
        "        return sorted(list(set([int(m) for m in matches])))\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_code(code: str) -> str:\n",
        "        code = re.sub(r'\\n\\s*\\n', '\\n\\n', code)\n",
        "        code = '\\n'.join([line.rstrip() for line in code.split('\\n')])\n",
        "        return code.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqSHEvTPQC3s"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BASE AGENT CLASS\n",
        "# =============================================================================\n",
        "\n",
        "class BaseAgent(ABC):\n",
        "\n",
        "    def __init__(self, llm: UnifiedLLMInterface, name: str):\n",
        "        self.llm = llm\n",
        "        self.name = name\n",
        "        self.execution_count = 0\n",
        "\n",
        "    @abstractmethod\n",
        "    def analyze(self, code: str) -> Any:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3T1PVhfQHKg"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PHASE 1: MULTI-VIEW DETECTION AGENTS\n",
        "# =============================================================================\n",
        "\n",
        "class MetaAgent(BaseAgent):\n",
        "\n",
        "    BASELINE_AGENTS = [\"StaticAnalyzerAgent\", \"BehaviorAnalyzerAgent\", \"MemoryLayoutAgent\"]\n",
        "\n",
        "    def analyze(self, code: str) -> List[str]:\n",
        "\n",
        "        prompt = f\"\"\"You are MetaAgent. Select specialized agents for maximum vulnerability recall.\n",
        "        Code:\n",
        "        {code}\n",
        "\n",
        "        Available Agents:\n",
        "        - StaticAnalyzerAgent: Pattern-based static analysis (ALWAYS ACTIVE)\n",
        "        - BehaviorAnalyzerAgent: Control/data flow reasoning (ALWAYS ACTIVE)\n",
        "        - MemoryLayoutAgent: Memory safety analysis (ALWAYS ACTIVE)\n",
        "        - FormatStringAgent: Format string vulnerabilities\n",
        "        - FilePermissionAgent: File I/O and permissions\n",
        "        - AuthFlowAgent: Authentication/authorization\n",
        "        - CryptoConfigAgent: Cryptographic misuse\n",
        "        - ConcurrencyAnalyzerAgent: Race conditions\n",
        "        - ErrorHandlingAgent: Error handling issues\n",
        "        - CodeInjectionAgent: Command/code injection\n",
        "\n",
        "        Return JSON: {{\"activated_agents\": [\"agent1\", \"agent2\", ...]}}\n",
        "        Include baseline agents + relevant specialized agents.\"\"\"\n",
        "\n",
        "        response = self.llm.call(prompt)\n",
        "        try:\n",
        "            data = json.loads(response)\n",
        "            agents = data.get(\"activated_agents\", [])\n",
        "            return list(set(agents + self.BASELINE_AGENTS))\n",
        "        except:\n",
        "            return self.BASELINE_AGENTS\n",
        "\n",
        "class SpecializedAgent(BaseAgent):\n",
        "\n",
        "    def __init__(self, llm: UnifiedLLMInterface, name: str, cwe_focus: List[str],\n",
        "                 triggers: List[str], description: str, cwe_examples: Dict[str, str] = None):\n",
        "        super().__init__(llm, name)\n",
        "        self.cwe_focus = cwe_focus\n",
        "        self.triggers = triggers\n",
        "        self.description = description\n",
        "        self.cwe_examples = cwe_examples or {}\n",
        "\n",
        "    def analyze(self, code: str) -> List[VulnerabilityReport]:\n",
        "\n",
        "        examples_text = \"\"\n",
        "        relevant_examples = []\n",
        "        for cwe in self.cwe_focus:\n",
        "          if cwe in self.cwe_examples:\n",
        "            relevant_examples.append(self.cwe_examples[cwe])\n",
        "\n",
        "        if relevant_examples:\n",
        "          examples_text = \"\\nHere are REAL WORLD EXAMPLES of what you are looking for:\\n\" + \"\\n\".join(relevant_examples) + \"\\n\"\n",
        "\n",
        "        prompt = f\"\"\"You are {self.name}. {self.description}\n",
        "                  Analyze for: {', '.join(self.cwe_focus)}\n",
        "                  Typical triggers: {', '.join(self.triggers)}\n",
        "\n",
        "                  {examples_text}\n",
        "\n",
        "                  MAXIMIZE RECALL - flag anything suspicious from your perspective.\n",
        "\n",
        "                  Code:\n",
        "                  {code}\n",
        "\n",
        "                  Return STRICT JSON:\n",
        "                  {{\n",
        "                    \"vulnerability_reported\": true/false,\n",
        "                    \"findings\": [\n",
        "                      {{\n",
        "                        \"cwe\": \"CWE-XXX\",\n",
        "                        \"code_lines\": \"L10,L15\",\n",
        "                        \"description\": \"Specific issue found\",\n",
        "                        \"evidence\": [\"evidence snippet 1\", \"evidence snippet 2\"],\n",
        "                        \"confidence\": 0.7\n",
        "                      }}\n",
        "                    ]\n",
        "                  }}\"\"\"\n",
        "\n",
        "        self.execution_count += 1\n",
        "        response = self.llm.call(prompt)\n",
        "\n",
        "        try:\n",
        "            data = json.loads(response)\n",
        "            reports = []\n",
        "\n",
        "            for finding in data.get(\"findings\", []):\n",
        "                reports.append(VulnerabilityReport(\n",
        "                    cwe=finding.get(\"cwe\", \"UNKNOWN\"),\n",
        "                    code_lines=CodePreprocessor.extract_line_numbers(finding.get(\"code_lines\", \"\")),\n",
        "                    description=finding.get(\"description\", \"\"),\n",
        "                    source_agent=self.name,\n",
        "                    confidence=finding.get(\"confidence\", 0.5),\n",
        "                    evidence=finding.get(\"evidence\", [])\n",
        "                ))\n",
        "\n",
        "            return reports\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"{self.name} failed: {e}\")\n",
        "            return []\n",
        "\n",
        "class StaticToolAgent(BaseAgent):\n",
        "\n",
        "    def __init__(self, llm: UnifiedLLMInterface, name: str, report_dir: Path):\n",
        "        super().__init__(llm, name)\n",
        "        self.report_dir = report_dir\n",
        "\n",
        "    def analyze(self, code_input: CodeInput) -> List[VulnerabilityReport]:\n",
        "\n",
        "        if self.name == \"Sonarqube\":\n",
        "            try:\n",
        "                pair_rank = code_input.metadata.get(\"pair_index\")\n",
        "                if pair_rank is None:\n",
        "                    return []\n",
        "\n",
        "                report_path = self.report_dir / f\"analysis-results_{pair_rank}\" / \"filtered_results.jsonl\"\n",
        "\n",
        "            except Exception:\n",
        "                return []\n",
        "\n",
        "            if not report_path.exists():\n",
        "                return []\n",
        "\n",
        "            try:\n",
        "                with open(report_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "                    content = f.read().strip()\n",
        "\n",
        "                if not content:\n",
        "                    return []\n",
        "\n",
        "                report = VulnerabilityReport(\n",
        "                    cwe=f\"External-Tool-{self.name}\",\n",
        "                    code_lines=[],\n",
        "                    description=f\"Raw static analysis output from {self.name} for pair index {pair_rank}.\",\n",
        "                    source_agent=self.name,\n",
        "                    confidence=0.9,\n",
        "                    evidence=[content]\n",
        "                )\n",
        "                return [report]\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Erreur de lecture du rapport Sonarqube : {e}\")\n",
        "                return []\n",
        "\n",
        "        else:\n",
        "            try:\n",
        "                item_idx = code_input.id.split('_')[-1]\n",
        "                report_path = self.report_dir / f\"{item_idx}_report.txt\"\n",
        "            except Exception:\n",
        "                return []\n",
        "\n",
        "            if not report_path.exists():\n",
        "                return []\n",
        "\n",
        "            try:\n",
        "                with open(report_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "                    content = f.read().strip()\n",
        "\n",
        "                if not content:\n",
        "                    return []\n",
        "\n",
        "                report = VulnerabilityReport(\n",
        "                    cwe=f\"External-Tool-{self.name}\",\n",
        "                    code_lines=[],\n",
        "                    description=f\"Raw static analysis output from {self.name}.\",\n",
        "                    source_agent=self.name,\n",
        "                    confidence=0.9,\n",
        "                    evidence=[content]\n",
        "                )\n",
        "                return [report]\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading report for {self.name}: {e}\")\n",
        "                return []\n",
        "\n",
        "class AggregatorAgent(BaseAgent):\n",
        "\n",
        "    def analyze(self, reports: List[VulnerabilityReport]) -> List[VulnerabilityReport]:\n",
        "\n",
        "        if not reports:\n",
        "            return []\n",
        "\n",
        "        grouped: Dict[str, List[VulnerabilityReport]] = {}\n",
        "        for report in reports:\n",
        "            if report.cwe not in grouped:\n",
        "                grouped[report.cwe] = []\n",
        "            grouped[report.cwe].append(report)\n",
        "\n",
        "        aggregated = []\n",
        "        for cwe, cwe_reports in grouped.items():\n",
        "            all_lines: Set[int] = set()\n",
        "            descriptions = []\n",
        "            agents = []\n",
        "            evidence = []\n",
        "            max_conf = 0.0\n",
        "\n",
        "            for r in cwe_reports:\n",
        "                all_lines.update(r.code_lines)\n",
        "                descriptions.append(r.description)\n",
        "                agents.append(r.source_agent)\n",
        "                evidence.extend(r.evidence)\n",
        "                max_conf = max(max_conf, r.confidence)\n",
        "\n",
        "            aggregated.append(VulnerabilityReport(\n",
        "                cwe=cwe,\n",
        "                code_lines=sorted(list(all_lines)),\n",
        "                description=\" | \".join(set(descriptions)),\n",
        "                source_agent=\", \".join(set(agents)),\n",
        "                confidence=max_conf,\n",
        "                evidence=list(set(evidence))\n",
        "            ))\n",
        "\n",
        "        return aggregated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1Ag3NwvQJvm"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PHASE 2: HYPOTHESIS CONSTRUCTION\n",
        "# =============================================================================\n",
        "\n",
        "class TriggerPlannerAgent(BaseAgent):\n",
        "\n",
        "    def analyze(self, reports: List[VulnerabilityReport], code: str) -> List[VulnerabilityHypothesis]:\n",
        "        hypotheses = []\n",
        "\n",
        "        print(f\"   [DEBUG] TriggerPlanner received {len(reports)} reports\")\n",
        "\n",
        "        for idx, report in enumerate(reports):\n",
        "            print(f\"   [DEBUG] Processing report {idx+1}/{len(reports)}: {report.cwe}\")\n",
        "\n",
        "            prompt = f\"\"\"You are TriggerPlannerAgent. Build a FEASIBLE trigger path for this vulnerability.\n",
        "            DO NOT judge validity - that comes later. Just construct the path.\n",
        "\n",
        "            Report:\n",
        "            - CWE: {report.cwe}\n",
        "            - Lines: {report.code_lines}\n",
        "            - Description: {report.description}\n",
        "            - Source: {report.source_agent}\n",
        "            - Evidence: {', '.join(report.evidence[:2]) if hasattr(report, 'evidence') and report.evidence else 'N/A'}\n",
        "\n",
        "            Code:\n",
        "            {code}\n",
        "\n",
        "            Construct:\n",
        "            1. Assumptions: What must be true for exploitation? (List specific, verifiable assumptions)\n",
        "            2. Trigger Path: attacker-source → transforms → sink\n",
        "            3. Trigger Lines: Specific lines where the vulnerability can be triggered\n",
        "\n",
        "            Return STRICT JSON:\n",
        "            {{\n",
        "              \"cwe\": \"{report.cwe}\",\n",
        "              \"assumptions\": [\"A1: Input comes from untrusted source\", \"A2: No bounds checking\"],\n",
        "              \"trigger_path\": \"user_input → strcpy → buffer overflow at L15\",\n",
        "              \"trigger_lines\": [10, 15],\n",
        "              \"is_valid\": true\n",
        "            }}\n",
        "\n",
        "            Only set is_valid to false if the vulnerability is clearly impossible.\"\"\"\n",
        "\n",
        "            response = self.llm.call(prompt)\n",
        "\n",
        "            try:\n",
        "                data = json.loads(response)\n",
        "                print(f\"   [DEBUG] TriggerPlanner parsed JSON for {report.cwe}\")\n",
        "                print(f\"   [DEBUG] is_valid={data.get('is_valid', True)}, assumptions={len(data.get('assumptions', []))}\")\n",
        "\n",
        "                if data.get(\"is_valid\", True):\n",
        "                    hypothesis = VulnerabilityHypothesis(\n",
        "                        cwe=report.cwe,\n",
        "                        code_lines=report.code_lines,\n",
        "                        description=report.description,\n",
        "                        source_agent=report.source_agent,\n",
        "                        confidence=report.confidence,\n",
        "                        trigger_lines=data.get(\"trigger_lines\", report.code_lines),\n",
        "                        evidence=report.evidence if hasattr(report, 'evidence') else []\n",
        "                    )\n",
        "\n",
        "                    hypothesis.assumptions = data.get(\"assumptions\", [])\n",
        "                    hypothesis.trigger_path = data.get(\"trigger_path\", \"Unknown path\")\n",
        "\n",
        "                    print(f\"   [SUCCESS] Created hypothesis with {len(hypothesis.assumptions)} assumptions\")\n",
        "                    hypotheses.append(hypothesis)\n",
        "                else:\n",
        "                    print(f\"   [SKIP] Hypothesis marked as invalid by LLM\")\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"   [ERROR] TriggerPlanner JSON parsing failed for {report.cwe}: {e}\")\n",
        "                print(f\"   [ERROR] Response: {response[:300]}\")\n",
        "                hypothesis = VulnerabilityHypothesis(\n",
        "                    cwe=report.cwe,\n",
        "                    code_lines=report.code_lines,\n",
        "                    description=report.description,\n",
        "                    source_agent=report.source_agent,\n",
        "                    confidence=report.confidence,\n",
        "                    trigger_lines=report.code_lines,\n",
        "                    evidence=report.evidence if hasattr(report, 'evidence') else []\n",
        "                )\n",
        "                hypothesis.assumptions = [\"Default: Requires manual review\"]\n",
        "                hypothesis.trigger_path = \"Unknown - parsing error\"\n",
        "                print(f\"   [FALLBACK] Created minimal hypothesis due to error\")\n",
        "                hypotheses.append(hypothesis)\n",
        "\n",
        "        print(f\"   [SUMMARY] TriggerPlanner created {len(hypotheses)} hypotheses total\")\n",
        "        return hypotheses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "er8rB3VYQMz3"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PHASE 3: ASSUMPTION VALIDATION\n",
        "# =============================================================================\n",
        "\n",
        "class AssumptionPrunerAgent(BaseAgent):\n",
        "\n",
        "    def __init__(self, llm: UnifiedLLMInterface, name: str):\n",
        "        super().__init__(llm, name)\n",
        "\n",
        "    def _format_hypotheses(self, hypotheses: List[VulnerabilityHypothesis]) -> str:\n",
        "        formatted = []\n",
        "        for i, h in enumerate(hypotheses):\n",
        "            assumptions_str = \"\\n    \".join(h.assumptions) if h.assumptions else \"None specified\"\n",
        "            formatted.append(\n",
        "                f\"Hypothesis {i+1} [SOURCE: {h.source_agent}]:\\n\"\n",
        "                f\"  CWE: {h.cwe}\\n\"\n",
        "                f\"  Lines: {h.code_lines}\\n\"\n",
        "                f\"  Trigger Lines: {h.trigger_lines}\\n\"\n",
        "                f\"  Trigger Path: {h.trigger_path}\\n\"\n",
        "                f\"  Assumptions:\\n    {assumptions_str}\\n\"\n",
        "                f\"  Description: {h.description}\\n\"\n",
        "                f\"  Confidence: {h.confidence:.2f}\\n\"\n",
        "                f\"  Evidence: {', '.join(h.evidence[:2]) if h.evidence else 'N/A'}\\n\"\n",
        "                f\"---\"\n",
        "            )\n",
        "        return \"\\n\".join(formatted)\n",
        "\n",
        "    def analyze(self, hypotheses: List[VulnerabilityHypothesis], code: str) -> List[Tuple[VulnerabilityHypothesis, Dict[str, str]]]:\n",
        "\n",
        "        if not hypotheses:\n",
        "            print(\"   [DEBUG] No hypotheses to validate\")\n",
        "            return []\n",
        "\n",
        "        print(f\"   [DEBUG] AssumptionPruner received {len(hypotheses)} hypotheses\")\n",
        "\n",
        "        hypotheses_with_assumptions = [h for h in hypotheses if h.assumptions]\n",
        "        print(f\"   [DEBUG] {len(hypotheses_with_assumptions)} hypotheses have assumptions\")\n",
        "\n",
        "        formatted_hypotheses = self._format_hypotheses(hypotheses)\n",
        "        print(f\"   [DEBUG] Formatted hypotheses length: {len(formatted_hypotheses)} chars\")\n",
        "\n",
        "        prompt = f\"\"\"You are the **Assumption Pruner Agent ({self.name})**, a senior security engineer reviewing vulnerability hypotheses.\n",
        "\n",
        "**YOUR TASK:**\n",
        "For each hypothesis, validate its assumptions and assess overall confidence.\n",
        "\n",
        "**EVALUATION FACTORS:**\n",
        "\n",
        "**For each assumption, determine:**\n",
        "- \"VALID\" - The assumption is clearly true in the code\n",
        "- \"LIKELY\" - The assumption is probably true but not certain\n",
        "- \"UNCERTAIN\" - Cannot determine from the code\n",
        "- \"INVALID\" - The assumption is clearly false\n",
        "\n",
        "**Overall confidence scoring:**\n",
        "- Increase confidence if:\n",
        "  * Well-established vulnerability pattern\n",
        "  * Known-unsafe functions used\n",
        "  * Untrusted input sources\n",
        "  * Missing protection mechanisms\n",
        "  * Clear attack path\n",
        "\n",
        "- Decrease confidence if:\n",
        "  * Non-existent code references\n",
        "  * Flagged lines are non-executable\n",
        "  * Multiple protection layers present\n",
        "  * Improbable preconditions required\n",
        "  * Code handles the edge case\n",
        "  * Input is validated/sanitized\n",
        "\n",
        "**Hypotheses to Review:**\n",
        "{formatted_hypotheses}\n",
        "\n",
        "**Code Under Analysis:**\n",
        "{code}\n",
        "\n",
        "**OUTPUT FORMAT (JSON list):**\n",
        "For each hypothesis, return:\n",
        "[\n",
        "  {{\n",
        "    \"hypothesis_id\": 0,\n",
        "    \"cwe\": \"CWE-XXX\",\n",
        "    \"assumption_validations\": {{\n",
        "      \"A1: Input comes from untrusted source\": \"VALID\",\n",
        "      \"A2: No bounds checking\": \"LIKELY\"\n",
        "    }},\n",
        "    \"overall_confidence\": 0.8,\n",
        "    \"assessment\": \"Brief explanation of your assessment\",\n",
        "    \"should_proceed\": true\n",
        "  }}\n",
        "]\n",
        "\n",
        "Only set should_proceed to true if overall_confidence > 0.5.\"\"\"\n",
        "\n",
        "        self.execution_count += 1\n",
        "        response = self.llm.call(prompt)\n",
        "\n",
        "        print(f\"   [DEBUG] LLM response length: {len(response)} chars\")\n",
        "        print(f\"   [DEBUG] Response preview: {response[:200]}...\")\n",
        "\n",
        "        try:\n",
        "            data = json.loads(response.strip())\n",
        "            print(f\"   [DEBUG] Parsed JSON successfully, type: {type(data)}\")\n",
        "\n",
        "            if not isinstance(data, list):\n",
        "                print(f\"   [ERROR] Pruner did not return a list. Response: {response[:500]}\")\n",
        "                return []\n",
        "\n",
        "            print(f\"   [DEBUG] JSON contains {len(data)} entries\")\n",
        "\n",
        "            validated_tuples = []\n",
        "\n",
        "            for i, finding in enumerate(data):\n",
        "                print(f\"   [DEBUG] Processing finding {i+1}: confidence={finding.get('overall_confidence', 0.0)}, should_proceed={finding.get('should_proceed', False)}\")\n",
        "\n",
        "                if finding.get(\"should_proceed\", False) and finding.get(\"overall_confidence\", 0.0) > 0.5:\n",
        "                    hypothesis_id = finding.get(\"hypothesis_id\", i)\n",
        "\n",
        "                    if hypothesis_id < len(hypotheses):\n",
        "                        original_hypothesis = hypotheses[hypothesis_id]\n",
        "\n",
        "                        updated_hypothesis = VulnerabilityHypothesis(\n",
        "                            cwe=original_hypothesis.cwe,\n",
        "                            code_lines=original_hypothesis.code_lines,\n",
        "                            description=original_hypothesis.description,\n",
        "                            source_agent=original_hypothesis.source_agent,\n",
        "                            confidence=finding.get(\"overall_confidence\", original_hypothesis.confidence),\n",
        "                            trigger_lines=original_hypothesis.trigger_lines,\n",
        "                            evidence=original_hypothesis.evidence,\n",
        "                            assumptions=original_hypothesis.assumptions,\n",
        "                            trigger_path=original_hypothesis.trigger_path\n",
        "                        )\n",
        "\n",
        "                        assumption_status = finding.get(\"assumption_validations\", {})\n",
        "\n",
        "                        validated_tuples.append((updated_hypothesis, assumption_status))\n",
        "                        print(f\"   [DEBUG] Added hypothesis {hypothesis_id} to validated list\")\n",
        "                    else:\n",
        "                        print(f\"   [DEBUG] Skipping - invalid hypothesis_id: {hypothesis_id} (max: {len(hypotheses)-1})\")\n",
        "                else:\n",
        "                    print(f\"   [DEBUG] Skipping finding {i} - should_proceed={finding.get('should_proceed', False)}, confidence={finding.get('overall_confidence', 0.0)}\")\n",
        "\n",
        "            print(f\"   [SUCCESS] Pruner validated {len(validated_tuples)}/{len(hypotheses)} hypotheses\")\n",
        "            return validated_tuples\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"   [ERROR] {self.name} JSON parsing failed: {e}\")\n",
        "            print(f\"   [ERROR] Response was: {response[:500]}\")\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            print(f\"   [ERROR] {self.name} failed: {e}\")\n",
        "            print(f\"   [ERROR] Response: {response[:500]}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqiUkK-CQPQL"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PHASE 4: PATH VERIFICATION\n",
        "# =============================================================================\n",
        "\n",
        "class FinalValidatorAgent(BaseAgent):\n",
        "\n",
        "    def analyze(self, validated_hypotheses: List[Tuple[VulnerabilityHypothesis, Dict[str, str]]],\n",
        "                code: str) -> List[ValidationResult]:\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for hyp, assumption_status in validated_hypotheses:\n",
        "            assumptions_str = \"\\n\".join([\n",
        "                f\"    - {assumption}: {status}\"\n",
        "                for assumption, status in assumption_status.items()\n",
        "            ])\n",
        "\n",
        "            prompt = f\"\"\"You are FinalValidatorAgent. Binary decision: Is this vulnerability EXPLOITABLE?\n",
        "\n",
        "            Hypothesis:\n",
        "            - CWE: {hyp.cwe}\n",
        "            - Code Lines: {hyp.code_lines}\n",
        "            - Trigger Lines: {hyp.trigger_lines}\n",
        "            - Trigger Path: {hyp.trigger_path}\n",
        "            - Description: {hyp.description}\n",
        "            - Current Confidence: {hyp.confidence}\n",
        "\n",
        "            Validated Assumptions:\n",
        "{assumptions_str}\n",
        "\n",
        "            Evidence: {', '.join(hyp.evidence[:3]) if hyp.evidence else 'N/A'}\n",
        "\n",
        "            Code:\n",
        "            {code}\n",
        "\n",
        "            Check for protections that COMPLETELY BLOCK exploitation:\n",
        "            - Bounds checks before sink\n",
        "            - Null checks\n",
        "            - Early returns / error handling\n",
        "            - Input sanitization\n",
        "            - Protective API behavior\n",
        "\n",
        "            IMPORTANT:\n",
        "            - Path is INVALID only if protections COMPLETELY BLOCK ALL routes\n",
        "            - If assumptions are mostly VALID/LIKELY, lean towards exploitable\n",
        "            - When uncertain, RETAIN the vulnerability (false positive is better than false negative)\n",
        "\n",
        "            Return STRICT JSON:\n",
        "            {{\n",
        "              \"path_valid\": true/false,\n",
        "              \"reasoning\": \"detailed explanation of why path is/isn't exploitable\",\n",
        "              \"protective_mechanisms\": [\"protection1\", \"protection2\"],\n",
        "              \"blocks_all_paths\": false,\n",
        "              \"final_verdict\": true/false\n",
        "            }}\n",
        "\n",
        "            final_verdict should be true if the vulnerability is exploitable.\"\"\"\n",
        "\n",
        "            response = self.llm.call(prompt)\n",
        "\n",
        "            try:\n",
        "                data = json.loads(response)\n",
        "\n",
        "                results.append(ValidationResult(\n",
        "                    hypothesis=hyp,\n",
        "                    assumption_status=assumption_status,\n",
        "                    path_valid=data.get(\"path_valid\", True),\n",
        "                    reasoning=data.get(\"reasoning\", \"\"),\n",
        "                    final_verdict=data.get(\"final_verdict\", True)\n",
        "                ))\n",
        "            except Exception as e:\n",
        "                print(f\"FinalValidator failed: {e}\")\n",
        "                results.append(ValidationResult(\n",
        "                    hypothesis=hyp,\n",
        "                    assumption_status=assumption_status,\n",
        "                    path_valid=True,\n",
        "                    reasoning=f\"Validation error: {e} - Keeping vulnerability by default\",\n",
        "                    final_verdict=True\n",
        "                ))\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIQp3QRpQUGQ"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPLETE PIPELINE WITH CHECKPOINTS\n",
        "# =============================================================================\n",
        "\n",
        "class VulPrunePipeline:\n",
        "\n",
        "    def __init__(self, llm: UnifiedLLMInterface):\n",
        "        self.llm = llm\n",
        "\n",
        "        self.cwe_examples = CWEExampleLoader.load_examples(Config.CWE_EXAMPLES_PATH)\n",
        "        print(f\"Pipeline loaded {len(self.cwe_examples)} CWE few-shot examples\")\n",
        "\n",
        "        self.meta_agent = MetaAgent(llm, \"MetaAgent\")\n",
        "        self.aggregator = AggregatorAgent(llm, \"AggregatorAgent\")\n",
        "        self.trigger_planner = TriggerPlannerAgent(llm, \"TriggerPlannerAgent\")\n",
        "        self.assumption_pruner = AssumptionPrunerAgent(llm, \"AssumptionPrunerAgent\")\n",
        "        self.final_validator = FinalValidatorAgent(llm, \"FinalValidatorAgent\")\n",
        "\n",
        "        self.specialized_agents = self._init_specialized_agents()\n",
        "\n",
        "        self.static_agents = [\n",
        "            StaticToolAgent(llm, \"Flawfinder\", Config.FLAWFINDER_DIR),\n",
        "            StaticToolAgent(llm, \"Cppcheck\", Config.CPPCHECK_DIR),\n",
        "            StaticToolAgent(llm, \"Sonarqube\", Config.SONARQUBE_DIR)\n",
        "        ]\n",
        "\n",
        "    def _init_specialized_agents(self) -> Dict[str, SpecializedAgent]:\n",
        "        return {\n",
        "            \"StaticAnalyzerAgent\": SpecializedAgent(\n",
        "                self.llm, \"StaticAnalyzerAgent\",\n",
        "                [\"CWE-476\", \"CWE-787\", \"CWE-125\"],\n",
        "                [\"NULL\", \"buffer\", \"pointer\", \"array\"],\n",
        "                \"Static pattern recognition for common vulnerabilities\",\n",
        "                self.cwe_examples\n",
        "            ),\n",
        "            \"BehaviorAnalyzerAgent\": SpecializedAgent(\n",
        "                self.llm, \"BehaviorAnalyzerAgent\",\n",
        "                [\"CWE-703\", \"CWE-416\", \"CWE-20\"],\n",
        "                [\"leak\", \"use-after-free\", \"error\", \"validation\"],\n",
        "                \"Control/data flow analysis for runtime issues\",\n",
        "                self.cwe_examples\n",
        "            ),\n",
        "            \"MemoryLayoutAgent\": SpecializedAgent(\n",
        "                self.llm, \"MemoryLayoutAgent\",\n",
        "                [\"CWE-787\", \"CWE-125\", \"CWE-190\"],\n",
        "                [\"memcpy\", \"strcpy\", \"buffer\", \"overflow\", \"underflow\"],\n",
        "                \"Memory operations and buffer safety\",\n",
        "                self.cwe_examples\n",
        "            ),\n",
        "            \"FormatStringAgent\": SpecializedAgent(\n",
        "                self.llm, \"FormatStringAgent\",\n",
        "                [\"CWE-134\"],\n",
        "                [\"printf\", \"sprintf\", \"fprintf\", \"format\", \"%s\", \"%d\"],\n",
        "                \"Format string vulnerability detection\",\n",
        "                self.cwe_examples\n",
        "            ),\n",
        "            \"FilePermissionAgent\": SpecializedAgent(\n",
        "                self.llm, \"FilePermissionAgent\",\n",
        "                [\"CWE-022\"],\n",
        "                [\"fopen\", \"chmod\", \"access\", \"file\", \"path\"],\n",
        "                \"File I/O and permission issues\",\n",
        "                self.cwe_examples\n",
        "            ),\n",
        "            \"AuthFlowAgent\": SpecializedAgent(\n",
        "                self.llm, \"AuthFlowAgent\",\n",
        "                [\"CWE-862\", \"CWE-863\"],\n",
        "                [\"auth\", \"permission\", \"access\", \"login\", \"session\", \"role\"],\n",
        "                \"Authentication and authorization flow analysis\",\n",
        "                self.cwe_examples\n",
        "            ),\n",
        "            \"CryptoConfigAgent\": SpecializedAgent(\n",
        "                self.llm, \"CryptoConfigAgent\",\n",
        "                [\"CWE-327\", \"CWE-338\"],\n",
        "                [\"crypto\", \"encrypt\", \"decrypt\", \"hash\", \"random\", \"key\"],\n",
        "                \"Cryptographic misuse and weak configurations\",\n",
        "                self.cwe_examples\n",
        "            ),\n",
        "            \"ConcurrencyAnalyzerAgent\": SpecializedAgent(\n",
        "                self.llm, \"ConcurrencyAnalyzerAgent\",\n",
        "                [\"CWE-362\", \"CWE-366\"],\n",
        "                [\"thread\", \"mutex\", \"lock\", \"atomic\", \"race\", \"concurrent\"],\n",
        "                \"Race conditions and synchronization issues\",\n",
        "                self.cwe_examples\n",
        "            ),\n",
        "            \"ErrorHandlingAgent\": SpecializedAgent(\n",
        "                self.llm, \"ErrorHandlingAgent\",\n",
        "                [\"CWE-703\", \"CWE-404\"],\n",
        "                [\"try\", \"catch\", \"error\", \"exception\", \"return\", \"cleanup\"],\n",
        "                \"Error handling and resource management\",\n",
        "                self.cwe_examples\n",
        "            ),\n",
        "            \"CodeInjectionAgent\": SpecializedAgent(\n",
        "                self.llm, \"CodeInjectionAgent\",\n",
        "                [\"CWE-078\", \"CWE-089\"],\n",
        "                [\"eval\", \"exec\", \"system\", \"shell\", \"command\", \"sql\"],\n",
        "                \"Command and code injection vulnerabilities\",\n",
        "                self.cwe_examples\n",
        "            )\n",
        "        }\n",
        "\n",
        "    def detect(self, code_input: CodeInput) -> PredictionResult:\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Analyzing: {code_input.id}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        numbered_code = CodePreprocessor.add_line_numbers(code_input.code)\n",
        "\n",
        "        # PHASE 1: Multi-View Detection\n",
        "        print(\"\\nPhase 1: Multi-View Detection\")\n",
        "        activated_agents = self.meta_agent.analyze(numbered_code)\n",
        "\n",
        "        llm_reports = []\n",
        "        static_tool_reports = []\n",
        "\n",
        "        print(\"   -> Running Static Tools (Flawfinder, Cppcheck, Sonarqube)...\")\n",
        "        for agent in self.static_agents:\n",
        "            reports = agent.analyze(code_input)\n",
        "            if reports:\n",
        "                print(f\"      {agent.name} found report.\")\n",
        "                static_tool_reports.extend(reports)\n",
        "\n",
        "        for agent_name in activated_agents:\n",
        "            if agent_name in self.specialized_agents:\n",
        "                agent = self.specialized_agents[agent_name]\n",
        "                reports = agent.analyze(numbered_code)\n",
        "                llm_reports.extend(reports)\n",
        "\n",
        "        aggregated_llm_reports = self.aggregator.analyze(llm_reports)\n",
        "\n",
        "        phase1_results = {\n",
        "            \"activated_agents\": activated_agents,\n",
        "            \"raw_reports\": len(aggregated_llm_reports),\n",
        "            \"aggregated_reports\": [r.to_dict() for r in aggregated_llm_reports],\n",
        "            \"static_tool_reports\": len(static_tool_reports)\n",
        "        }\n",
        "\n",
        "        all_reports_for_planner = aggregated_llm_reports + static_tool_reports\n",
        "\n",
        "        if not all_reports_for_planner:\n",
        "            print(\"   No vulnerabilities detected in Phase 1\")\n",
        "            return PredictionResult(\n",
        "                id=code_input.id,\n",
        "                is_vulnerable=False,\n",
        "                detected_cwes=[],\n",
        "                confidence=0.0,\n",
        "                phases={\"phase1\": phase1_results}\n",
        "            )\n",
        "\n",
        "        # PHASE 2: Hypothesis Construction\n",
        "        print(\"\\nPhase 2: Hypothesis Construction\")\n",
        "        print(f\"   Input: {len(all_reports_for_planner)} reports\")\n",
        "        hypotheses = self.trigger_planner.analyze(all_reports_for_planner, numbered_code)\n",
        "        print(f\"   Output: {len(hypotheses)} hypotheses\")\n",
        "\n",
        "        if not hypotheses:\n",
        "            print(\"   No valid hypotheses constructed\")\n",
        "            return PredictionResult(\n",
        "                id=code_input.id,\n",
        "                is_vulnerable=False,\n",
        "                detected_cwes=[],\n",
        "                confidence=0.0,\n",
        "                phases={\n",
        "                    \"phase1\": phase1_results,\n",
        "                    \"phase2\": {\"hypotheses_count\": 0}\n",
        "                }\n",
        "            )\n",
        "\n",
        "        print(f\"   Constructed {len(hypotheses)} hypotheses\")\n",
        "\n",
        "        # PHASE 3: Assumption Validation\n",
        "        print(\"\\nPhase 3: Assumption Validation\")\n",
        "        print(f\"   Input: {len(hypotheses)} hypotheses\")\n",
        "        validated_tuples = self.assumption_pruner.analyze(hypotheses, numbered_code)\n",
        "        print(f\"   Output: {len(validated_tuples)} validated tuples\")\n",
        "\n",
        "        if not validated_tuples:\n",
        "            print(\"   No hypotheses passed assumption validation\")\n",
        "            return PredictionResult(\n",
        "                id=code_input.id,\n",
        "                is_vulnerable=False,\n",
        "                detected_cwes=[],\n",
        "                confidence=0.0,\n",
        "                phases={\n",
        "                    \"phase1\": phase1_results,\n",
        "                    \"phase2\": {\"hypotheses\": [h.to_dict() for h in hypotheses]},\n",
        "                    \"phase3\": {\"validated_count\": 0}\n",
        "                }\n",
        "            )\n",
        "\n",
        "        # PHASE 4: Final Validation\n",
        "        print(\"\\nPhase 4: Final Validation\")\n",
        "        print(f\"   Input: {len(validated_tuples)} tuples\")\n",
        "        validations = self.final_validator.analyze(validated_tuples, numbered_code)\n",
        "        print(f\"   Output: {len(validations)} validation results\")\n",
        "\n",
        "        # Final decision\n",
        "        exploitable = [v for v in validations if v.final_verdict]\n",
        "        detected_cwes = list({v.hypothesis.cwe for v in exploitable})\n",
        "\n",
        "        print(f\"   Final verdict: {len(exploitable)}/{len(validations)} exploitable vulnerabilities\")\n",
        "\n",
        "        return PredictionResult(\n",
        "            id=code_input.id,\n",
        "            is_vulnerable=len(exploitable) > 0,\n",
        "            detected_cwes=detected_cwes,\n",
        "            confidence=max([v.hypothesis.confidence for v in exploitable], default=0.0),\n",
        "            phases={\n",
        "                \"phase1\": phase1_results,\n",
        "                \"phase2\": {\n",
        "                    \"hypotheses_count\": len(hypotheses),\n",
        "                    \"hypotheses\": [h.to_dict() for h in hypotheses]\n",
        "                },\n",
        "                \"phase3\": {\n",
        "                    \"validated_count\": len(validated_tuples),\n",
        "                    \"validated_hypotheses\": [\n",
        "                        {\n",
        "                            \"hypothesis\": hyp.to_dict(),\n",
        "                            \"assumption_status\": status\n",
        "                        }\n",
        "                        for hyp, status in validated_tuples\n",
        "                    ]\n",
        "                },\n",
        "                \"phase4\": {\n",
        "                    \"total_validations\": len(validations),\n",
        "                    \"exploitable_count\": len(exploitable),\n",
        "                    \"validations\": [v.to_dict() for v in validations]\n",
        "                }\n",
        "            })\n",
        "\n",
        "    def _save_checkpoint(self, results: List[Dict], last_index: int, checkpoint_path: Path):\n",
        "\n",
        "        checkpoint = {\n",
        "            \"results\": results,\n",
        "            \"last_index\": last_index,\n",
        "            \"timestamp\": time.time(),\n",
        "            \"llm_stats\": self.llm.get_stats()\n",
        "        }\n",
        "        try:\n",
        "            with open(checkpoint_path, \"w\") as f:\n",
        "                json.dump(checkpoint, f, indent=2)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to save checkpoint: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ss_o4HvtQSZa"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATASET LOADERS\n",
        "# =============================================================================\n",
        "\n",
        "class DatasetManager:\n",
        "\n",
        "    def load_primevul_pairs(max_pairs: Optional[int] = None, random_sample: Any = True) -> List[Dict]:\n",
        "\n",
        "        base_dir = Path(\"/content/VulnerabilitiesDetectionProject/PrimeVul\")\n",
        "        paired_path = base_dir / \"primevul_filtered.jsonl\"\n",
        "\n",
        "        if not paired_path.exists():\n",
        "            raise FileNotFoundError(f\"{paired_path} not found. Git clone first.\")\n",
        "\n",
        "        print(f\"Loading from {paired_path.name}\")\n",
        "\n",
        "        data = []\n",
        "        with open(paired_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                try:\n",
        "                    data.append(json.loads(line))\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "\n",
        "        print(f\"Loaded {len(data)} total entries\")\n",
        "\n",
        "        groups = defaultdict(list)\n",
        "        for o in data:\n",
        "            groups[o.get(\"commit_id\")].append(o)\n",
        "\n",
        "        pairs = []\n",
        "        count_vuln_alone = 0\n",
        "        count_clean_alone = 0\n",
        "\n",
        "        for cid, items in groups.items():\n",
        "            vuln = next((x for x in items if x.get(\"target\") == 1), None)\n",
        "            fix = next((x for x in items if x.get(\"target\") == 0), None)\n",
        "            if vuln and fix:\n",
        "                pairs.append((vuln, fix))\n",
        "            elif vuln:\n",
        "                count_vuln_alone += 1\n",
        "            elif fix:\n",
        "                count_clean_alone += 1\n",
        "\n",
        "        print(f\"Found {len(pairs)} valid vuln/fix pairs\")\n",
        "        print(f\"    {count_vuln_alone} vuln alone | {count_clean_alone} fix alone\")\n",
        "\n",
        "        if isinstance(random_sample, int):\n",
        "            selected_pairs = [pairs[random_sample - 1]]\n",
        "        elif isinstance(random_sample, (list, tuple)):\n",
        "            selected_pairs = [pairs[i] for i in random_sample]\n",
        "        else:\n",
        "            if random_sample and len(pairs) > (max_pairs or 10):\n",
        "                random.shuffle(pairs)\n",
        "                selected_pairs = pairs[:max_pairs or 10]\n",
        "            else:\n",
        "                selected_pairs = pairs[:max_pairs] if max_pairs else pairs\n",
        "\n",
        "        clean_samples = []\n",
        "        vuln_samples = []\n",
        "        results = []\n",
        "\n",
        "        for i, (v, f) in enumerate(selected_pairs, 1):\n",
        "            func_v = v.get(\"func\", \"\").splitlines()\n",
        "            func_f = f.get(\"func\", \"\").splitlines()\n",
        "            seq = difflib.SequenceMatcher(None, \"\\n\".join(func_v), \"\\n\".join(func_f))\n",
        "            ratio = seq.ratio()\n",
        "\n",
        "            results.append({\n",
        "                \"pair_id\": i,\n",
        "                \"commit_id\": v.get(\"commit_id\"),\n",
        "                \"idx_vuln\": v.get(\"idx\"),\n",
        "                \"idx_fix\": f.get(\"idx\"),\n",
        "                \"ratio\": ratio\n",
        "            })\n",
        "\n",
        "            base_entry_v = {\n",
        "                \"idx\": v.get(\"idx\"),\n",
        "                \"id\": v.get(\"commit_id\"),\n",
        "                \"project\": v.get(\"project\", \"unknown\"),\n",
        "                \"dataset\": \"PrimeVul\",\n",
        "                \"cwe\": v.get(\"cwe\", []),\n",
        "                \"cve\": v.get(\"cve\", \"None\"),\n",
        "                \"cve_desc\": v.get(\"cve_desc\", \"N/A\")\n",
        "            }\n",
        "\n",
        "            base_entry_f = base_entry_v.copy()\n",
        "            base_entry_f[\"idx\"] = f.get(\"idx\")\n",
        "\n",
        "            vuln_samples.append({\n",
        "                **base_entry_v,\n",
        "                \"code\": v.get(\"func\", \"\"),\n",
        "                \"target\": 1\n",
        "            })\n",
        "\n",
        "            clean_samples.append({\n",
        "                **base_entry_f,\n",
        "                \"code\": f.get(\"func\", \"\"),\n",
        "                \"target\": 0\n",
        "            })\n",
        "\n",
        "        print(f\"Selected {len(vuln_samples)} vuln & {len(clean_samples)} clean samples\")\n",
        "        return clean_samples, vuln_samples, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9R7LnNDQcVS"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MAIN EVALUATION FUNCTION\n",
        "# =============================================================================\n",
        "\n",
        "def evaluate_primevul_paired_with_checkpoints(\n",
        "    pipeline: VulPrunePipeline,\n",
        "    num_pairs: int = 10,\n",
        "    random_sample: bool = True,\n",
        "    checkpoint_interval: int = 3\n",
        "):\n",
        "\n",
        "    print(\"\\n\" + \"=\"*69)\n",
        "    print(\"VULPRUNE PAIRED EVALUATION (WITH CHECKPOINT SUPPORT)\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    checkpoint_manager = PairedCheckpointManager()\n",
        "\n",
        "    checkpoint_data = checkpoint_manager.load()\n",
        "\n",
        "    if checkpoint_data:\n",
        "        print(\"Checkpoint found! Loading previous progress...\")\n",
        "        last_pair_index = checkpoint_data.get(\"last_pair_index\", -1)\n",
        "        vuln_predictions = checkpoint_data.get(\"vuln_predictions\", [])\n",
        "        clean_predictions = checkpoint_data.get(\"clean_predictions\", [])\n",
        "        pair_results = checkpoint_data.get(\"pair_results\", [])\n",
        "\n",
        "        print(f\"   Resuming from pair {last_pair_index + 2}/{num_pairs}\")\n",
        "        print(f\"   Already processed: {len(vuln_predictions)} vulnerable samples\")\n",
        "        print(f\"   Already processed: {len(clean_predictions)} clean samples\")\n",
        "        print(f\"   Pair results so far: {len(pair_results)}\\n\")\n",
        "\n",
        "        start_pair_index = last_pair_index + 1\n",
        "    else:\n",
        "        print(\"No checkpoint found. Starting fresh evaluation.\\n\")\n",
        "        vuln_predictions = []\n",
        "        clean_predictions = []\n",
        "        pair_results = []\n",
        "        start_pair_index = 0\n",
        "    try:\n",
        "        clean_samples, vuln_samples, pair_stats = DatasetManager.load_primevul_pairs(\n",
        "            max_pairs=num_pairs,\n",
        "            random_sample=random_sample\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load data: {e}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Loaded {len(vuln_samples)} vulnerable + {len(clean_samples)} fixed samples\\n\")\n",
        "\n",
        "    vuln_inputs = [\n",
        "        CodeInput(\n",
        "            id=f\"vuln_{v['idx']}\",\n",
        "            code=v['code'],\n",
        "            project=v.get('project')\n",
        "        )\n",
        "        for v in vuln_samples\n",
        "    ]\n",
        "\n",
        "    clean_inputs = [\n",
        "        CodeInput(\n",
        "            id=f\"clean_{c['idx']}\",\n",
        "            code=c['code'],\n",
        "            project=c.get('project')\n",
        "        )\n",
        "        for c in clean_samples\n",
        "    ]\n",
        "\n",
        "    vuln_truths = [\n",
        "        GroundTruth(\n",
        "            id=f\"vuln_{v['idx']}\",\n",
        "            cwe=str(v.get('cwe', ['UNKNOWN'])[0] if isinstance(v.get('cwe'), list) else v.get('cwe', 'UNKNOWN')),\n",
        "            label=1,\n",
        "            commit_id=v.get('id')\n",
        "        )\n",
        "        for v in vuln_samples\n",
        "    ]\n",
        "\n",
        "    clean_truths = [\n",
        "        GroundTruth(\n",
        "            id=f\"clean_{c['idx']}\",\n",
        "            cwe=str(c.get('cwe', ['UNKNOWN'])[0] if isinstance(c.get('cwe'), list) else c.get('cwe', 'UNKNOWN')),\n",
        "            label=0,\n",
        "            commit_id=c.get('id')\n",
        "        )\n",
        "        for c in clean_samples\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        for i in range(start_pair_index, num_pairs):\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(f\"Processing PAIR {i+1}/{num_pairs}\")\n",
        "            print(f\"{'='*70}\\n\")\n",
        "\n",
        "            print(f\"[VULN] Analyzing vulnerable sample {vuln_inputs[i].id}...\")\n",
        "            try:\n",
        "                vuln_pred = pipeline.detect(vuln_inputs[i])\n",
        "                vuln_predictions.append({\n",
        "                    \"id\": vuln_pred.id,\n",
        "                    \"is_vulnerable\": vuln_pred.is_vulnerable,\n",
        "                    \"detected_cwes\": vuln_pred.detected_cwes,\n",
        "                    \"confidence\": vuln_pred.confidence,\n",
        "                    \"phases\": vuln_pred.phases if hasattr(vuln_pred, 'phases') else {}\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"  Error: {e}\")\n",
        "                vuln_predictions.append({\n",
        "                    \"id\": vuln_inputs[i].id,\n",
        "                    \"error\": str(e)\n",
        "                })\n",
        "\n",
        "            print(f\"\\n[CLEAN] Analyzing clean sample {clean_inputs[i].id}...\")\n",
        "            try:\n",
        "                clean_pred = pipeline.detect(clean_inputs[i])\n",
        "                clean_predictions.append({\n",
        "                    \"id\": clean_pred.id,\n",
        "                    \"is_vulnerable\": clean_pred.is_vulnerable,\n",
        "                    \"detected_cwes\": clean_pred.detected_cwes,\n",
        "                    \"confidence\": clean_pred.confidence,\n",
        "                    \"phases\": clean_pred.phases if hasattr(clean_pred, 'phases') else {}\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"  Error: {e}\")\n",
        "                clean_predictions.append({\n",
        "                    \"id\": clean_inputs[i].id,\n",
        "                    \"error\": str(e)\n",
        "                })\n",
        "\n",
        "            if \"error\" not in vuln_predictions[-1] and \"error\" not in clean_predictions[-1]:\n",
        "                vuln_pred_dict = vuln_predictions[-1]\n",
        "                clean_pred_dict = clean_predictions[-1]\n",
        "                vuln_truth = vuln_truths[i]\n",
        "                clean_truth = clean_truths[i]\n",
        "\n",
        "                cwe_matched = False\n",
        "                if vuln_truth.cwe and vuln_truth.cwe != 'UNKNOWN':\n",
        "                    cwe_matched = any(vuln_truth.cwe in str(dc) for dc in vuln_pred_dict['detected_cwes'])\n",
        "\n",
        "                vuln_correct = vuln_pred_dict['is_vulnerable'] == True\n",
        "                clean_correct = clean_pred_dict['is_vulnerable'] == False\n",
        "\n",
        "                is_pair_correct = vuln_correct and clean_correct\n",
        "                is_pair_reversed = (not vuln_correct) and (not clean_correct)\n",
        "\n",
        "                pair_result = {\n",
        "                    \"pair_id\": i + 1,\n",
        "                    \"commit_id\": vuln_truth.commit_id,\n",
        "                    \"ground_truth_cwe\": vuln_truth.cwe,\n",
        "                    \"vulnerable\": {\n",
        "                        \"prediction\": vuln_pred_dict['is_vulnerable'],\n",
        "                        \"detected_cwes\": vuln_pred_dict['detected_cwes'],\n",
        "                        \"cwe_matched\": cwe_matched,\n",
        "                        \"correct\": vuln_correct\n",
        "                    },\n",
        "                    \"fixed\": {\n",
        "                        \"prediction\": clean_pred_dict['is_vulnerable'],\n",
        "                        \"detected_cwes\": clean_pred_dict['detected_cwes'],\n",
        "                        \"correct\": clean_correct\n",
        "                    },\n",
        "                    \"pair_result\": {\n",
        "                        \"is_correct\": is_pair_correct,\n",
        "                        \"is_reversed\": is_pair_reversed,\n",
        "                        \"classification\": (\n",
        "                            \"P-C\" if is_pair_correct else\n",
        "                            \"P-R\" if is_pair_reversed else\n",
        "                            \"both_vuln\" if (vuln_pred_dict['is_vulnerable'] and clean_pred_dict['is_vulnerable']) else\n",
        "                            \"both_safe\"\n",
        "                        )\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                pair_results.append(pair_result)\n",
        "\n",
        "                print(f\"\\nPair Result: {pair_result['pair_result']['classification']}\")\n",
        "\n",
        "            if (i + 1) % checkpoint_interval == 0:\n",
        "                print(f\"\\nSaving checkpoint at pair {i+1}/{num_pairs}...\")\n",
        "                checkpoint_manager.save(\n",
        "                    last_pair_index=i,\n",
        "                    vuln_predictions=vuln_predictions,\n",
        "                    clean_predictions=clean_predictions,\n",
        "                    pair_results=pair_results,\n",
        "                    llm_stats=pipeline.llm.get_stats(),\n",
        "                    metadata={\n",
        "                        \"num_pairs\": num_pairs,\n",
        "                        \"random_sample\": random_sample\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                stats = pipeline.llm.get_stats()\n",
        "                print(f\"Progress: {stats['total_calls']} LLM calls, \"\n",
        "                      f\"{stats['success_rate']*100:.1f}% success rate\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\nEvaluation interrupted by user!\")\n",
        "        print(\"Saving current progress to checkpoint...\")\n",
        "        checkpoint_manager.save(\n",
        "            last_pair_index=i,\n",
        "            vuln_predictions=vuln_predictions,\n",
        "            clean_predictions=clean_predictions,\n",
        "            pair_results=pair_results,\n",
        "            llm_stats=pipeline.llm.get_stats(),\n",
        "            metadata={\n",
        "                \"num_pairs\": num_pairs,\n",
        "                \"random_sample\": random_sample,\n",
        "                \"interrupted\": True\n",
        "            }\n",
        "        )\n",
        "        print(\"Checkpoint saved. Run resume_paired_evaluation() to continue.\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n\\nEvaluation failed with error: {e}\")\n",
        "        print(\"Saving partial results to checkpoint...\")\n",
        "        checkpoint_manager.save(\n",
        "            last_pair_index=i if 'i' in locals() else -1,\n",
        "            vuln_predictions=vuln_predictions,\n",
        "            clean_predictions=clean_predictions,\n",
        "            pair_results=pair_results,\n",
        "            llm_stats=pipeline.llm.get_stats(),\n",
        "            metadata={\n",
        "                \"num_pairs\": num_pairs,\n",
        "                \"random_sample\": random_sample,\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "        )\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "    print(\"\\nCleaning up checkpoint...\")\n",
        "    checkpoint_manager.delete()\n",
        "\n",
        "    print(\"\\nEVALUATION COMPLETE!\\n\")\n",
        "\n",
        "    return evaluation_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUB2k4DAQorC"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# RESUME FROM CHECKPOINT\n",
        "# =============================================================================\n",
        "\n",
        "def resume_paired_evaluation():\n",
        "\n",
        "    checkpoint_manager = PairedCheckpointManager()\n",
        "\n",
        "    if not checkpoint_manager.exists():\n",
        "        print(\"No checkpoint found!\")\n",
        "        print(\"   Use evaluate_primevul_paired_with_checkpoints() to start a new evaluation\")\n",
        "        return None\n",
        "\n",
        "    print(\"Checkpoint found! Loading...\")\n",
        "\n",
        "    checkpoint_info = checkpoint_manager.get_info()\n",
        "    if checkpoint_info:\n",
        "        print(f\"   Last processed pair: {checkpoint_info['last_pair_index'] + 1}\")\n",
        "        print(f\"   Vulnerable predictions: {checkpoint_info['num_vuln_predictions']}\")\n",
        "        print(f\"   Clean predictions: {checkpoint_info['num_clean_predictions']}\")\n",
        "        print(f\"   Pair results: {checkpoint_info['num_pair_results']}\")\n",
        "        print(f\"   Checkpoint time: {time.ctime(checkpoint_info['timestamp'])}\")\n",
        "\n",
        "    print(\"\\nInitializing pipeline...\")\n",
        "    Config.setup_directories()\n",
        "\n",
        "    llm = UnifiedLLMInterface(\n",
        "        api_key=Config.GROQ_API_KEY,\n",
        "        models=None\n",
        "    )\n",
        "\n",
        "    pipeline = VulPrunePipeline(llm)\n",
        "\n",
        "    checkpoint_data = checkpoint_manager.load()\n",
        "    metadata = checkpoint_data.get(\"metadata\", {})\n",
        "    num_pairs = metadata.get(\"num_pairs\", 10)\n",
        "    random_sample = metadata.get(\"random_sample\", True)\n",
        "\n",
        "    print(f\"\\nResuming evaluation...\")\n",
        "    print(f\"   Total pairs: {num_pairs}\")\n",
        "    print(f\"   Remaining pairs: {num_pairs - (checkpoint_info['last_pair_index'] + 1)}\")\n",
        "\n",
        "    return evaluate_primevul_paired_with_checkpoints(\n",
        "        pipeline=pipeline,\n",
        "        num_pairs=num_pairs,\n",
        "        random_sample=random_sample\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKzyENn3Qq8Z"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CHECKPOINT MANAGEMENT UTILITIES\n",
        "# =============================================================================\n",
        "\n",
        "def list_paired_checkpoints():\n",
        "    checkpoint_manager = PairedCheckpointManager()\n",
        "\n",
        "    if not checkpoint_manager.exists():\n",
        "        print(\"No paired evaluation checkpoints found\")\n",
        "        return\n",
        "\n",
        "    info = checkpoint_manager.get_info()\n",
        "    if info:\n",
        "        print(f\"Paired Evaluation Checkpoint:\")\n",
        "        print(f\"   Date: {time.ctime(info['timestamp'])}\")\n",
        "        print(f\"   Progress: Pair {info['last_pair_index'] + 1}\")\n",
        "        print(f\"   Vulnerable predictions: {info['num_vuln_predictions']}\")\n",
        "        print(f\"   Clean predictions: {info['num_clean_predictions']}\")\n",
        "        print(f\"   Pair results: {info['num_pair_results']}\")\n",
        "\n",
        "        if info.get('metadata'):\n",
        "            print(f\"   Metadata: {info['metadata']}\")\n",
        "\n",
        "def clear_paired_checkpoints():\n",
        "    checkpoint_manager = PairedCheckpointManager()\n",
        "\n",
        "    if not checkpoint_manager.exists():\n",
        "        print(\"No checkpoints to clear\")\n",
        "        return\n",
        "\n",
        "    if checkpoint_manager.delete():\n",
        "        print(\"Paired evaluation checkpoint deleted\")\n",
        "    else:\n",
        "        print(\"Failed to delete checkpoint\")\n",
        "\n",
        "def get_paired_checkpoint_stats():\n",
        "    checkpoint_manager = PairedCheckpointManager()\n",
        "\n",
        "    if not checkpoint_manager.exists():\n",
        "        print(\"No active checkpoint\")\n",
        "        return None\n",
        "\n",
        "    checkpoint_data = checkpoint_manager.load()\n",
        "    if not checkpoint_data:\n",
        "        return None\n",
        "\n",
        "    llm_stats = checkpoint_data.get(\"llm_stats\", {})\n",
        "\n",
        "    if llm_stats:\n",
        "        print(f\"\\nLLM Usage:\")\n",
        "        print(f\"  Total calls: {llm_stats.get('total_calls', 0)}\")\n",
        "        print(f\"  Success rate: {llm_stats.get('success_rate', 0)*100:.1f}%\")\n",
        "\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    return checkpoint_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wmjkb8WAQ5_9"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# POST-ANALYSIS TOOLS FOR PAIRED RESULTS\n",
        "# =============================================================================\n",
        "\n",
        "def analyze_results(results_path: str = None):\n",
        "    if results_path is None:\n",
        "        results_dir = Config.RESULTS_DIR\n",
        "        result_files = sorted(results_dir.glob(\"*paired*.json\"), key=lambda x: x.stat().st_mtime)\n",
        "\n",
        "        if not result_files:\n",
        "            result_files = sorted(results_dir.glob(\"*results*.json\"), key=lambda x: x.stat().st_mtime)\n",
        "\n",
        "        if not result_files:\n",
        "            print(\"No results files found\")\n",
        "            return\n",
        "\n",
        "        results_path = result_files[-1]\n",
        "        print(f\"Using latest results: {results_path.name}\")\n",
        "\n",
        "    with open(results_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    results = data.get(\"results\", [])\n",
        "    metrics = data.get(\"metrics\", {})\n",
        "    cwe_detection = data.get(\"cwe_detection\", {})\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"DETAILED PAIRED RESULTS ANALYSIS\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    total = len(results)\n",
        "    with_errors = sum(1 for r in results if \"error\" in r)\n",
        "    successful = total - with_errors\n",
        "\n",
        "    vuln_samples = [r for r in results if r.get('is_vulnerable', False)]\n",
        "    fixed_samples = [r for r in results if not r.get('is_vulnerable', False)]\n",
        "\n",
        "    print(f\"Total samples: {total}\")\n",
        "    print(f\"  Vulnerable samples: {len(vuln_samples)}\")\n",
        "    print(f\"  Fixed samples: {len(fixed_samples)}\")\n",
        "    print(f\"Successful: {successful}\")\n",
        "    print(f\"Errors: {with_errors}\")\n",
        "\n",
        "    if metrics:\n",
        "        print(f\"\\nPaired Metrics Summary:\")\n",
        "        print(f\"  P-C (Pair Correct): {metrics.get('P-C', 0)} ({metrics.get('P-C%', 0):.2f}%)\")\n",
        "        print(f\"  P-R (Pair Reversed): {metrics.get('P-R', 0)} ({metrics.get('P-R%', 0):.2f}%)\")\n",
        "        print(f\"  VPS (Pair Score): {metrics.get('VPS', 0)} ({metrics.get('VPS%', 0):.2f}%)\")\n",
        "        print(f\"  Accuracy: {metrics.get('ACC%', 0):.2f}%\")\n",
        "        print(f\"  F1 Score: {metrics.get('F1%', 0):.2f}%\")\n",
        "\n",
        "    if cwe_detection:\n",
        "        print(f\"\\nCWE Detection Rate:\")\n",
        "        print(f\"{'CWE':<15} {'Total':>10} {'Detected':>10} {'Rate':>10}\")\n",
        "        print(\"-\"*50)\n",
        "\n",
        "        for cwe, stats in sorted(cwe_detection.items(),\n",
        "                                key=lambda x: x[1].get('total', 0), reverse=True):\n",
        "            total_cwe = stats.get('total', 0)\n",
        "            detected_cwe = stats.get('detected', 0)\n",
        "            rate = (detected_cwe / total_cwe * 100) if total_cwe > 0 else 0\n",
        "            print(f\"{cwe:<15} {total_cwe:>10} {detected_cwe:>10} {rate:>9.1f}%\")\n",
        "\n",
        "    print(f\"\\nFalse Positives Analysis:\")\n",
        "    fps = [r for r in fixed_samples if r.get('prediction') == 1]\n",
        "    print(f\"  Total FPs: {len(fps)} / {len(fixed_samples)} fixed samples\")\n",
        "\n",
        "    if fps:\n",
        "        fp_cwes = {}\n",
        "        for fp in fps:\n",
        "            detected = fp.get('detected_cwes', [])\n",
        "            for cwe in detected:\n",
        "                fp_cwes[cwe] = fp_cwes.get(cwe, 0) + 1\n",
        "\n",
        "        print(f\"  Most common false CWEs:\")\n",
        "        for cwe, count in sorted(fp_cwes.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
        "            print(f\"    - {cwe}: {count} times\")\n",
        "\n",
        "    print(f\"\\n❌ False Negatives Analysis:\")\n",
        "    fns = [r for r in vuln_samples if r.get('prediction') == 0]\n",
        "    print(f\"  Total FNs: {len(fns)} / {len(vuln_samples)} vulnerable samples\")\n",
        "\n",
        "    if fns:\n",
        "        fn_cwes = {}\n",
        "        for fn in fns:\n",
        "            cwe = fn.get('ground_truth_cwe', 'UNKNOWN')\n",
        "            fn_cwes[cwe] = fn_cwes.get(cwe, 0) + 1\n",
        "\n",
        "        print(f\"  Missed CWE types:\")\n",
        "        for cwe, count in sorted(fn_cwes.items(), key=lambda x: x[1], reverse=True):\n",
        "            print(f\"    - {cwe}: {count} times ({count/len(vuln_samples)*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nCWE Match Analysis (Correct CWE Detection):\")\n",
        "    vuln_with_match = [r for r in vuln_samples if r.get('cwe_matched', False)]\n",
        "    vuln_detected = [r for r in vuln_samples if r.get('prediction') == 1]\n",
        "\n",
        "    print(f\"  Vulnerable samples detected: {len(vuln_detected)} / {len(vuln_samples)}\")\n",
        "    print(f\"  With correct CWE: {len(vuln_with_match)} / {len(vuln_samples)} ({len(vuln_with_match)/len(vuln_samples)*100:.1f}%)\")\n",
        "\n",
        "    if vuln_detected:\n",
        "        match_rate = len(vuln_with_match) / len(vuln_detected) * 100 if vuln_detected else 0\n",
        "        print(f\"  Match rate (among detected): {len(vuln_with_match)} / {len(vuln_detected)} ({match_rate:.1f}%)\")\n",
        "\n",
        "    print(f\"\\n{'='*70}\\n\")\n",
        "\n",
        "def export_results_csv(results_path: str = None, output_path: str = None):\n",
        "\n",
        "    if results_path is None:\n",
        "        results_dir = Config.RESULTS_DIR\n",
        "        result_files = sorted(results_dir.glob(\"*paired*.json\"), key=lambda x: x.stat().st_mtime)\n",
        "        if not result_files:\n",
        "            result_files = sorted(results_dir.glob(\"*results*.json\"), key=lambda x: x.stat().st_mtime)\n",
        "        if not result_files:\n",
        "            print(\"❌ No results files found\")\n",
        "            return\n",
        "        results_path = result_files[-1]\n",
        "\n",
        "    if output_path is None:\n",
        "        output_path = Config.RESULTS_DIR / \"paired_results_export.csv\"\n",
        "\n",
        "    with open(results_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    results = data.get(\"results\", [])\n",
        "\n",
        "    with open(output_path, \"w\", newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "\n",
        "        writer.writerow([\n",
        "            \"ID\", \"Commit_ID\", \"Is_Vulnerable\", \"Ground_Truth_CWE\", \"Target\",\n",
        "            \"Prediction\", \"Detected_CWEs\", \"Num_Detected_CWEs\",\n",
        "            \"Correct\", \"CWE_Matched\", \"Error\"\n",
        "        ])\n",
        "\n",
        "        for r in results:\n",
        "            row = [\n",
        "                r.get(\"id\", \"\"),\n",
        "                r.get(\"commit_id\", \"\"),\n",
        "                r.get(\"is_vulnerable\", \"\"),\n",
        "                r.get(\"ground_truth_cwe\", \"\"),\n",
        "                r.get(\"target\", \"\"),\n",
        "                r.get(\"prediction\", \"\"),\n",
        "                \";\".join(r.get(\"detected_cwes\", [])),\n",
        "                len(r.get(\"detected_cwes\", [])),\n",
        "                r.get(\"prediction\") == r.get(\"target\"),\n",
        "                r.get(\"cwe_matched\", False),\n",
        "                \"error\" in r\n",
        "            ]\n",
        "            writer.writerow(row)\n",
        "\n",
        "    print(f\"Results exported to: {output_path}\")\n",
        "    print(f\"   Total rows: {len(results)}\")\n",
        "\n",
        "    pairs_output = Config.RESULTS_DIR / \"pairs_summary.csv\"\n",
        "    _export_pairs_summary(results, pairs_output)\n",
        "\n",
        "def _export_pairs_summary(results: List[Dict], output_path: Path):\n",
        "\n",
        "    pairs = {}\n",
        "    for r in results:\n",
        "        commit_id = r.get('commit_id', '')\n",
        "        if not commit_id:\n",
        "            continue\n",
        "\n",
        "        if commit_id not in pairs:\n",
        "            pairs[commit_id] = {\n",
        "                'commit_id': commit_id,\n",
        "                'cwe': r.get('ground_truth_cwe', ''),\n",
        "                'vuln_pred': None,\n",
        "                'fixed_pred': None,\n",
        "                'vuln_cwes': [],\n",
        "                'fixed_cwes': []\n",
        "            }\n",
        "\n",
        "        if r.get('is_vulnerable'):\n",
        "            pairs[commit_id]['vuln_pred'] = r.get('prediction')\n",
        "            pairs[commit_id]['vuln_cwes'] = r.get('detected_cwes', [])\n",
        "        else:\n",
        "            pairs[commit_id]['fixed_pred'] = r.get('prediction')\n",
        "            pairs[commit_id]['fixed_cwes'] = r.get('detected_cwes', [])\n",
        "\n",
        "    with open(output_path, \"w\", newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "\n",
        "        writer.writerow([\n",
        "            \"Commit_ID\", \"Ground_Truth_CWE\",\n",
        "            \"Vuln_Pred\", \"Fixed_Pred\",\n",
        "            \"Vuln_CWEs\", \"Fixed_CWEs\",\n",
        "            \"Pair_Correct\", \"Pair_Reversed\"\n",
        "        ])\n",
        "\n",
        "        for commit_id, pair in pairs.items():\n",
        "            pair_correct = pair['vuln_pred'] == 1 and pair['fixed_pred'] == 0\n",
        "            pair_reversed = pair['vuln_pred'] == 0 and pair['fixed_pred'] == 1\n",
        "\n",
        "            writer.writerow([\n",
        "                commit_id,\n",
        "                pair['cwe'],\n",
        "                pair['vuln_pred'],\n",
        "                pair['fixed_pred'],\n",
        "                \";\".join(pair['vuln_cwes']),\n",
        "                \";\".join(pair['fixed_cwes']),\n",
        "                pair_correct,\n",
        "                pair_reversed\n",
        "            ])\n",
        "\n",
        "    print(f\"Pairs summary exported to: {output_path}\")\n",
        "    print(f\"   Total pairs: {len(pairs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIEJpFLRNzQT"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PROVIDERS INITIALISATION\n",
        "# =============================================================================\n",
        "\n",
        "def create_groq_only():\n",
        "    groq = GroqProvider(\n",
        "        api_key=APIConfig.GROQ_API_KEY,\n",
        "        model_names=None\n",
        "    )\n",
        "    return UnifiedLLMInterface([groq], rotation_strategy=\"priority\")\n",
        "\n",
        "def create_openai_only():\n",
        "    openai_provider = OpenAIProvider(\n",
        "        api_key=APIConfig.OPENAI_API_KEY,\n",
        "        model_names=[\"gpt-4o\"]\n",
        "    )\n",
        "    return UnifiedLLMInterface([openai_provider], rotation_strategy=\"priority\")\n",
        "\n",
        "def create_deepseek_only():\n",
        "    deepseek = DeepSeekProvider(\n",
        "        api_key=APIConfig.DEEPSEEK_API_KEY,\n",
        "        model_names=[\"deepseek-chat\"]\n",
        "    )\n",
        "    return UnifiedLLMInterface([deepseek], rotation_strategy=\"priority\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR9SGP_mTIcV"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DIFFERENTS PROVIDERS EXECUTION\n",
        "# =============================================================================\n",
        "\n",
        "Config.setup_directories()\n",
        "\n",
        "# OPTION 1 : Groq only\n",
        "llm = create_groq_only()\n",
        "\n",
        "# OPTION 2 : OpenAI 4o only\n",
        "# llm = create_openai_only()\n",
        "\n",
        "# OPTION 3 : DeepSeek only\n",
        "# llm = create_deepseek_only()\n",
        "\n",
        "pipeline = VulPrunePipeline(llm)\n",
        "\n",
        "results = evaluate_primevul_paired_with_checkpoints(\n",
        "    pipeline=pipeline,\n",
        "    num_pairs=21,\n",
        "    random_sample=[154, 113, 196, 151, 133, 86, 11, 33, 324, 162, 38, 37, 231, 283, 271, 267, 19, 290, 227, 169, 168],\n",
        "    checkpoint_interval=1\n",
        ")\n",
        "\n",
        "llm.print_stats()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
