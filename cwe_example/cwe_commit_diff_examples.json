{
  "CWE-79": {
    "cve": "CVE-2024-34357",
    "commit_url": "https://github.com/TYPO3/typo3/commit/376474904f6b9a54dc1b785a2e45277cbd13b0d7",
    "diff": "diff --git a/typo3/sysext/frontend/Classes/Controller/ShowImageController.php b/typo3/sysext/frontend/Classes/Controller/ShowImageController.php\nindex 0e98b15c79a7..1444d0bc339b 100644\n--- a/typo3/sysext/frontend/Classes/Controller/ShowImageController.php\n+++ b/typo3/sysext/frontend/Classes/Controller/ShowImageController.php\n@@ -166,12 +166,12 @@ public function main()\n             '###publicUrl###' => htmlspecialchars($processedImage->getPublicUrl() ?? ''),\n             '###alt###' => htmlspecialchars($this->file->getProperty('alternative') ?: $this->title),\n             '###title###' => htmlspecialchars($this->file->getProperty('title') ?: $this->title),\n-            '###width###' => $processedImage->getProperty('width'),\n-            '###height###' => $processedImage->getProperty('height'),\n+            '###width###' => htmlspecialchars((string)$processedImage->getProperty('width')),\n+            '###height###' => htmlspecialchars((string)$processedImage->getProperty('height')),\n         ];\n         $this->imageTag = str_replace(array_keys($imageTagMarkers), array_values($imageTagMarkers), $this->imageTag);\n         $markerArray = [\n-            '###TITLE###' => $this->file->getProperty('title') ?: $this->title,\n+            '###TITLE###' => htmlspecialchars($this->file->getProperty('title') ?: $this->title),\n             '###IMAGE###' => $this->imageTag,\n             '###BODY###' => $this->bodyTag,\n         ];\n"
  },
  "CWE-269": {
    "cve": "CVE-2024-41666",
    "commit_url": "https://github.com/argoproj/argo-cd/commit/05edb2a9ca48f0f10608c1b49fbb0cf7164f6476",
    "diff": "diff --git a/server/application/terminal.go b/server/application/terminal.go\nindex 53784fc5ffcc1..9886992e81b9e 100644\n--- a/server/application/terminal.go\n+++ b/server/application/terminal.go\n@@ -225,7 +225,7 @@ func (s *terminalHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n \n \tfieldLog.Info(\"terminal session starting\")\n \n-\tsession, err := newTerminalSession(w, r, nil, s.sessionManager)\n+\tsession, err := newTerminalSession(ctx, w, r, nil, s.sessionManager, appRBACName, s.enf)\n \tif err != nil {\n \t\thttp.Error(w, \"Failed to start terminal session\", http.StatusBadRequest)\n \t\treturn\ndiff --git a/server/application/websocket.go b/server/application/websocket.go\nindex b04330c45c3d7..ab3cce72f1239 100644\n--- a/server/application/websocket.go\n+++ b/server/application/websocket.go\n@@ -1,15 +1,19 @@\n package application\n \n import (\n+\t\"context\"\n \t\"encoding/json\"\n \t\"fmt\"\n-\t\"github.com/argoproj/argo-cd/v2/common\"\n-\thttputil \"github.com/argoproj/argo-cd/v2/util/http\"\n-\tutil_session \"github.com/argoproj/argo-cd/v2/util/session\"\n \t\"net/http\"\n \t\"sync\"\n \t\"time\"\n \n+\t\"github.com/argoproj/argo-cd/v2/common\"\n+\t\"github.com/argoproj/argo-cd/v2/server/rbacpolicy\"\n+\thttputil \"github.com/argoproj/argo-cd/v2/util/http\"\n+\t\"github.com/argoproj/argo-cd/v2/util/rbac\"\n+\tutil_session \"github.com/argoproj/argo-cd/v2/util/session\"\n+\n \t\"github.com/gorilla/websocket\"\n \tlog \"github.com/sirupsen/logrus\"\n \t\"k8s.io/client-go/tools/remotecommand\"\n@@ -31,6 +35,7 @@ var upgrader = func() websocket.Upgrader {\n \n // terminalSession implements PtyHandler\n type terminalSession struct {\n+\tctx            context.Context\n \twsConn         *websocket.Conn\n \tsizeChan       chan remotecommand.TerminalSize\n \tdoneChan       chan struct{}\n@@ -39,6 +44,8 @@ type terminalSession struct {\n \twriteLock      sync.Mutex\n \tsessionManager *util_session.SessionManager\n \ttoken          *string\n+\tappRBACName    string\n+\tenf            *rbac.Enforcer\n }\n \n // getToken get auth token from web socket request\n@@ -48,7 +55,7 @@ func getToken(r *http.Request) (string, error) {\n }\n \n // newTerminalSession create terminalSession\n-func newTerminalSession(w http.ResponseWriter, r *http.Request, responseHeader http.Header, sessionManager *util_session.SessionManager) (*terminalSession, error) {\n+func newTerminalSession(ctx context.Context, w http.ResponseWriter, r *http.Request, responseHeader http.Header, sessionManager *util_session.SessionManager, appRBACName string, enf *rbac.Enforcer) (*terminalSession, error) {\n \ttoken, err := getToken(r)\n \tif err != nil {\n \t\treturn nil, err\n@@ -59,12 +66,15 @@ func newTerminalSession(w http.ResponseWriter, r *http.Request, responseHeader h\n \t\treturn nil, err\n \t}\n \tsession := &terminalSession{\n+\t\tctx:            ctx,\n \t\twsConn:         conn,\n \t\ttty:            true,\n \t\tsizeChan:       make(chan remotecommand.TerminalSize),\n \t\tdoneChan:       make(chan struct{}),\n \t\tsessionManager: sessionManager,\n \t\ttoken:          &token,\n+\t\tappRBACName:    appRBACName,\n+\t\tenf:            enf,\n \t}\n \treturn session, nil\n }\n@@ -125,6 +135,29 @@ func (t *terminalSession) reconnect() (int, error) {\n \treturn 0, nil\n }\n \n+func (t *terminalSession) validatePermissions(p []byte) (int, error) {\n+\tpermissionDeniedMessage, _ := json.Marshal(TerminalMessage{\n+\t\tOperation: \"stdout\",\n+\t\tData:      \"Permission denied\",\n+\t})\n+\tif err := t.enf.EnforceErr(t.ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionGet, t.appRBACName); err != nil {\n+\t\terr = t.wsConn.WriteMessage(websocket.TextMessage, permissionDeniedMessage)\n+\t\tif err != nil {\n+\t\t\tlog.Errorf(\"permission denied message err: %v\", err)\n+\t\t}\n+\t\treturn copy(p, EndOfTransmission), permissionDeniedErr\n+\t}\n+\n+\tif err := t.enf.EnforceErr(t.ctx.Value(\"claims\"), rbacpolicy.ResourceExec, rbacpolicy.ActionCreate, t.appRBACName); err != nil {\n+\t\terr = t.wsConn.WriteMessage(websocket.TextMessage, permissionDeniedMessage)\n+\t\tif err != nil {\n+\t\t\tlog.Errorf(\"permission denied message err: %v\", err)\n+\t\t}\n+\t\treturn copy(p, EndOfTransmission), permissionDeniedErr\n+\t}\n+\treturn 0, nil\n+}\n+\n // Read called in a loop from remotecommand as long as the process is running\n func (t *terminalSession) Read(p []byte) (int, error) {\n \t// check if token still valid\n@@ -135,6 +168,12 @@ func (t *terminalSession) Read(p []byte) (int, error) {\n \t\treturn t.reconnect()\n \t}\n \n+\t// validate permissions\n+\tcode, err := t.validatePermissions(p)\n+\tif err != nil {\n+\t\treturn code, err\n+\t}\n+\n \tt.readLock.Lock()\n \t_, message, err := t.wsConn.ReadMessage()\n \tt.readLock.Unlock()\ndiff --git a/server/application/websocket_test.go b/server/application/websocket_test.go\nindex 30c5ffa232328..b95129418d9ce 100644\n--- a/server/application/websocket_test.go\n+++ b/server/application/websocket_test.go\n@@ -1,23 +1,65 @@\n package application\n \n import (\n+\t\"context\"\n \t\"encoding/json\"\n-\t\"github.com/gorilla/websocket\"\n-\t\"github.com/stretchr/testify/assert\"\n \t\"net/http\"\n \t\"net/http/httptest\"\n \t\"strings\"\n \t\"testing\"\n+\n+\tv1 \"k8s.io/api/core/v1\"\n+\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+\t\"k8s.io/client-go/kubernetes/fake\"\n+\n+\t\"github.com/argoproj/argo-cd/v2/common\"\n+\t\"github.com/argoproj/argo-cd/v2/util/assets\"\n+\t\"github.com/argoproj/argo-cd/v2/util/rbac\"\n+\n+\t\"github.com/golang-jwt/jwt/v4\"\n+\t\"github.com/gorilla/websocket\"\n+\t\"github.com/stretchr/testify/assert\"\n+\t\"github.com/stretchr/testify/require\"\n )\n \n-func reconnect(w http.ResponseWriter, r *http.Request) {\n+func newTestTerminalSession(w http.ResponseWriter, r *http.Request) terminalSession {\n \tvar upgrader = websocket.Upgrader{}\n \tc, err := upgrader.Upgrade(w, r, nil)\n \tif err != nil {\n-\t\treturn\n+\t\treturn terminalSession{}\n \t}\n \n-\tts := terminalSession{wsConn: c}\n+\treturn terminalSession{wsConn: c}\n+}\n+\n+func newEnforcer() *rbac.Enforcer {\n+\tadditionalConfig := make(map[string]string, 0)\n+\tkubeclientset := fake.NewSimpleClientset(&v1.ConfigMap{\n+\t\tObjectMeta: metav1.ObjectMeta{\n+\t\t\tNamespace: testNamespace,\n+\t\t\tName:      \"argocd-cm\",\n+\t\t\tLabels: map[string]string{\n+\t\t\t\t\"app.kubernetes.io/part-of\": \"argocd\",\n+\t\t\t},\n+\t\t},\n+\t\tData: additionalConfig,\n+\t}, &v1.Secret{\n+\t\tObjectMeta: metav1.ObjectMeta{\n+\t\t\tName:      \"argocd-secret\",\n+\t\t\tNamespace: testNamespace,\n+\t\t},\n+\t\tData: map[string][]byte{\n+\t\t\t\"admin.password\":   []byte(\"test\"),\n+\t\t\t\"server.secretkey\": []byte(\"test\"),\n+\t\t},\n+\t})\n+\n+\tenforcer := rbac.NewEnforcer(kubeclientset, testNamespace, common.ArgoCDRBACConfigMapName, nil)\n+\treturn enforcer\n+}\n+\n+func reconnect(w http.ResponseWriter, r *http.Request) {\n+\tts := newTestTerminalSession(w, r)\n \t_, _ = ts.reconnect()\n }\n \n@@ -44,3 +86,71 @@ func TestReconnect(t *testing.T) {\n \tassert.Equal(t, message.Data, ReconnectMessage)\n \n }\n+\n+func TestValidateWithAdminPermissions(t *testing.T) {\n+\tvalidate := func(w http.ResponseWriter, r *http.Request) {\n+\t\tenf := newEnforcer()\n+\t\t_ = enf.SetBuiltinPolicy(assets.BuiltinPolicyCSV)\n+\t\tenf.SetDefaultRole(\"role:admin\")\n+\t\tenf.SetClaimsEnforcerFunc(func(claims jwt.Claims, rvals ...interface{}) bool {\n+\t\t\treturn true\n+\t\t})\n+\t\tts := newTestTerminalSession(w, r)\n+\t\tts.enf = enf\n+\t\tts.appRBACName = \"test\"\n+\t\t// nolint:staticcheck\n+\t\tts.ctx = context.WithValue(context.Background(), \"claims\", &jwt.MapClaims{\"groups\": []string{\"admin\"}})\n+\t\t_, err := ts.validatePermissions([]byte{})\n+\t\trequire.NoError(t, err)\n+\t}\n+\n+\ts := httptest.NewServer(http.HandlerFunc(validate))\n+\tdefer s.Close()\n+\n+\tu := \"ws\" + strings.TrimPrefix(s.URL, \"http\")\n+\n+\t// Connect to the server\n+\tws, _, err := websocket.DefaultDialer.Dial(u, nil)\n+\trequire.NoError(t, err)\n+\n+\tdefer ws.Close()\n+}\n+\n+func TestValidateWithoutPermissions(t *testing.T) {\n+\tvalidate := func(w http.ResponseWriter, r *http.Request) {\n+\t\tenf := newEnforcer()\n+\t\t_ = enf.SetBuiltinPolicy(assets.BuiltinPolicyCSV)\n+\t\tenf.SetDefaultRole(\"role:test\")\n+\t\tenf.SetClaimsEnforcerFunc(func(claims jwt.Claims, rvals ...interface{}) bool {\n+\t\t\treturn false\n+\t\t})\n+\t\tts := newTestTerminalSession(w, r)\n+\t\tts.enf = enf\n+\t\tts.appRBACName = \"test\"\n+\t\t// nolint:staticcheck\n+\t\tts.ctx = context.WithValue(context.Background(), \"claims\", &jwt.MapClaims{\"groups\": []string{\"test\"}})\n+\t\t_, err := ts.validatePermissions([]byte{})\n+\t\trequire.Error(t, err)\n+\t\tassert.Equal(t, permissionDeniedErr.Error(), err.Error())\n+\t}\n+\n+\ts := httptest.NewServer(http.HandlerFunc(validate))\n+\tdefer s.Close()\n+\n+\tu := \"ws\" + strings.TrimPrefix(s.URL, \"http\")\n+\n+\t// Connect to the server\n+\tws, _, err := websocket.DefaultDialer.Dial(u, nil)\n+\trequire.NoError(t, err)\n+\n+\tdefer ws.Close()\n+\n+\t_, p, _ := ws.ReadMessage()\n+\n+\tvar message TerminalMessage\n+\n+\terr = json.Unmarshal(p, &message)\n+\n+\trequire.NoError(t, err)\n+\tassert.Equal(t, \"Permission denied\", message.Data)\n+}\n"
  },
  "CWE-125": {
    "cve": "CVE-2024-25178",
    "commit_url": "https://github.com/LuaJIT/LuaJIT/commit/defe61a56751a0db5f00ff3ab7b8f45436ba74c8",
    "diff": "diff --git a/src/lj_debug.c b/src/lj_debug.c\nindex e6a8be54e7..bca1d7a5be 100644\n--- a/src/lj_debug.c\n+++ b/src/lj_debug.c\n@@ -63,6 +63,7 @@ static BCPos debug_framepc(lua_State *L, GCfunc *fn, cTValue *nextframe)\n     if (cf == NULL || (char *)cframe_pc(cf) == (char *)cframe_L(cf))\n       return NO_BCPOS;\n     ins = cframe_pc(cf);  /* Only happens during error/hook handling. */\n+    if (!ins) return NO_BCPOS;\n   } else {\n     if (frame_islua(nextframe)) {\n       ins = frame_pc(nextframe);\ndiff --git a/src/lj_err.c b/src/lj_err.c\nindex 4a2d6bbd13..7afe1e291e 100644\n--- a/src/lj_err.c\n+++ b/src/lj_err.c\n@@ -488,7 +488,14 @@ LJ_NOINLINE void lj_err_mem(lua_State *L)\n {\n   if (L->status == LUA_ERRERR+1)  /* Don't touch the stack during lua_open. */\n     lj_vm_unwind_c(L->cframe, LUA_ERRMEM);\n-  if (curr_funcisL(L)) L->top = curr_topL(L);\n+  if (curr_funcisL(L)) {\n+    L->top = curr_topL(L);\n+    if (LJ_UNLIKELY(L->top > tvref(L->maxstack))) {\n+      /* The current Lua frame violates the stack. Replace it with a dummy. */\n+      L->top = L->base;\n+      setframe_gc(L->base - 1, obj2gco(L));\n+    }\n+  }\n   setstrV(L, L->top++, lj_err_str(L, LJ_ERR_ERRMEM));\n   lj_err_throw(L, LUA_ERRMEM);\n }\n@@ -551,9 +558,11 @@ LJ_NOINLINE void LJ_FASTCALL lj_err_run(lua_State *L)\n {\n   ptrdiff_t ef = finderrfunc(L);\n   if (ef) {\n-    TValue *errfunc = restorestack(L, ef);\n-    TValue *top = L->top;\n+    TValue *errfunc, *top;\n+    lj_state_checkstack(L, LUA_MINSTACK * 2);  /* Might raise new error. */\n     lj_trace_abort(G(L));\n+    errfunc = restorestack(L, ef);\n+    top = L->top;\n     if (!tvisfunc(errfunc) || L->status == LUA_ERRERR) {\n       setstrV(L, top-1, lj_err_str(L, LJ_ERR_ERRERR));\n       lj_err_throw(L, LUA_ERRERR);\n@@ -567,6 +576,13 @@ LJ_NOINLINE void LJ_FASTCALL lj_err_run(lua_State *L)\n   lj_err_throw(L, LUA_ERRRUN);\n }\n \n+/* Stack overflow error. */\n+void LJ_FASTCALL lj_err_stkov(lua_State *L)\n+{\n+  lj_debug_addloc(L, err2msg(LJ_ERR_STKOV), L->base-1, NULL);\n+  lj_err_run(L);\n+}\n+\n /* Formatted runtime error message. */\n LJ_NORET LJ_NOINLINE static void err_msgv(lua_State *L, ErrMsg em, ...)\n {\ndiff --git a/src/lj_err.h b/src/lj_err.h\nindex 321719a9bf..150409228e 100644\n--- a/src/lj_err.h\n+++ b/src/lj_err.h\n@@ -23,6 +23,7 @@ LJ_DATA const char *lj_err_allmsg;\n LJ_FUNC GCstr *lj_err_str(lua_State *L, ErrMsg em);\n LJ_FUNCA_NORET void LJ_FASTCALL lj_err_throw(lua_State *L, int errcode);\n LJ_FUNC_NORET void lj_err_mem(lua_State *L);\n+LJ_FUNC_NORET void LJ_FASTCALL lj_err_stkov(lua_State *L);\n LJ_FUNCA_NORET void LJ_FASTCALL lj_err_run(lua_State *L);\n LJ_FUNC_NORET void lj_err_msg(lua_State *L, ErrMsg em);\n LJ_FUNC_NORET void lj_err_lex(lua_State *L, GCstr *src, const char *tok,\ndiff --git a/src/lj_state.c b/src/lj_state.c\nindex c2f0b115a2..adedb66c17 100644\n--- a/src/lj_state.c\n+++ b/src/lj_state.c\n@@ -96,27 +96,45 @@ void lj_state_shrinkstack(lua_State *L, MSize used)\n /* Try to grow stack. */\n void LJ_FASTCALL lj_state_growstack(lua_State *L, MSize need)\n {\n-  MSize n;\n-  if (L->stacksize >= LJ_STACK_MAXEX) {\n-    /* 4. Throw 'error in error handling' when we are _over_ the limit. */\n-    if (L->stacksize > LJ_STACK_MAXEX)\n+  MSize n = L->stacksize + need;\n+  if (LJ_LIKELY(n < LJ_STACK_MAX)) {  /* The stack can grow as requested. */\n+    if (n < 2 * L->stacksize) {  /* Try to double the size. */\n+      n = 2 * L->stacksize;\n+      if (n > LJ_STACK_MAX)\n+\tn = LJ_STACK_MAX;\n+    }\n+    resizestack(L, n);\n+  } else {  /* Request would overflow. Raise a stack overflow error. */\n+    if (curr_funcisL(L)) {\n+      L->top = curr_topL(L);\n+      if (L->top > tvref(L->maxstack)) {\n+\t/* The current Lua frame violates the stack, so replace it with a\n+\t** dummy. This can happen when BC_IFUNCF is trying to grow the stack.\n+\t*/\n+\tL->top = L->base;\n+\tsetframe_gc(L->base - 1, obj2gco(L));\n+      }\n+    }\n+    if (L->stacksize <= LJ_STACK_MAXEX) {\n+      /* An error handler might want to inspect the stack overflow error, but\n+      ** will need some stack space to run in. We give it a stack size beyond\n+      ** the normal limit in order to do so, then rely on lj_state_relimitstack\n+      ** calls during unwinding to bring us back to a convential stack size.\n+      ** The + 1 is space for the error message, and 2 * LUA_MINSTACK is for\n+      ** the lj_state_checkstack() call in lj_err_run().\n+      */\n+      resizestack(L, LJ_STACK_MAX + 1 + 2 * LUA_MINSTACK);\n+      lj_err_stkov(L);  /* May invoke an error handler. */\n+    } else {\n+      /* If we're here, then the stack overflow error handler is requesting\n+      ** to grow the stack even further. We have no choice but to abort the\n+      ** error handler.\n+      */\n+      GCstr *em = lj_err_str(L, LJ_ERR_STKOV);  /* Might OOM. */\n+      setstrV(L, L->top++, em);  /* There is always space to push an error. */\n       lj_err_throw(L, LUA_ERRERR);  /* Does not invoke an error handler. */\n-    /* 1. We are _at_ the limit after the last growth. */\n-    if (L->status < LUA_ERRRUN) {  /* 2. Throw 'stack overflow'. */\n-      L->status = LUA_ERRRUN;  /* Prevent ending here again for pushed msg. */\n-      lj_err_msg(L, LJ_ERR_STKOV);  /* May invoke an error handler. */\n     }\n-    /* 3. Add space (over the limit) for pushed message and error handler. */\n-  }\n-  n = L->stacksize + need;\n-  if (n > LJ_STACK_MAX) {\n-    n += 2*LUA_MINSTACK;\n-  } else if (n < 2*L->stacksize) {\n-    n = 2*L->stacksize;\n-    if (n >= LJ_STACK_MAX)\n-      n = LJ_STACK_MAX;\n   }\n-  resizestack(L, n);\n }\n \n void LJ_FASTCALL lj_state_growstack1(lua_State *L)\n"
  },
  "CWE-20": {
    "cve": "CVE-2025-55173",
    "commit_url": "https://github.com/vercel/next.js/commit/6b12c60c61ee80cb0443ccd20de82ca9b4422ddd",
    "diff": "diff --git a/packages/next/src/server/image-optimizer.ts b/packages/next/src/server/image-optimizer.ts\nindex aef6ed227f8793..0b69e5b3fdd8b1 100644\n--- a/packages/next/src/server/image-optimizer.ts\n+++ b/packages/next/src/server/image-optimizer.ts\n@@ -634,7 +634,6 @@ export async function fetchInternalImage(\n     const mocked = createRequestResponseMocks({\n       url: href,\n       method: _req.method || 'GET',\n-      headers: _req.headers,\n       socket: _req.socket,\n     })\n \ndiff --git a/test/integration/image-optimizer/app/pages/api/conditional-cookie.js b/test/integration/image-optimizer/app/pages/api/conditional-cookie.js\nnew file mode 100644\nindex 00000000000000..f67ce6872504cf\n--- /dev/null\n+++ b/test/integration/image-optimizer/app/pages/api/conditional-cookie.js\n@@ -0,0 +1,11 @@\n+const pixel =\n+  'iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNkYPj/HwADBwIAMCbHYQAAAABJRU5ErkJggg=='\n+\n+export default function handler(req, res) {\n+  if (req.headers['cookie']) {\n+    res.setHeader('content-type', 'image/png')\n+    res.end(Buffer.from(pixel, 'base64'))\n+  } else {\n+    res.status(401).end('cookie was not found')\n+  }\n+}\ndiff --git a/test/integration/image-optimizer/test/util.ts b/test/integration/image-optimizer/test/util.ts\nindex 2a8c43431e4fb7..559222a5398ef6 100644\n--- a/test/integration/image-optimizer/test/util.ts\n+++ b/test/integration/image-optimizer/test/util.ts\n@@ -308,6 +308,13 @@ export function runTests(ctx: RunTestsCtx) {\n     expect(ctx.nextOutput).toContain(animatedWarnText)\n   })\n \n+  it('should not forward cookie header', async () => {\n+    const query = { w: ctx.w, q: 30, url: '/api/conditional-cookie' }\n+    const opts = { headers: { accept: 'image/webp', cookie: '1' } }\n+    const res = await fetchViaHTTP(ctx.appPort, '/_next/image', query, opts)\n+    expect(res.status).toBe(400)\n+  })\n+\n   if (ctx.nextConfigImages?.dangerouslyAllowSVG) {\n     it('should maintain vector svg', async () => {\n       const query = { w: ctx.w, q: 90, url: '/test.svg' }\n"
  },
  "CWE-22": {
    "cve": "CVE-2025-7107",
    "commit_url": "https://github.com/simstudioai/sim/commit/b2450530d1ddd0397a11001a72aa0fde401db16a",
    "diff": "diff --git a/apps/sim/app/api/files/parse/route.test.ts b/apps/sim/app/api/files/parse/route.test.ts\nindex 510de5bf52..4258b5dd91 100644\n--- a/apps/sim/app/api/files/parse/route.test.ts\n+++ b/apps/sim/app/api/files/parse/route.test.ts\n@@ -1,4 +1,5 @@\n import path from 'path'\n+import { NextRequest } from 'next/server'\n /**\n  * Tests for file parse API route\n  *\n@@ -6,10 +7,9 @@ import path from 'path'\n  */\n import { afterEach, beforeEach, describe, expect, it, vi } from 'vitest'\n import { createMockRequest } from '@/app/api/__test-utils__/utils'\n+import { POST } from './route'\n \n-// Create actual mocks for path functions that we can use instead of using vi.doMock for path\n const mockJoin = vi.fn((...args: string[]): string => {\n-  // For the UPLOAD_DIR paths, just return a test path\n   if (args[0] === '/test/uploads') {\n     return `/test/uploads/${args[args.length - 1]}`\n   }\n@@ -17,7 +17,6 @@ const mockJoin = vi.fn((...args: string[]): string => {\n })\n \n describe('File Parse API Route', () => {\n-  // Mock file system and parser modules\n   const mockReadFile = vi.fn().mockResolvedValue(Buffer.from('test file content'))\n   const mockWriteFile = vi.fn().mockResolvedValue(undefined)\n   const mockUnlink = vi.fn().mockResolvedValue(undefined)\n@@ -36,15 +35,12 @@ describe('File Parse API Route', () => {\n   beforeEach(() => {\n     vi.resetModules()\n \n-    // Reset all mocks\n     vi.resetAllMocks()\n \n-    // Create a test upload file that exists for all tests\n     mockReadFile.mockResolvedValue(Buffer.from('test file content'))\n     mockAccessFs.mockResolvedValue(undefined)\n     mockStatFs.mockImplementation(() => ({ isFile: () => true }))\n \n-    // Mock filesystem operations\n     vi.doMock('fs', () => ({\n       existsSync: vi.fn().mockReturnValue(true),\n       constants: { R_OK: 4 },\n@@ -63,19 +59,16 @@ describe('File Parse API Route', () => {\n       stat: mockStatFs,\n     }))\n \n-    // Mock the S3 client\n     vi.doMock('@/lib/uploads/s3-client', () => ({\n       downloadFromS3: mockDownloadFromS3,\n     }))\n \n-    // Mock file parsers\n     vi.doMock('@/lib/file-parsers', () => ({\n       isSupportedFileType: vi.fn().mockReturnValue(true),\n       parseFile: mockParseFile,\n       parseBuffer: mockParseBuffer,\n     }))\n \n-    // Mock path module with our custom join function\n     vi.doMock('path', () => {\n       return {\n         ...path,\n@@ -85,7 +78,6 @@ describe('File Parse API Route', () => {\n       }\n     })\n \n-    // Mock the logger\n     vi.doMock('@/lib/logs/console-logger', () => ({\n       createLogger: vi.fn().mockReturnValue({\n         info: vi.fn(),\n@@ -95,7 +87,6 @@ describe('File Parse API Route', () => {\n       }),\n     }))\n \n-    // Configure upload directory and S3 mode\n     vi.doMock('@/lib/uploads/setup', () => ({\n       UPLOAD_DIR: '/test/uploads',\n       USE_S3_STORAGE: false,\n@@ -105,7 +96,6 @@ describe('File Parse API Route', () => {\n       },\n     }))\n \n-    // Skip setup.server.ts side effects\n     vi.doMock('@/lib/uploads/setup.server', () => ({}))\n   })\n \n@@ -113,7 +103,6 @@ describe('File Parse API Route', () => {\n     vi.clearAllMocks()\n   })\n \n-  // Basic tests testing the API structure\n   it('should handle missing file path', async () => {\n     const req = createMockRequest('POST', {})\n     const { POST } = await import('./route')\n@@ -125,47 +114,37 @@ describe('File Parse API Route', () => {\n     expect(data).toHaveProperty('error', 'No file path provided')\n   })\n \n-  // Test skipping the implementation details and testing what users would care about\n   it('should accept and process a local file', async () => {\n-    // Given: A request with a file path\n     const req = createMockRequest('POST', {\n       filePath: '/api/files/serve/test-file.txt',\n     })\n \n-    // When: The API processes the request\n     const { POST } = await import('./route')\n     const response = await POST(req)\n     const data = await response.json()\n \n-    // Then: Check the API contract without making assumptions about implementation\n     expect(response.status).toBe(200)\n-    expect(data).not.toBeNull() // We got a response\n+    expect(data).not.toBeNull()\n \n-    // The response either has a success indicator with output OR an error\n     if (data.success === true) {\n       expect(data).toHaveProperty('output')\n     } else {\n-      // If error, there should be an error message\n       expect(data).toHaveProperty('error')\n       expect(typeof data.error).toBe('string')\n     }\n   })\n \n   it('should process S3 files', async () => {\n-    // Given: A request with an S3 file path\n     const req = createMockRequest('POST', {\n       filePath: '/api/files/serve/s3/test-file.pdf',\n     })\n \n-    // When: The API processes the request\n     const { POST } = await import('./route')\n     const response = await POST(req)\n     const data = await response.json()\n \n-    // Then: We should get a response with parsed content or error\n     expect(response.status).toBe(200)\n \n-    // The data should either have a success flag with output or an error\n     if (data.success === true) {\n       expect(data).toHaveProperty('output')\n     } else {\n@@ -174,17 +153,14 @@ describe('File Parse API Route', () => {\n   })\n \n   it('should handle multiple files', async () => {\n-    // Given: A request with multiple file paths\n     const req = createMockRequest('POST', {\n       filePath: ['/api/files/serve/file1.txt', '/api/files/serve/file2.txt'],\n     })\n \n-    // When: The API processes the request\n     const { POST } = await import('./route')\n     const response = await POST(req)\n     const data = await response.json()\n \n-    // Then: We get an array of results\n     expect(response.status).toBe(200)\n     expect(data).toHaveProperty('success')\n     expect(data).toHaveProperty('results')\n@@ -193,20 +169,16 @@ describe('File Parse API Route', () => {\n   })\n \n   it('should handle S3 access errors gracefully', async () => {\n-    // Given: S3 will throw an error\n     mockDownloadFromS3.mockRejectedValueOnce(new Error('S3 access denied'))\n \n-    // And: A request with an S3 file path\n     const req = createMockRequest('POST', {\n       filePath: '/api/files/serve/s3/access-denied.pdf',\n     })\n \n-    // When: The API processes the request\n     const { POST } = await import('./route')\n     const response = await POST(req)\n     const data = await response.json()\n \n-    // Then: We get an appropriate error\n     expect(response.status).toBe(200)\n     expect(data).toHaveProperty('success', false)\n     expect(data).toHaveProperty('error')\n@@ -214,22 +186,203 @@ describe('File Parse API Route', () => {\n   })\n \n   it('should handle access errors gracefully', async () => {\n-    // Given: File access will fail\n     mockAccessFs.mockRejectedValueOnce(new Error('ENOENT: no such file'))\n \n-    // And: A request with a nonexistent file\n     const req = createMockRequest('POST', {\n       filePath: '/api/files/serve/nonexistent.txt',\n     })\n \n-    // When: The API processes the request\n     const { POST } = await import('./route')\n     const response = await POST(req)\n     const data = await response.json()\n \n-    // Then: We get an appropriate error response\n     expect(response.status).toBe(200)\n     expect(data).toHaveProperty('success')\n     expect(data).toHaveProperty('error')\n   })\n })\n+\n+describe('Files Parse API - Path Traversal Security', () => {\n+  beforeEach(() => {\n+    vi.clearAllMocks()\n+  })\n+\n+  describe('Path Traversal Prevention', () => {\n+    it('should reject path traversal attempts with .. segments', async () => {\n+      const maliciousRequests = [\n+        '../../../etc/passwd',\n+        '/api/files/serve/../../../etc/passwd',\n+        '/api/files/serve/../../app.js',\n+        '/api/files/serve/../.env',\n+        'uploads/../../../etc/hosts',\n+      ]\n+\n+      for (const maliciousPath of maliciousRequests) {\n+        const request = new NextRequest('http://localhost:3000/api/files/parse', {\n+          method: 'POST',\n+          body: JSON.stringify({\n+            filePath: maliciousPath,\n+          }),\n+        })\n+\n+        const response = await POST(request)\n+        const result = await response.json()\n+\n+        expect(result.success).toBe(false)\n+        expect(result.error).toMatch(/Access denied|Invalid path|Path outside allowed directory/)\n+      }\n+    })\n+\n+    it('should reject paths with tilde characters', async () => {\n+      const maliciousPaths = [\n+        '~/../../etc/passwd',\n+        '/api/files/serve/~/secret.txt',\n+        '~root/.ssh/id_rsa',\n+      ]\n+\n+      for (const maliciousPath of maliciousPaths) {\n+        const request = new NextRequest('http://localhost:3000/api/files/parse', {\n+          method: 'POST',\n+          body: JSON.stringify({\n+            filePath: maliciousPath,\n+          }),\n+        })\n+\n+        const response = await POST(request)\n+        const result = await response.json()\n+\n+        expect(result.success).toBe(false)\n+        expect(result.error).toMatch(/Access denied|Invalid path/)\n+      }\n+    })\n+\n+    it('should reject absolute paths outside upload directory', async () => {\n+      const maliciousPaths = [\n+        '/etc/passwd',\n+        '/root/.bashrc',\n+        '/app/.env',\n+        '/var/log/auth.log',\n+        'C:\\\\Windows\\\\System32\\\\drivers\\\\etc\\\\hosts', // Windows path\n+      ]\n+\n+      for (const maliciousPath of maliciousPaths) {\n+        const request = new NextRequest('http://localhost:3000/api/files/parse', {\n+          method: 'POST',\n+          body: JSON.stringify({\n+            filePath: maliciousPath,\n+          }),\n+        })\n+\n+        const response = await POST(request)\n+        const result = await response.json()\n+\n+        expect(result.success).toBe(false)\n+        expect(result.error).toMatch(/Access denied|Path outside allowed directory/)\n+      }\n+    })\n+\n+    it('should allow valid paths within upload directory', async () => {\n+      // Test that valid paths don't trigger path validation errors\n+      const validPaths = [\n+        '/api/files/serve/document.txt',\n+        '/api/files/serve/folder/file.pdf',\n+        '/api/files/serve/subfolder/image.png',\n+      ]\n+\n+      for (const validPath of validPaths) {\n+        const request = new NextRequest('http://localhost:3000/api/files/parse', {\n+          method: 'POST',\n+          body: JSON.stringify({\n+            filePath: validPath,\n+          }),\n+        })\n+\n+        const response = await POST(request)\n+        const result = await response.json()\n+\n+        // Should not fail due to path validation (may fail for other reasons like file not found)\n+        if (result.error) {\n+          expect(result.error).not.toMatch(\n+            /Access denied|Path outside allowed directory|Invalid path/\n+          )\n+        }\n+      }\n+    })\n+\n+    it('should handle encoded path traversal attempts', async () => {\n+      const encodedMaliciousPaths = [\n+        '/api/files/serve/%2e%2e%2f%2e%2e%2fetc%2fpasswd', // ../../../etc/passwd\n+        '/api/files/serve/..%2f..%2f..%2fetc%2fpasswd',\n+        '/api/files/serve/%2e%2e/%2e%2e/etc/passwd',\n+      ]\n+\n+      for (const maliciousPath of encodedMaliciousPaths) {\n+        const request = new NextRequest('http://localhost:3000/api/files/parse', {\n+          method: 'POST',\n+          body: JSON.stringify({\n+            filePath: decodeURIComponent(maliciousPath), // Simulate URL decoding\n+          }),\n+        })\n+\n+        const response = await POST(request)\n+        const result = await response.json()\n+\n+        expect(result.success).toBe(false)\n+        expect(result.error).toMatch(/Access denied|Invalid path|Path outside allowed directory/)\n+      }\n+    })\n+\n+    it('should handle null byte injection attempts', async () => {\n+      const nullBytePaths = [\n+        '/api/files/serve/file.txt\\0../../etc/passwd',\n+        'file.txt\\0/etc/passwd',\n+        '/api/files/serve/document.pdf\\0/var/log/auth.log',\n+      ]\n+\n+      for (const maliciousPath of nullBytePaths) {\n+        const request = new NextRequest('http://localhost:3000/api/files/parse', {\n+          method: 'POST',\n+          body: JSON.stringify({\n+            filePath: maliciousPath,\n+          }),\n+        })\n+\n+        const response = await POST(request)\n+        const result = await response.json()\n+\n+        expect(result.success).toBe(false)\n+        // Should be rejected either by path validation or file system access\n+      }\n+    })\n+  })\n+\n+  describe('Edge Cases', () => {\n+    it('should handle empty file paths', async () => {\n+      const request = new NextRequest('http://localhost:3000/api/files/parse', {\n+        method: 'POST',\n+        body: JSON.stringify({\n+          filePath: '',\n+        }),\n+      })\n+\n+      const response = await POST(request)\n+      const result = await response.json()\n+\n+      expect(response.status).toBe(400)\n+      expect(result.error).toBe('No file path provided')\n+    })\n+\n+    it('should handle missing filePath parameter', async () => {\n+      const request = new NextRequest('http://localhost:3000/api/files/parse', {\n+        method: 'POST',\n+        body: JSON.stringify({}),\n+      })\n+\n+      const response = await POST(request)\n+      const result = await response.json()\n+\n+      expect(response.status).toBe(400)\n+      expect(result.error).toBe('No file path provided')\n+    })\n+  })\n+})\ndiff --git a/apps/sim/app/api/files/parse/route.ts b/apps/sim/app/api/files/parse/route.ts\nindex e8211fa1e1..c635a815bd 100644\n--- a/apps/sim/app/api/files/parse/route.ts\n+++ b/apps/sim/app/api/files/parse/route.ts\n@@ -15,7 +15,6 @@ export const dynamic = 'force-dynamic'\n \n const logger = createLogger('FilesParseAPI')\n \n-// Constants for URL downloads\n const MAX_DOWNLOAD_SIZE_BYTES = 100 * 1024 * 1024 // 100 MB\n const DOWNLOAD_TIMEOUT_MS = 30000 // 30 seconds\n \n@@ -71,21 +70,6 @@ const fileTypeMap: Record<string, string> = {\n   zip: 'application/zip',\n }\n \n-// Binary file extensions\n-const _binaryExtensions = [\n-  'doc',\n-  'docx',\n-  'xls',\n-  'xlsx',\n-  'ppt',\n-  'pptx',\n-  'zip',\n-  'png',\n-  'jpg',\n-  'jpeg',\n-  'gif',\n-]\n-\n /**\n  * Main API route handler\n  */\n@@ -529,11 +513,57 @@ async function parseBufferAsPdf(buffer: Buffer) {\n   }\n }\n \n+/**\n+ * Validate that a file path is safe and within allowed directories\n+ */\n+function validateAndResolvePath(inputPath: string): {\n+  isValid: boolean\n+  resolvedPath?: string\n+  error?: string\n+} {\n+  try {\n+    let targetPath = inputPath\n+    if (inputPath.startsWith('/api/files/serve/')) {\n+      const filename = inputPath.replace('/api/files/serve/', '')\n+      targetPath = path.join(UPLOAD_DIR, filename)\n+    }\n+\n+    const resolvedPath = path.resolve(targetPath)\n+    const resolvedUploadDir = path.resolve(UPLOAD_DIR)\n+\n+    if (\n+      !resolvedPath.startsWith(resolvedUploadDir + path.sep) &&\n+      resolvedPath !== resolvedUploadDir\n+    ) {\n+      return {\n+        isValid: false,\n+        error: `Access denied: Path outside allowed directory`,\n+      }\n+    }\n+\n+    if (inputPath.includes('..') || inputPath.includes('~')) {\n+      return {\n+        isValid: false,\n+        error: `Access denied: Invalid path characters detected`,\n+      }\n+    }\n+\n+    return {\n+      isValid: true,\n+      resolvedPath,\n+    }\n+  } catch (error) {\n+    return {\n+      isValid: false,\n+      error: `Path validation error: ${(error as Error).message}`,\n+    }\n+  }\n+}\n+\n /**\n  * Handle a local file from the filesystem\n  */\n async function handleLocalFile(filePath: string, fileType?: string): Promise<ParseResult> {\n-  // Check if this is an S3 path that was incorrectly routed\n   if (filePath.includes('/api/files/serve/s3/')) {\n     logger.warn(`S3 path detected in handleLocalFile, redirecting to S3 handler: ${filePath}`)\n     return handleS3File(filePath, fileType)\n@@ -542,15 +572,19 @@ async function handleLocalFile(filePath: string, fileType?: string): Promise<Par\n   try {\n     logger.info(`Handling local file: ${filePath}`)\n \n-    // Extract the filename from the path for API serve paths\n-    let localFilePath = filePath\n-    if (filePath.startsWith('/api/files/serve/')) {\n-      const filename = filePath.replace('/api/files/serve/', '')\n-      localFilePath = path.join(UPLOAD_DIR, filename)\n-      logger.info(`Resolved API path to local file: ${localFilePath}`)\n+    const pathValidation = validateAndResolvePath(filePath)\n+    if (!pathValidation.isValid) {\n+      logger.error(`Path validation failed: ${pathValidation.error}`, { filePath })\n+      return {\n+        success: false,\n+        error: pathValidation.error || 'Invalid file path',\n+        filePath,\n+      }\n     }\n \n-    // Make sure the file is actually a file that exists\n+    const localFilePath = pathValidation.resolvedPath!\n+    logger.info(`Validated and resolved path: ${localFilePath}`)\n+\n     try {\n       await fsPromises.access(localFilePath, fsPromises.constants.R_OK)\n     } catch (error) {\ndiff --git a/apps/sim/app/api/function/execute/route.ts b/apps/sim/app/api/function/execute/route.ts\nindex 5063257a98..1d538ecc19 100644\n--- a/apps/sim/app/api/function/execute/route.ts\n+++ b/apps/sim/app/api/function/execute/route.ts\n@@ -38,7 +38,7 @@ function resolveCodeVariables(\n   for (const match of tagMatches) {\n     const tagName = match.slice(1, -1).trim()\n     const tagValue = params[tagName] || ''\n-    resolvedCode = resolvedCode.replace(match, tagValue)\n+    resolvedCode = resolvedCode.replace(match, JSON.stringify(tagValue))\n   }\n \n   return resolvedCode\n@@ -61,6 +61,14 @@ export async function POST(req: NextRequest) {\n       isCustomTool = false,\n     } = body\n \n+    logger.info(`[${requestId}] Function execution request`, {\n+      hasCode: !!code,\n+      paramsCount: Object.keys(params).length,\n+      timeout,\n+      workflowId,\n+      isCustomTool,\n+    })\n+\n     // Extract internal parameters that shouldn't be passed to the execution context\n     const executionParams = { ...params }\n     executionParams._context = undefined\n@@ -181,7 +189,7 @@ export async function POST(req: NextRequest) {\n               const errorMessage = `${args\n                 .map((arg) => (typeof arg === 'object' ? JSON.stringify(arg) : String(arg)))\n                 .join(' ')}\\n`\n-              logger.error(`[${requestId}] Code Console Error:`, errorMessage)\n+              logger.error(`[${requestId}] Code Console Error: ${errorMessage}`)\n               stdout += `ERROR: ${errorMessage}`\n             },\n           },\n@@ -234,7 +242,7 @@ export async function POST(req: NextRequest) {\n             const errorMessage = `${args\n               .map((arg) => (typeof arg === 'object' ? JSON.stringify(arg) : String(arg)))\n               .join(' ')}\\n`\n-            logger.error(`[${requestId}] Code Console Error:`, errorMessage)\n+            logger.error(`[${requestId}] Code Console Error: ${errorMessage}`)\n             stdout += `ERROR: ${errorMessage}`\n           },\n         },\ndiff --git a/apps/sim/app/api/providers/route.ts b/apps/sim/app/api/providers/route.ts\nindex 400b9e1a21..ee0cbc0157 100644\n--- a/apps/sim/app/api/providers/route.ts\n+++ b/apps/sim/app/api/providers/route.ts\n@@ -36,6 +36,7 @@ export async function POST(request: NextRequest) {\n       workflowId,\n       stream,\n       messages,\n+      environmentVariables,\n     } = body\n \n     logger.info(`[${requestId}] Provider request details`, {\n@@ -51,6 +52,8 @@ export async function POST(request: NextRequest) {\n       stream: !!stream,\n       hasMessages: !!messages?.length,\n       messageCount: messages?.length || 0,\n+      hasEnvironmentVariables:\n+        !!environmentVariables && Object.keys(environmentVariables).length > 0,\n     })\n \n     let finalApiKey: string\n@@ -89,6 +92,7 @@ export async function POST(request: NextRequest) {\n       workflowId,\n       stream,\n       messages,\n+      environmentVariables,\n     })\n \n     const executionTime = Date.now() - startTime\ndiff --git a/apps/sim/app/chat/[subdomain]/chat-client.tsx b/apps/sim/app/chat/[subdomain]/chat-client.tsx\nindex 85f56d6594..f9f27ba55a 100644\n--- a/apps/sim/app/chat/[subdomain]/chat-client.tsx\n+++ b/apps/sim/app/chat/[subdomain]/chat-client.tsx\n@@ -407,21 +407,6 @@ export default function ChatClient({ subdomain }: { subdomain: string }) {\n \n   return (\n     <div className='fixed inset-0 z-[100] flex flex-col bg-background'>\n-      <style jsx>{`\n-        @keyframes growShrink {\n-          0%,\n-          100% {\n-            transform: scale(0.9);\n-          }\n-          50% {\n-            transform: scale(1.1);\n-          }\n-        }\n-        .loading-dot {\n-          animation: growShrink 1.5s infinite ease-in-out;\n-        }\n-      `}</style>\n-\n       {/* Header component */}\n       <ChatHeader chatConfig={chatConfig} starCount={starCount} />\n \ndiff --git a/apps/sim/app/chat/[subdomain]/components/message-container/message-container.tsx b/apps/sim/app/chat/[subdomain]/components/message-container/message-container.tsx\nindex 44a41604af..6c59e578d2 100644\n--- a/apps/sim/app/chat/[subdomain]/components/message-container/message-container.tsx\n+++ b/apps/sim/app/chat/[subdomain]/components/message-container/message-container.tsx\n@@ -30,6 +30,21 @@ export const ChatMessageContainer = memo(function ChatMessageContainer({\n }: ChatMessageContainerProps) {\n   return (\n     <div className='relative flex flex-1 flex-col overflow-hidden bg-white'>\n+      <style jsx>{`\n+        @keyframes growShrink {\n+          0%,\n+          100% {\n+            transform: scale(0.9);\n+          }\n+          50% {\n+            transform: scale(1.1);\n+          }\n+        }\n+        .loading-dot {\n+          animation: growShrink 1.5s infinite ease-in-out;\n+        }\n+      `}</style>\n+\n       {/* Scrollable Messages Area */}\n       <div\n         ref={messagesContainerRef}\ndiff --git a/apps/sim/executor/handlers/agent/agent-handler.test.ts b/apps/sim/executor/handlers/agent/agent-handler.test.ts\nindex 4a3d1c48b0..8151c9b124 100644\n--- a/apps/sim/executor/handlers/agent/agent-handler.test.ts\n+++ b/apps/sim/executor/handlers/agent/agent-handler.test.ts\n@@ -1,6 +1,7 @@\n import { beforeEach, describe, expect, it, type Mock, vi } from 'vitest'\n import { isHosted } from '@/lib/environment'\n import { getAllBlocks } from '@/blocks'\n+import { executeProviderRequest } from '@/providers'\n import { getProviderFromModel, transformBlockTool } from '@/providers/utils'\n import type { SerializedBlock, SerializedWorkflow } from '@/serializer/types'\n import { executeTool } from '@/tools'\n@@ -21,6 +22,21 @@ vi.mock('@/providers/utils', () => ({\n   getProviderFromModel: vi.fn().mockReturnValue('mock-provider'),\n   transformBlockTool: vi.fn(),\n   getBaseModelProviders: vi.fn().mockReturnValue({ openai: {}, anthropic: {} }),\n+  getApiKey: vi.fn().mockReturnValue('mock-api-key'),\n+  getProvider: vi.fn().mockReturnValue({\n+    chat: {\n+      completions: {\n+        create: vi.fn().mockResolvedValue({\n+          content: 'Mocked response content',\n+          model: 'mock-model',\n+          tokens: { prompt: 10, completion: 20, total: 30 },\n+          toolCalls: [],\n+          cost: 0.001,\n+          timing: { total: 100 },\n+        }),\n+      },\n+    },\n+  }),\n }))\n \n vi.mock('@/blocks', () => ({\n@@ -31,6 +47,17 @@ vi.mock('@/tools', () => ({\n   executeTool: vi.fn(),\n }))\n \n+vi.mock('@/providers', () => ({\n+  executeProviderRequest: vi.fn().mockResolvedValue({\n+    content: 'Mocked response content',\n+    model: 'mock-model',\n+    tokens: { prompt: 10, completion: 20, total: 30 },\n+    toolCalls: [],\n+    cost: 0.001,\n+    timing: { total: 100 },\n+  }),\n+}))\n+\n global.fetch = Object.assign(vi.fn(), { preconnect: vi.fn() }) as typeof fetch\n \n const mockGetAllBlocks = getAllBlocks as Mock\n@@ -39,6 +66,7 @@ const mockIsHosted = isHosted as unknown as Mock\n const mockGetProviderFromModel = getProviderFromModel as Mock\n const mockTransformBlockTool = transformBlockTool as Mock\n const mockFetch = global.fetch as unknown as Mock\n+const mockExecuteProviderRequest = executeProviderRequest as Mock\n \n describe('AgentBlockHandler', () => {\n   let handler: AgentBlockHandler\n@@ -50,7 +78,12 @@ describe('AgentBlockHandler', () => {\n     handler = new AgentBlockHandler()\n     vi.clearAllMocks()\n \n-    // Save original Promise.all to restore later\n+    Object.defineProperty(global, 'window', {\n+      value: {},\n+      writable: true,\n+      configurable: true,\n+    })\n+\n     originalPromiseAll = Promise.all\n \n     mockBlock = {\n@@ -85,7 +118,7 @@ describe('AgentBlockHandler', () => {\n         loops: {},\n       } as SerializedWorkflow,\n     }\n-    mockIsHosted.mockReturnValue(false) // Default to non-hosted env for tests\n+    mockIsHosted.mockReturnValue(false)\n     mockGetProviderFromModel.mockReturnValue('mock-provider')\n \n     mockFetch.mockImplementation(() => {\n@@ -130,8 +163,15 @@ describe('AgentBlockHandler', () => {\n   })\n \n   afterEach(() => {\n-    // Restore original Promise.all\n     Promise.all = originalPromiseAll\n+\n+    try {\n+      Object.defineProperty(global, 'window', {\n+        value: undefined,\n+        writable: true,\n+        configurable: true,\n+      })\n+    } catch (e) {}\n   })\n \n   describe('canHandle', () => {\n@@ -164,7 +204,7 @@ describe('AgentBlockHandler', () => {\n         userPrompt: 'User query: Hello!',\n         temperature: 0.7,\n         maxTokens: 100,\n-        apiKey: 'test-api-key', // Add API key for non-hosted env\n+        apiKey: 'test-api-key',\n       }\n \n       mockGetProviderFromModel.mockReturnValue('openai')\n@@ -193,7 +233,6 @@ describe('AgentBlockHandler', () => {\n       Promise.all = vi.fn().mockImplementation((promises: Promise<any>[]) => {\n         const result = originalPromiseAll.call(Promise, promises)\n \n-        // When result resolves, capture the tools\n         result.then((tools: any[]) => {\n           if (tools?.length) {\n             capturedTools = tools.filter((t) => t !== null)\n@@ -255,7 +294,7 @@ describe('AgentBlockHandler', () => {\n                 },\n               },\n             },\n-            usageControl: 'auto',\n+            usageControl: 'auto' as const,\n           },\n           {\n             type: 'custom-tool',\n@@ -274,7 +313,7 @@ describe('AgentBlockHandler', () => {\n                 },\n               },\n             },\n-            usageControl: 'force',\n+            usageControl: 'force' as const,\n           },\n           {\n             type: 'custom-tool',\n@@ -293,7 +332,7 @@ describe('AgentBlockHandler', () => {\n                 },\n               },\n             },\n-            usageControl: 'none', // This tool should be filtered out\n+            usageControl: 'none' as const,\n           },\n         ],\n       }\n@@ -355,21 +394,21 @@ describe('AgentBlockHandler', () => {\n             title: 'Tool 1',\n             type: 'tool-type-1',\n             operation: 'operation1',\n-            usageControl: 'auto', // default setting\n+            usageControl: 'auto' as const,\n           },\n           {\n             id: 'tool_2',\n             title: 'Tool 2',\n             type: 'tool-type-2',\n             operation: 'operation2',\n-            usageControl: 'none', // should be filtered out\n+            usageControl: 'none' as const,\n           },\n           {\n             id: 'tool_3',\n             title: 'Tool 3',\n             type: 'tool-type-3',\n             operation: 'operation3',\n-            usageControl: 'force', // should be included\n+            usageControl: 'force' as const,\n           },\n         ],\n       }\n@@ -400,14 +439,14 @@ describe('AgentBlockHandler', () => {\n             title: 'Tool 1',\n             type: 'tool-type-1',\n             operation: 'operation1',\n-            usageControl: 'auto',\n+            usageControl: 'auto' as const,\n           },\n           {\n             id: 'tool_2',\n             title: 'Tool 2',\n             type: 'tool-type-2',\n             operation: 'operation2',\n-            usageControl: 'force',\n+            usageControl: 'force' as const,\n           },\n         ],\n       }\n@@ -449,7 +488,7 @@ describe('AgentBlockHandler', () => {\n                 },\n               },\n             },\n-            usageControl: 'auto',\n+            usageControl: 'auto' as const,\n           },\n           {\n             type: 'custom-tool',\n@@ -464,7 +503,7 @@ describe('AgentBlockHandler', () => {\n                 },\n               },\n             },\n-            usageControl: 'force',\n+            usageControl: 'force' as const,\n           },\n           {\n             type: 'custom-tool',\n@@ -479,7 +518,7 @@ describe('AgentBlockHandler', () => {\n                 },\n               },\n             },\n-            usageControl: 'none', // Should be filtered out\n+            usageControl: 'none' as const,\n           },\n         ],\n       }\n@@ -635,6 +674,8 @@ describe('AgentBlockHandler', () => {\n               model: 'mock-model',\n               tokens: { prompt: 10, completion: 20, total: 30 },\n               timing: { total: 100 },\n+              toolCalls: [],\n+              cost: undefined,\n             }),\n         })\n       })\n@@ -654,7 +695,9 @@ describe('AgentBlockHandler', () => {\n           result: 'Success',\n           score: 0.95,\n           tokens: { prompt: 10, completion: 20, total: 30 },\n+          toolCalls: { list: [], count: 0 },\n           providerTiming: { total: 100 },\n+          cost: undefined,\n         },\n       })\n     })\n@@ -729,23 +772,35 @@ describe('AgentBlockHandler', () => {\n     })\n \n     it('should handle streaming responses with text/event-stream content type', async () => {\n-      const mockStreamBody = {\n-        getReader: vi.fn().mockReturnValue({\n-          read: vi.fn().mockResolvedValue({ done: true, value: undefined }),\n-        }),\n-      }\n+      const mockStreamBody = new ReadableStream({\n+        start(controller) {\n+          controller.close()\n+        },\n+      })\n \n       mockFetch.mockImplementationOnce(() => {\n         return Promise.resolve({\n           ok: true,\n           headers: {\n             get: (name: string) => {\n-              if (name === 'Content-Type') return 'text/event-stream'\n+              if (name === 'Content-Type') return 'application/json'\n               if (name === 'X-Execution-Data') return null\n               return null\n             },\n           },\n-          body: mockStreamBody,\n+          json: () =>\n+            Promise.resolve({\n+              stream: mockStreamBody,\n+              execution: {\n+                success: true,\n+                output: { response: {} },\n+                logs: [],\n+                metadata: {\n+                  duration: 0,\n+                  startTime: new Date().toISOString(),\n+                },\n+              },\n+            }),\n         })\n       })\n \n@@ -771,11 +826,11 @@ describe('AgentBlockHandler', () => {\n     })\n \n     it('should handle streaming responses with execution data in header', async () => {\n-      const mockStreamBody = {\n-        getReader: vi.fn().mockReturnValue({\n-          read: vi.fn().mockResolvedValue({ done: true, value: undefined }),\n-        }),\n-      }\n+      const mockStreamBody = new ReadableStream({\n+        start(controller) {\n+          controller.close()\n+        },\n+      })\n \n       const mockExecutionData = {\n         success: true,\n@@ -807,12 +862,16 @@ describe('AgentBlockHandler', () => {\n           ok: true,\n           headers: {\n             get: (name: string) => {\n-              if (name === 'Content-Type') return 'text/event-stream'\n+              if (name === 'Content-Type') return 'application/json'\n               if (name === 'X-Execution-Data') return JSON.stringify(mockExecutionData)\n               return null\n             },\n           },\n-          body: mockStreamBody,\n+          json: () =>\n+            Promise.resolve({\n+              stream: mockStreamBody,\n+              execution: mockExecutionData,\n+            }),\n         })\n       })\n \ndiff --git a/apps/sim/executor/handlers/agent/agent-handler.ts b/apps/sim/executor/handlers/agent/agent-handler.ts\nindex 70984ae3d8..50a65de073 100644\n--- a/apps/sim/executor/handlers/agent/agent-handler.ts\n+++ b/apps/sim/executor/handlers/agent/agent-handler.ts\n@@ -2,14 +2,21 @@ import { env } from '@/lib/env'\n import { createLogger } from '@/lib/logs/console-logger'\n import { getAllBlocks } from '@/blocks'\n import type { BlockOutput } from '@/blocks/types'\n-import { getProviderFromModel, transformBlockTool } from '@/providers/utils'\n+import { executeProviderRequest } from '@/providers'\n+import { getApiKey, getProviderFromModel, transformBlockTool } from '@/providers/utils'\n import type { SerializedBlock } from '@/serializer/types'\n import { executeTool } from '@/tools'\n import { getTool, getToolAsync } from '@/tools/utils'\n import type { BlockHandler, ExecutionContext, StreamingExecution } from '../../types'\n+import type { AgentInputs, Message, StreamingConfig, ToolInput } from './types'\n \n const logger = createLogger('AgentBlockHandler')\n \n+const DEFAULT_MODEL = 'gpt-4o'\n+const DEFAULT_FUNCTION_TIMEOUT = 5000\n+const REQUEST_TIMEOUT = 120000\n+const CUSTOM_TOOL_PREFIX = 'custom_'\n+\n /**\n  * Handler for Agent blocks that process LLM requests with optional tools.\n  */\n@@ -20,698 +27,676 @@ export class AgentBlockHandler implements BlockHandler {\n \n   async execute(\n     block: SerializedBlock,\n-    inputs: Record<string, any>,\n+    inputs: AgentInputs,\n     context: ExecutionContext\n   ): Promise<BlockOutput | StreamingExecution> {\n     logger.info(`Executing agent block: ${block.id}`)\n \n-    // Parse response format if provided\n-    let responseFormat: any\n-    if (inputs.responseFormat) {\n-      // Handle empty string case - treat it as no response format\n-      if (inputs.responseFormat === '') {\n-        responseFormat = undefined\n-      } else {\n-        try {\n-          responseFormat =\n-            typeof inputs.responseFormat === 'string'\n-              ? JSON.parse(inputs.responseFormat)\n-              : inputs.responseFormat\n-\n-          // Ensure the responseFormat is properly structured\n-          if (responseFormat && typeof responseFormat === 'object') {\n-            // If it's just a raw schema without the expected wrapper properties,\n-            // wrap it properly for the provider\n-            if (!responseFormat.schema && !responseFormat.name) {\n-              responseFormat = {\n-                name: 'response_schema',\n-                schema: responseFormat,\n-                strict: true,\n-              }\n-            }\n+    const responseFormat = this.parseResponseFormat(inputs.responseFormat)\n+    const model = inputs.model || DEFAULT_MODEL\n+    const providerId = getProviderFromModel(model)\n+    const formattedTools = await this.formatTools(inputs.tools || [], context)\n+    const streamingConfig = this.getStreamingConfig(block, context)\n+    const messages = this.buildMessages(inputs)\n+\n+    const providerRequest = this.buildProviderRequest({\n+      providerId,\n+      model,\n+      messages,\n+      inputs,\n+      formattedTools,\n+      responseFormat,\n+      context,\n+      streaming: streamingConfig.shouldUseStreaming ?? false,\n+    })\n+\n+    this.logRequestDetails(providerRequest, messages, streamingConfig)\n+\n+    return this.executeProviderRequest(providerRequest, block, responseFormat, context)\n+  }\n+\n+  private parseResponseFormat(responseFormat?: string | object): any {\n+    if (!responseFormat || responseFormat === '') return undefined\n+\n+    try {\n+      const parsed =\n+        typeof responseFormat === 'string' ? JSON.parse(responseFormat) : responseFormat\n+\n+      if (parsed && typeof parsed === 'object' && !parsed.schema && !parsed.name) {\n+        return {\n+          name: 'response_schema',\n+          schema: parsed,\n+          strict: true,\n+        }\n+      }\n+      return parsed\n+    } catch (error: any) {\n+      logger.error('Failed to parse response format:', { error })\n+      throw new Error(`Invalid response format: ${error.message}`)\n+    }\n+  }\n+\n+  private async formatTools(inputTools: ToolInput[], context: ExecutionContext): Promise<any[]> {\n+    if (!Array.isArray(inputTools)) return []\n+\n+    const tools = await Promise.all(\n+      inputTools\n+        .filter((tool) => {\n+          const usageControl = tool.usageControl || 'auto'\n+          return usageControl !== 'none'\n+        })\n+        .map(async (tool) => {\n+          if (tool.type === 'custom-tool' && tool.schema) {\n+            return this.createCustomTool(tool, context)\n           }\n-        } catch (error: any) {\n-          logger.error('Failed to parse response format:', { error })\n-          throw new Error(`Invalid response format: ${error.message}`)\n+          return this.transformBlockTool(tool, context)\n+        })\n+    )\n+\n+    return tools.filter(\n+      (tool): tool is NonNullable<typeof tool> => tool !== null && tool !== undefined\n+    )\n+  }\n+\n+  private createCustomTool(tool: ToolInput, context: ExecutionContext): any {\n+    const base: any = {\n+      id: `${CUSTOM_TOOL_PREFIX}${tool.title}`,\n+      name: tool.schema.function.name,\n+      description: tool.schema.function.description || '',\n+      params: tool.params || {},\n+      parameters: {\n+        type: tool.schema.function.parameters.type,\n+        properties: tool.schema.function.parameters.properties,\n+        required: tool.schema.function.parameters.required || [],\n+      },\n+      usageControl: tool.usageControl || 'auto',\n+    }\n+\n+    if (tool.code) {\n+      base.executeFunction = async (callParams: Record<string, any>) => {\n+        const result = await executeTool('function_execute', {\n+          code: tool.code,\n+          ...tool.params,\n+          ...callParams,\n+          timeout: tool.timeout ?? DEFAULT_FUNCTION_TIMEOUT,\n+          envVars: context.environmentVariables || {},\n+          isCustomTool: true,\n+          _context: { workflowId: context.workflowId },\n+        })\n+\n+        if (!result.success) {\n+          throw new Error(result.error || 'Function execution failed')\n         }\n+        return result.output\n       }\n     }\n \n-    const model = inputs.model || 'gpt-4o'\n-    const providerId = getProviderFromModel(model)\n-    logger.info(`Using provider: ${providerId}, model: ${model}`)\n-\n-    // Format tools for provider API\n-    const formattedTools = Array.isArray(inputs.tools)\n-      ? (\n-          await Promise.all(\n-            // First filter out any tools with usageControl set to 'none'\n-            inputs.tools\n-              .filter((tool: any) => {\n-                const usageControl = tool.usageControl || 'auto'\n-                if (usageControl === 'none') {\n-                  logger.info(`Filtering out tool set to 'none': ${tool.title || tool.type}`)\n-                  return false\n-                }\n-                return true\n-              })\n-              .map(async (tool: any) => {\n-                // Handle custom tools\n-                if (tool.type === 'custom-tool' && tool.schema) {\n-                  // Add function execution capability to custom tools with code\n-                  if (tool.code) {\n-                    // Store the tool's code and make it available for execution\n-                    const toolName = tool.schema.function.name\n-                    const params = tool.params || {}\n-\n-                    // Create a tool that can execute the code\n-                    return {\n-                      id: `custom_${tool.title}`,\n-                      name: toolName,\n-                      description: tool.schema.function.description || '',\n-                      params: params,\n-                      parameters: {\n-                        type: tool.schema.function.parameters.type,\n-                        properties: tool.schema.function.parameters.properties,\n-                        required: tool.schema.function.parameters.required || [],\n-                      },\n-                      usageControl: tool.usageControl || 'auto',\n-                      executeFunction: async (callParams: Record<string, any>) => {\n-                        try {\n-                          // Execute the code using the function_execute tool\n-                          const result = await executeTool('function_execute', {\n-                            code: tool.code,\n-                            ...params,\n-                            ...callParams,\n-                            timeout: tool.timeout || 5000,\n-                          })\n-\n-                          if (!result.success) {\n-                            throw new Error(result.error || 'Function execution failed')\n-                          }\n-\n-                          return result.output\n-                        } catch (error: any) {\n-                          logger.error(`Error executing custom tool ${toolName}:`, error)\n-                          throw new Error(`Error in ${toolName}: ${error.message}`)\n-                        }\n-                      },\n-                    }\n-                  }\n-\n-                  return {\n-                    id: `custom_${tool.title}`,\n-                    name: tool.schema.function.name,\n-                    description: tool.schema.function.description || '',\n-                    params: tool.params || {},\n-                    parameters: {\n-                      type: tool.schema.function.parameters.type,\n-                      properties: tool.schema.function.parameters.properties,\n-                      required: tool.schema.function.parameters.required || [],\n-                    },\n-                    usageControl: tool.usageControl || 'auto',\n-                  }\n-                }\n-\n-                // Handle regular block tools with operation selection\n-                const transformedTool = await transformBlockTool(tool, {\n-                  selectedOperation: tool.operation,\n-                  getAllBlocks,\n-                  getToolAsync: (toolId: string) => getToolAsync(toolId, context.workflowId),\n-                  getTool,\n-                })\n-\n-                // Add usageControl to the transformed tool if it exists\n-                if (transformedTool) {\n-                  transformedTool.usageControl = tool.usageControl || 'auto'\n-                }\n-\n-                return transformedTool\n-              })\n-          )\n-        ).filter((t: any): t is NonNullable<typeof t> => t !== null)\n-      : []\n-\n-    // Check if streaming is requested and this block is selected for streaming\n+    return base\n+  }\n+\n+  private async transformBlockTool(tool: ToolInput, context: ExecutionContext) {\n+    const transformedTool = await transformBlockTool(tool, {\n+      selectedOperation: tool.operation,\n+      getAllBlocks,\n+      getToolAsync: (toolId: string) => getToolAsync(toolId, context.workflowId),\n+      getTool,\n+    })\n+\n+    if (transformedTool) {\n+      transformedTool.usageControl = tool.usageControl || 'auto'\n+    }\n+    return transformedTool\n+  }\n+\n+  private getStreamingConfig(block: SerializedBlock, context: ExecutionContext): StreamingConfig {\n     const isBlockSelectedForOutput =\n       context.selectedOutputIds?.some((outputId) => {\n-        // First check for direct match (if the entire outputId is the blockId)\n-        if (outputId === block.id) {\n-          logger.info(`Direct match found for block ${block.id} in selected outputs`)\n-          return true\n-        }\n-\n-        // Then try parsing the blockId from the blockId_path format\n+        if (outputId === block.id) return true\n         const firstUnderscoreIndex = outputId.indexOf('_')\n-        if (firstUnderscoreIndex !== -1) {\n-          const blockId = outputId.substring(0, firstUnderscoreIndex)\n-          const isMatch = blockId === block.id\n-          if (isMatch) {\n-            logger.info(\n-              `Path match found for block ${block.id} in selected outputs (from ${outputId})`\n-            )\n-          }\n-          return isMatch\n-        }\n-        return false\n+        return (\n+          firstUnderscoreIndex !== -1 && outputId.substring(0, firstUnderscoreIndex) === block.id\n+        )\n       }) ?? false\n \n-    // Check if this block has any outgoing connections\n     const hasOutgoingConnections = context.edges?.some((edge) => edge.source === block.id) ?? false\n-\n-    // Determine if we should use streaming for this block\n-    const shouldUseStreaming = context.stream && isBlockSelectedForOutput\n+    const shouldUseStreaming = Boolean(context.stream) && isBlockSelectedForOutput\n \n     if (shouldUseStreaming) {\n-      logger.info(\n-        `Block ${block.id} will use streaming response (selected for output with no outgoing connections)`\n-      )\n+      logger.info(`Block ${block.id} will use streaming response`)\n     }\n \n-    // Initialize parsedMessages - will be built from memories/prompts if provided\n-    let parsedMessages: any[] | undefined\n+    return { shouldUseStreaming, isBlockSelectedForOutput, hasOutgoingConnections }\n+  }\n \n-    // Check if we're in advanced mode with the memories field\n-    if (inputs.memories || (inputs.systemPrompt && inputs.userPrompt)) {\n-      const messages: any[] = []\n+  private buildMessages(inputs: AgentInputs): Message[] | undefined {\n+    if (!inputs.memories && !(inputs.systemPrompt && inputs.userPrompt)) {\n+      return undefined\n+    }\n \n-      if (inputs.memories) {\n-        const memories = inputs.memories\n+    const messages: Message[] = []\n \n-        const memoryMessages = processMemories(memories, logger)\n-        messages.push(...memoryMessages)\n-      }\n+    if (inputs.memories) {\n+      messages.push(...this.processMemories(inputs.memories))\n+    }\n \n-      // Handle system prompt with clear precedence rules\n-      if (inputs.systemPrompt) {\n-        // Check for existing system messages in memories\n-        const systemMessages = messages.filter((msg) => msg.role === 'system')\n-\n-        if (systemMessages.length > 1) {\n-          logger.warn(\n-            `Found ${systemMessages.length} system messages in memories. Explicit systemPrompt will take precedence.`\n-          )\n-        } else if (systemMessages.length === 1) {\n-          logger.info(\n-            'Found system message in memories. Explicit systemPrompt will take precedence.'\n-          )\n-        }\n+    if (inputs.systemPrompt) {\n+      this.addSystemPrompt(messages, inputs.systemPrompt)\n+    }\n \n-        // Remove any existing system messages and add the explicit one at the beginning\n-        messages.splice(0, 0, {\n-          role: 'system',\n-          content: inputs.systemPrompt,\n-        })\n+    if (inputs.userPrompt) {\n+      this.addUserPrompt(messages, inputs.userPrompt)\n+    }\n \n-        // Remove any other system messages that came from memories\n-        for (let i = messages.length - 1; i >= 1; i--) {\n-          if (messages[i].role === 'system') {\n-            messages.splice(i, 1)\n-          }\n-        }\n+    return messages.length > 0 ? messages : undefined\n+  }\n \n-        logger.info(\n-          'Added explicit system prompt as first message, removed any system messages from memories'\n-        )\n-      } else {\n-        // No explicit system prompt provided, check for multiple system messages in memories\n-        const systemMessages = messages.filter((msg) => msg.role === 'system')\n-\n-        if (systemMessages.length > 1) {\n-          logger.warn(\n-            `Found ${systemMessages.length} system messages in memories with no explicit systemPrompt. Consider providing an explicit systemPrompt for consistent behavior.`\n-          )\n-        } else if (systemMessages.length === 1) {\n-          logger.info('Using system message from memories')\n-        }\n-      }\n+  private processMemories(memories: any): Message[] {\n+    if (!memories) return []\n \n-      if (inputs.userPrompt) {\n-        let userContent = inputs.userPrompt\n-        if (typeof userContent === 'object' && userContent.input) {\n-          userContent = userContent.input\n-        } else if (typeof userContent === 'object') {\n-          userContent = JSON.stringify(userContent)\n-        }\n+    let memoryArray: any[] = []\n+    if (memories?.response?.memories && Array.isArray(memories.response.memories)) {\n+      memoryArray = memories.response.memories\n+    } else if (memories?.memories && Array.isArray(memories.memories)) {\n+      memoryArray = memories.memories\n+    } else if (Array.isArray(memories)) {\n+      memoryArray = memories\n+    }\n \n+    const messages: Message[] = []\n+    memoryArray.forEach((memory: any) => {\n+      if (memory.data && Array.isArray(memory.data)) {\n+        memory.data.forEach((msg: any) => {\n+          if (msg.role && msg.content && ['system', 'user', 'assistant'].includes(msg.role)) {\n+            messages.push({\n+              role: msg.role as 'system' | 'user' | 'assistant',\n+              content: msg.content,\n+            })\n+          }\n+        })\n+      } else if (\n+        memory.role &&\n+        memory.content &&\n+        ['system', 'user', 'assistant'].includes(memory.role)\n+      ) {\n         messages.push({\n-          role: 'user',\n-          content: userContent,\n+          role: memory.role as 'system' | 'user' | 'assistant',\n+          content: memory.content,\n         })\n-        logger.info('Added user prompt to messages', { contentType: typeof userContent })\n       }\n+    })\n \n-      if (messages.length > 0) {\n-        parsedMessages = messages\n-        logger.info('Built messages from advanced mode', {\n-          messageCount: messages.length,\n-          firstMessage: messages[0],\n-          lastMessage: messages[messages.length - 1],\n-        })\n+    return messages\n+  }\n+\n+  private addSystemPrompt(messages: Message[], systemPrompt: string) {\n+    const systemMessages = messages.filter((msg) => msg.role === 'system')\n+\n+    if (systemMessages.length > 0) {\n+      messages.splice(0, 0, { role: 'system', content: systemPrompt })\n+      for (let i = messages.length - 1; i >= 1; i--) {\n+        if (messages[i].role === 'system') {\n+          messages.splice(i, 1)\n+        }\n       }\n+    } else {\n+      messages.splice(0, 0, { role: 'system', content: systemPrompt })\n     }\n+  }\n \n-    // Fast validation of parsed messages\n-    const validMessages =\n-      Array.isArray(parsedMessages) &&\n-      parsedMessages.length > 0 &&\n-      parsedMessages.every(\n-        (msg) =>\n-          typeof msg === 'object' &&\n-          msg !== null &&\n-          'role' in msg &&\n-          typeof msg.role === 'string' &&\n-          ('content' in msg ||\n-            (msg.role === 'assistant' && ('function_call' in msg || 'tool_calls' in msg)))\n-      )\n-\n-    if (Array.isArray(parsedMessages) && parsedMessages.length > 0 && !validMessages) {\n-      logger.warn('Messages array has invalid format:', {\n-        messageCount: parsedMessages.length,\n-      })\n-    } else if (validMessages) {\n-      logger.info('Messages validated successfully')\n+  private addUserPrompt(messages: Message[], userPrompt: any) {\n+    let content = userPrompt\n+    if (typeof userPrompt === 'object' && userPrompt.input) {\n+      content = userPrompt.input\n+    } else if (typeof userPrompt === 'object') {\n+      content = JSON.stringify(userPrompt)\n     }\n \n-    // Debug request before sending to provider\n-    const providerRequest = {\n+    messages.push({ role: 'user', content })\n+  }\n+\n+  private buildProviderRequest(config: {\n+    providerId: string\n+    model: string\n+    messages: Message[] | undefined\n+    inputs: AgentInputs\n+    formattedTools: any[]\n+    responseFormat: any\n+    context: ExecutionContext\n+    streaming: boolean\n+  }) {\n+    const {\n+      providerId,\n+      model,\n+      messages,\n+      inputs,\n+      formattedTools,\n+      responseFormat,\n+      context,\n+      streaming,\n+    } = config\n+\n+    const validMessages = this.validateMessages(messages)\n+\n+    return {\n       provider: providerId,\n       model,\n-      // If messages are provided (advanced mode), use them exclusively and skip systemPrompt/context\n-      ...(validMessages\n-        ? { messages: parsedMessages }\n-        : {\n-            systemPrompt: inputs.systemPrompt,\n-            context: inputs.userPrompt\n-              ? Array.isArray(inputs.userPrompt)\n-                ? JSON.stringify(inputs.userPrompt, null, 2)\n-                : typeof inputs.userPrompt === 'string'\n-                  ? inputs.userPrompt\n-                  : JSON.stringify(inputs.userPrompt, null, 2)\n-              : undefined,\n-          }),\n-      tools: formattedTools.length > 0 ? formattedTools : undefined,\n+      systemPrompt: validMessages ? undefined : inputs.systemPrompt,\n+      context: JSON.stringify(messages),\n+      tools: formattedTools,\n       temperature: inputs.temperature,\n       maxTokens: inputs.maxTokens,\n       apiKey: inputs.apiKey,\n       responseFormat,\n       workflowId: context.workflowId,\n-      stream: shouldUseStreaming,\n+      stream: streaming,\n+      messages,\n+      environmentVariables: context.environmentVariables || {},\n     }\n+  }\n+\n+  private validateMessages(messages: Message[] | undefined): boolean {\n+    return (\n+      Array.isArray(messages) &&\n+      messages.length > 0 &&\n+      messages.every(\n+        (msg: any) =>\n+          typeof msg === 'object' &&\n+          msg !== null &&\n+          'role' in msg &&\n+          typeof msg.role === 'string' &&\n+          ('content' in msg ||\n+            (msg.role === 'assistant' && ('function_call' in msg || 'tool_calls' in msg)))\n+      )\n+    )\n+  }\n \n+  private logRequestDetails(\n+    providerRequest: any,\n+    messages: Message[] | undefined,\n+    streamingConfig: StreamingConfig\n+  ) {\n     logger.info('Provider request prepared', {\n       model: providerRequest.model,\n-      hasMessages: Array.isArray(parsedMessages) && parsedMessages.length > 0,\n-      hasSystemPrompt:\n-        !(Array.isArray(parsedMessages) && parsedMessages.length > 0) && !!inputs.systemPrompt,\n-      hasContext:\n-        !(Array.isArray(parsedMessages) && parsedMessages.length > 0) && !!inputs.userPrompt,\n+      hasMessages: !!messages?.length,\n+      hasSystemPrompt: !messages?.length && !!providerRequest.systemPrompt,\n+      hasContext: !messages?.length && !!providerRequest.context,\n       hasTools: !!providerRequest.tools,\n       hasApiKey: !!providerRequest.apiKey,\n       workflowId: providerRequest.workflowId,\n-      stream: shouldUseStreaming,\n-      isBlockSelectedForOutput,\n-      hasOutgoingConnections,\n-      // Debug info about messages to help diagnose issues\n-      messagesProvided: 'messages' in providerRequest,\n-      messagesCount:\n-        'messages' in providerRequest && Array.isArray(providerRequest.messages)\n-          ? providerRequest.messages.length\n-          : 0,\n+      stream: providerRequest.stream,\n+      messagesCount: messages?.length || 0,\n     })\n+  }\n \n-    const baseUrl = env.NEXT_PUBLIC_APP_URL || ''\n-    const url = new URL('/api/providers', baseUrl)\n+  private async executeProviderRequest(\n+    providerRequest: any,\n+    block: SerializedBlock,\n+    responseFormat: any,\n+    context: ExecutionContext\n+  ): Promise<BlockOutput | StreamingExecution> {\n+    const providerId = providerRequest.provider\n+    const model = providerRequest.model\n+    const providerStartTime = Date.now()\n \n     try {\n-      logger.info(`Making provider request to: ${url.toString()}`, {\n-        workflowId: context.workflowId,\n-        blockId: block.id,\n-        provider: providerId,\n-        model,\n-        timestamp: new Date().toISOString(),\n-      })\n+      const isBrowser = typeof window !== 'undefined'\n+\n+      if (!isBrowser) {\n+        return this.executeServerSide(\n+          providerRequest,\n+          providerId,\n+          model,\n+          block,\n+          responseFormat,\n+          context,\n+          providerStartTime\n+        )\n+      }\n+      return this.executeBrowserSide(\n+        providerRequest,\n+        block,\n+        responseFormat,\n+        context,\n+        providerStartTime\n+      )\n+    } catch (error) {\n+      this.handleExecutionError(error, providerStartTime, providerId, model, context, block)\n+      throw error\n+    }\n+  }\n \n-      const response = await fetch(url.toString(), {\n-        method: 'POST',\n-        headers: {\n-          'Content-Type': 'application/json',\n-        },\n-        body: JSON.stringify(providerRequest),\n-        // Add timeout and signal for better error handling\n-        signal: AbortSignal.timeout(120000), // 2 minute timeout\n-      })\n+  private async executeServerSide(\n+    providerRequest: any,\n+    providerId: string,\n+    model: string,\n+    block: SerializedBlock,\n+    responseFormat: any,\n+    context: ExecutionContext,\n+    providerStartTime: number\n+  ) {\n+    logger.info('Using direct provider execution (server environment)')\n \n-      if (!response.ok) {\n-        // Try to extract a helpful error message\n-        let errorMessage = `Provider API request failed with status ${response.status}`\n-        let errorDetails = null\n+    const finalApiKey = this.getApiKey(providerId, model, providerRequest.apiKey)\n \n-        try {\n-          const errorData = await response.json()\n-          if (errorData.error) {\n-            errorMessage = errorData.error\n-            errorDetails = errorData\n-          }\n-        } catch (_e) {\n-          // If JSON parsing fails, try to get text response\n-          try {\n-            const textError = await response.text()\n-            if (textError) {\n-              errorDetails = { textResponse: textError }\n-            }\n-          } catch (_textError) {\n-            // If text parsing also fails, use the original error message\n-          }\n-        }\n+    const response = await executeProviderRequest(providerId, {\n+      model,\n+      systemPrompt: 'systemPrompt' in providerRequest ? providerRequest.systemPrompt : undefined,\n+      context: 'context' in providerRequest ? providerRequest.context : undefined,\n+      tools: providerRequest.tools,\n+      temperature: providerRequest.temperature,\n+      maxTokens: providerRequest.maxTokens,\n+      apiKey: finalApiKey,\n+      responseFormat: providerRequest.responseFormat,\n+      workflowId: providerRequest.workflowId,\n+      stream: providerRequest.stream,\n+      messages: 'messages' in providerRequest ? providerRequest.messages : undefined,\n+      environmentVariables: context.environmentVariables || {},\n+    })\n \n-        logger.error('Provider API request failed', {\n-          workflowId: context.workflowId,\n-          blockId: block.id,\n-          status: response.status,\n-          statusText: response.statusText,\n-          url: url.toString(),\n-          errorMessage,\n-          errorDetails,\n-          headers: Object.fromEntries(response.headers.entries()),\n-        })\n+    this.logExecutionSuccess(providerId, model, context, block, providerStartTime, response)\n+    return this.processProviderResponse(response, block, responseFormat)\n+  }\n \n-        throw new Error(errorMessage)\n-      }\n+  private async executeBrowserSide(\n+    providerRequest: any,\n+    block: SerializedBlock,\n+    responseFormat: any,\n+    context: ExecutionContext,\n+    providerStartTime: number\n+  ) {\n+    logger.info('Using HTTP provider request (browser environment)')\n+\n+    const url = new URL('/api/providers', env.NEXT_PUBLIC_APP_URL || '')\n+    const response = await fetch(url.toString(), {\n+      method: 'POST',\n+      headers: { 'Content-Type': 'application/json' },\n+      body: JSON.stringify(providerRequest),\n+      signal: AbortSignal.timeout(REQUEST_TIMEOUT),\n+    })\n \n-      // Check if we're getting a streaming response\n-      const contentType = response.headers.get('Content-Type')\n-      if (contentType?.includes('text/event-stream')) {\n-        logger.info(`Received streaming response for block ${block.id}`)\n+    if (!response.ok) {\n+      const errorMessage = await this.extractErrorMessage(response)\n+      throw new Error(errorMessage)\n+    }\n \n-        // Ensure we have a valid body stream\n-        if (!response.body) {\n-          throw new Error(`No response body in streaming response for block ${block.id}`)\n-        }\n+    this.logExecutionSuccess(\n+      providerRequest.provider,\n+      providerRequest.model,\n+      context,\n+      block,\n+      providerStartTime,\n+      'HTTP response'\n+    )\n+\n+    // Check if this is a streaming response\n+    const contentType = response.headers.get('Content-Type')\n+    if (contentType?.includes('text/event-stream')) {\n+      // Handle streaming response\n+      return this.handleStreamingResponse(response, block)\n+    }\n \n-        // Check if we have execution data in the header\n-        const executionDataHeader = response.headers.get('X-Execution-Data')\n-        if (executionDataHeader) {\n-          try {\n-            // Parse the execution data from the header\n-            const executionData = JSON.parse(executionDataHeader)\n-\n-            // Add block-specific data to the execution logs if needed\n-            if (executionData?.logs) {\n-              for (const log of executionData.logs) {\n-                if (!log.blockId) log.blockId = block.id\n-                if (!log.blockName && block.metadata?.name) log.blockName = block.metadata.name\n-                if (!log.blockType && block.metadata?.id) log.blockType = block.metadata.id\n-              }\n-            }\n-\n-            // Add block metadata to the execution data if missing\n-            if (executionData.output?.response) {\n-              // Ensure model and block info is set\n-              if (block.metadata?.name && !executionData.blockName) {\n-                executionData.blockName = block.metadata.name\n-              }\n-              if (block.metadata?.id && !executionData.blockType) {\n-                executionData.blockType = block.metadata.id\n-              }\n-              if (!executionData.blockId) {\n-                executionData.blockId = block.id\n-              }\n-\n-              // Add explicit streaming flag to make it easier to identify streaming executions\n-              executionData.isStreaming = true\n-            }\n-\n-            // Return both the stream and the execution data as separate properties\n-            const streamingExecution: StreamingExecution = {\n-              stream: response.body,\n-              execution: executionData,\n-            }\n-            return streamingExecution\n-          } catch (error) {\n-            logger.error(`Error parsing execution data header: ${error}`)\n-            // Continue with just the stream if there's an error\n-          }\n-        }\n+    // Handle regular JSON response\n+    const result = await response.json()\n+    return this.processProviderResponse(result, block, responseFormat)\n+  }\n \n-        // No execution data in header, just return the stream\n-        // Create a minimal StreamingExecution with empty execution data\n-        const minimalExecution: StreamingExecution = {\n-          stream: response.body,\n+  private async handleStreamingResponse(\n+    response: Response,\n+    block: SerializedBlock\n+  ): Promise<StreamingExecution> {\n+    // Check if we have execution data in headers (from StreamingExecution)\n+    const executionDataHeader = response.headers.get('X-Execution-Data')\n+\n+    if (executionDataHeader) {\n+      // Parse execution data from header\n+      try {\n+        const executionData = JSON.parse(executionDataHeader)\n+\n+        // Create StreamingExecution object\n+        return {\n+          stream: response.body!,\n           execution: {\n-            success: true,\n-            output: { response: {} },\n-            logs: [],\n-            metadata: {\n+            success: executionData.success,\n+            output: executionData.output || { response: {} },\n+            error: executionData.error,\n+            logs: [], // Logs are stripped from headers, will be populated by executor\n+            metadata: executionData.metadata || {\n               duration: 0,\n               startTime: new Date().toISOString(),\n             },\n-          },\n+            isStreaming: true,\n+            blockId: block.id,\n+            blockName: block.metadata?.name,\n+            blockType: block.metadata?.id,\n+          } as any,\n         }\n-        return minimalExecution\n+      } catch (error) {\n+        logger.error('Failed to parse execution data from header:', error)\n+        // Fall back to minimal streaming execution\n       }\n+    }\n \n-      // Check if we have a combined response with both stream and execution data\n-      const result = await response.json()\n-\n-      if (result && typeof result === 'object' && 'stream' in result && 'execution' in result) {\n-        logger.info(`Received combined streaming response for block ${block.id}`)\n-\n-        // Get the stream as a ReadableStream (need to convert from serialized format)\n-        const stream = new ReadableStream({\n-          start(controller) {\n-            // Since stream was serialized as JSON, we need to reconstruct it\n-            // For now, we'll just use a placeholder message\n-            const encoder = new TextEncoder()\n-            controller.enqueue(\n-              encoder.encode(\n-                'Stream data cannot be serialized as JSON. You will need to return a proper stream.'\n-              )\n-            )\n-            controller.close()\n-          },\n-        })\n+    // Fallback for plain ReadableStream or when header parsing fails\n+    return this.createMinimalStreamingExecution(response.body!)\n+  }\n \n-        // Return both in a format the executor can handle\n-        const streamingExecution: StreamingExecution = {\n-          stream,\n-          execution: result.execution,\n-        }\n-        return streamingExecution\n+  private getApiKey(providerId: string, model: string, inputApiKey: string): string {\n+    try {\n+      return getApiKey(providerId, model, inputApiKey)\n+    } catch (error) {\n+      logger.error('Failed to get API key:', {\n+        provider: providerId,\n+        model,\n+        error: error instanceof Error ? error.message : String(error),\n+        hasProvidedApiKey: !!inputApiKey,\n+      })\n+      throw new Error(error instanceof Error ? error.message : 'API key error')\n+    }\n+  }\n+\n+  private async extractErrorMessage(response: Response): Promise<string> {\n+    let errorMessage = `Provider API request failed with status ${response.status}`\n+    try {\n+      const errorData = await response.json()\n+      if (errorData.error) {\n+        errorMessage = errorData.error\n       }\n+    } catch (_e) {\n+      // Use default message if JSON parsing fails\n+    }\n+    return errorMessage\n+  }\n \n-      logger.info('Provider response received', {\n-        contentLength: result.content ? result.content.length : 0,\n-        model: result.model,\n-        hasTokens: !!result.tokens,\n-        hasToolCalls: !!result.toolCalls,\n-        toolCallsCount: result.toolCalls?.length || 0,\n-      })\n+  private logExecutionSuccess(\n+    provider: string,\n+    model: string,\n+    context: ExecutionContext,\n+    block: SerializedBlock,\n+    startTime: number,\n+    response: any\n+  ) {\n+    const executionTime = Date.now() - startTime\n+    const responseType =\n+      response instanceof ReadableStream\n+        ? 'stream'\n+        : response && typeof response === 'object' && 'stream' in response\n+          ? 'streaming-execution'\n+          : 'json'\n+\n+    logger.info('Provider request completed successfully', {\n+      provider,\n+      model,\n+      workflowId: context.workflowId,\n+      blockId: block.id,\n+      executionTime,\n+      responseType,\n+    })\n+  }\n \n-      // If structured responses, try to parse the content\n-      if (responseFormat) {\n-        try {\n-          const parsedContent = JSON.parse(result.content)\n-\n-          const responseResult = {\n-            response: {\n-              ...parsedContent,\n-              tokens: result.tokens || {\n-                prompt: 0,\n-                completion: 0,\n-                total: 0,\n-              },\n-              toolCalls: result.toolCalls\n-                ? {\n-                    list: result.toolCalls.map((tc: any) => ({\n-                      ...tc,\n-                      // Strip the 'custom_' prefix from tool names for display\n-                      name: stripCustomToolPrefix(tc.name),\n-                      // Preserve timing information if available\n-                      startTime: tc.startTime,\n-                      endTime: tc.endTime,\n-                      duration: tc.duration,\n-                      input: tc.arguments || tc.input,\n-                      output: tc.result || tc.output,\n-                    })),\n-                    count: result.toolCalls.length,\n-                  }\n-                : undefined,\n-              providerTiming: result.timing || undefined,\n-              cost: result.cost || undefined,\n-            },\n-          }\n+  private handleExecutionError(\n+    error: any,\n+    startTime: number,\n+    provider: string,\n+    model: string,\n+    context: ExecutionContext,\n+    block: SerializedBlock\n+  ) {\n+    const executionTime = Date.now() - startTime\n+\n+    logger.error('Error executing provider request:', {\n+      error,\n+      executionTime,\n+      provider,\n+      model,\n+      workflowId: context.workflowId,\n+      blockId: block.id,\n+    })\n \n-          return responseResult\n-        } catch (error) {\n-          logger.error('Failed to parse response content:', { error })\n-          logger.info('Falling back to standard response format')\n-\n-          // Fall back to standard response if parsing fails\n-          return {\n-            response: {\n-              content: result.content,\n-              model: result.model,\n-              tokens: result.tokens || {\n-                prompt: 0,\n-                completion: 0,\n-                total: 0,\n-              },\n-              toolCalls: {\n-                list: result.toolCalls\n-                  ? result.toolCalls.map((tc: any) => ({\n-                      ...tc,\n-                      // Strip the 'custom_' prefix from tool names for display\n-                      name: stripCustomToolPrefix(tc.name),\n-                      // Preserve timing information if available\n-                      startTime: tc.startTime,\n-                      endTime: tc.endTime,\n-                      duration: tc.duration,\n-                      input: tc.arguments || tc.input,\n-                      output: tc.result || tc.output,\n-                    }))\n-                  : [],\n-                count: result.toolCalls?.length || 0,\n-              },\n-              providerTiming: result.timing || undefined,\n-              cost: result.cost || undefined,\n-            },\n-          }\n-        }\n-      }\n+    if (!(error instanceof Error)) return\n+\n+    logger.error('Provider request error details', {\n+      workflowId: context.workflowId,\n+      blockId: block.id,\n+      errorName: error.name,\n+      errorMessage: error.message,\n+      errorStack: error.stack,\n+      timestamp: new Date().toISOString(),\n+    })\n+\n+    if (error.name === 'AbortError') {\n+      throw new Error('Provider request timed out - the API took too long to respond')\n+    }\n+    if (error.name === 'TypeError' && error.message.includes('fetch')) {\n+      throw new Error(\n+        'Network error - unable to connect to provider API. Please check your internet connection.'\n+      )\n+    }\n+    if (error.message.includes('ENOTFOUND') || error.message.includes('ECONNREFUSED')) {\n+      throw new Error('Unable to connect to server - DNS or connection issue')\n+    }\n+  }\n+\n+  private processProviderResponse(\n+    response: any,\n+    block: SerializedBlock,\n+    responseFormat: any\n+  ): BlockOutput | StreamingExecution {\n+    if (this.isStreamingExecution(response)) {\n+      return this.processStreamingExecution(response, block)\n+    }\n+\n+    if (response instanceof ReadableStream) {\n+      return this.createMinimalStreamingExecution(response)\n+    }\n+\n+    return this.processRegularResponse(response, responseFormat)\n+  }\n+\n+  private isStreamingExecution(response: any): boolean {\n+    return (\n+      response && typeof response === 'object' && 'stream' in response && 'execution' in response\n+    )\n+  }\n+\n+  private processStreamingExecution(\n+    response: StreamingExecution,\n+    block: SerializedBlock\n+  ): StreamingExecution {\n+    const streamingExec = response as StreamingExecution\n+    logger.info(`Received StreamingExecution for block ${block.id}`)\n+\n+    if (streamingExec.execution.output?.response) {\n+      const execution = streamingExec.execution as any\n+      if (block.metadata?.name) execution.blockName = block.metadata.name\n+      if (block.metadata?.id) execution.blockType = block.metadata.id\n+      execution.blockId = block.id\n+      execution.isStreaming = true\n+    }\n+\n+    return streamingExec\n+  }\n+\n+  private createMinimalStreamingExecution(stream: ReadableStream): StreamingExecution {\n+    return {\n+      stream,\n+      execution: {\n+        success: true,\n+        output: { response: {} },\n+        logs: [],\n+        metadata: {\n+          duration: 0,\n+          startTime: new Date().toISOString(),\n+        },\n+      },\n+    }\n+  }\n \n-      // Return standard response if no responseFormat\n+  private processRegularResponse(result: any, responseFormat: any): BlockOutput {\n+    logger.info('Provider response received', {\n+      contentLength: result.content ? result.content.length : 0,\n+      model: result.model,\n+      hasTokens: !!result.tokens,\n+      hasToolCalls: !!result.toolCalls,\n+      toolCallsCount: result.toolCalls?.length || 0,\n+    })\n+\n+    if (responseFormat) {\n+      return this.processStructuredResponse(result, responseFormat)\n+    }\n+\n+    return this.processStandardResponse(result)\n+  }\n+\n+  private processStructuredResponse(result: any, responseFormat: any): BlockOutput {\n+    try {\n+      const parsedContent = JSON.parse(result.content)\n       return {\n         response: {\n-          content: result.content,\n-          model: result.model,\n-          tokens: result.tokens || {\n-            prompt: 0,\n-            completion: 0,\n-            total: 0,\n-          },\n-          toolCalls: {\n-            list: result.toolCalls\n-              ? result.toolCalls.map((tc: any) => ({\n-                  ...tc,\n-                  // Strip the 'custom_' prefix from tool names for display\n-                  name: stripCustomToolPrefix(tc.name),\n-                  // Preserve timing information if available\n-                  startTime: tc.startTime,\n-                  endTime: tc.endTime,\n-                  duration: tc.duration,\n-                  input: tc.arguments || tc.input,\n-                  output: tc.result || tc.output,\n-                }))\n-              : [],\n-            count: result.toolCalls?.length || 0,\n-          },\n-          providerTiming: result.timing || undefined,\n-          cost: result.cost || undefined,\n+          ...parsedContent,\n+          ...this.createResponseMetadata(result),\n         },\n       }\n     } catch (error) {\n-      logger.error('Error executing provider request:', { error })\n-\n-      // Enhanced error logging for different error types\n-      if (error instanceof Error) {\n-        logger.error('Provider request error details', {\n-          workflowId: context.workflowId,\n-          blockId: block.id,\n-          errorName: error.name,\n-          errorMessage: error.message,\n-          errorStack: error.stack,\n-          url: url.toString(),\n-          timestamp: new Date().toISOString(),\n-        })\n-\n-        // Check for specific error types\n-        if (error.name === 'AbortError') {\n-          logger.error('Request timed out after 2 minutes', {\n-            workflowId: context.workflowId,\n-            blockId: block.id,\n-            url: url.toString(),\n-          })\n-          throw new Error('Provider request timed out - the API took too long to respond')\n-        }\n-        if (error.name === 'TypeError' && error.message.includes('fetch')) {\n-          logger.error('Network fetch error - possible connectivity issue', {\n-            workflowId: context.workflowId,\n-            blockId: block.id,\n-            url: url.toString(),\n-            errorMessage: error.message,\n-          })\n-          throw new Error(\n-            'Network error - unable to connect to provider API. Please check your internet connection.'\n-          )\n-        }\n-        if (error.message.includes('ENOTFOUND') || error.message.includes('ECONNREFUSED')) {\n-          logger.error('DNS/Connection error', {\n-            workflowId: context.workflowId,\n-            blockId: block.id,\n-            url: url.toString(),\n-            errorMessage: error.message,\n-          })\n-          throw new Error('Unable to connect to server - DNS or connection issue')\n-        }\n-      }\n-\n-      throw error\n+      logger.error('Failed to parse response content:', { error })\n+      return this.processStandardResponse(result)\n     }\n   }\n-}\n-\n-export function stripCustomToolPrefix(name: string) {\n-  return name.startsWith('custom_') ? name.replace('custom_', '') : name\n-}\n \n-/**\n- * Helper function to process memories and convert them to message format\n- */\n-function processMemories(memories: any, logger: any): any[] {\n-  const messages: any[] = []\n-\n-  if (!memories) {\n-    return messages\n+  private processStandardResponse(result: any): BlockOutput {\n+    return {\n+      response: {\n+        content: result.content,\n+        model: result.model,\n+        ...this.createResponseMetadata(result),\n+      },\n+    }\n   }\n \n-  let memoryArray: any[] = []\n-\n-  // Handle different memory input formats\n-  if (memories?.response?.memories && Array.isArray(memories.response.memories)) {\n-    // Memory block output format: { response: { memories: [...] } }\n-    memoryArray = memories.response.memories\n-  } else if (memories?.memories && Array.isArray(memories.memories)) {\n-    // Direct memory output format: { memories: [...] }\n-    memoryArray = memories.memories\n-  } else if (Array.isArray(memories)) {\n-    // Direct array of messages: [{ role, content }, ...]\n-    memoryArray = memories\n-  } else {\n-    logger.warn('Unexpected memories format', { memories })\n-    return messages\n+  private createResponseMetadata(result: any) {\n+    return {\n+      tokens: result.tokens || { prompt: 0, completion: 0, total: 0 },\n+      toolCalls: {\n+        list: result.toolCalls ? result.toolCalls.map(this.formatToolCall.bind(this)) : [],\n+        count: result.toolCalls?.length || 0,\n+      },\n+      providerTiming: result.timing,\n+      cost: result.cost,\n+    }\n   }\n \n-  // Process the memory array\n-  memoryArray.forEach((memory: any) => {\n-    if (memory.data && Array.isArray(memory.data)) {\n-      // Memory object with data array: { key, type, data: [{ role, content }, ...] }\n-      memory.data.forEach((msg: any) => {\n-        if (msg.role && msg.content) {\n-          messages.push({\n-            role: msg.role,\n-            content: msg.content,\n-          })\n-        }\n-      })\n-    } else if (memory.role && memory.content) {\n-      // Direct message object: { role, content }\n-      messages.push({\n-        role: memory.role,\n-        content: memory.content,\n-      })\n+  private formatToolCall(tc: any) {\n+    return {\n+      ...tc,\n+      name: this.stripCustomToolPrefix(tc.name),\n+      startTime: tc.startTime,\n+      endTime: tc.endTime,\n+      duration: tc.duration,\n+      input: tc.arguments || tc.input,\n+      output: tc.result || tc.output,\n     }\n-  })\n+  }\n \n-  return messages\n+  private stripCustomToolPrefix(name: string): string {\n+    return name.startsWith('custom_') ? name.replace('custom_', '') : name\n+  }\n }\ndiff --git a/apps/sim/executor/handlers/agent/types.ts b/apps/sim/executor/handlers/agent/types.ts\nnew file mode 100644\nindex 0000000000..5658223b17\n--- /dev/null\n+++ b/apps/sim/executor/handlers/agent/types.ts\n@@ -0,0 +1,35 @@\n+export interface AgentInputs {\n+  model?: string\n+  responseFormat?: string | object\n+  tools?: ToolInput[]\n+  systemPrompt?: string\n+  userPrompt?: string | object\n+  memories?: any\n+  temperature?: number\n+  maxTokens?: number\n+  apiKey?: string\n+}\n+\n+export interface ToolInput {\n+  type?: string\n+  schema?: any\n+  title?: string\n+  code?: string\n+  params?: Record<string, any>\n+  timeout?: number\n+  usageControl?: 'auto' | 'force' | 'none'\n+  operation?: string\n+}\n+\n+export interface Message {\n+  role: 'system' | 'user' | 'assistant'\n+  content: string\n+  function_call?: any\n+  tool_calls?: any[]\n+}\n+\n+export interface StreamingConfig {\n+  shouldUseStreaming: boolean\n+  isBlockSelectedForOutput: boolean\n+  hasOutgoingConnections: boolean\n+}\ndiff --git a/apps/sim/executor/handlers/function/function-handler.test.ts b/apps/sim/executor/handlers/function/function-handler.test.ts\nindex 87f89ecd72..92d2ba7d49 100644\n--- a/apps/sim/executor/handlers/function/function-handler.test.ts\n+++ b/apps/sim/executor/handlers/function/function-handler.test.ts\n@@ -1,5 +1,3 @@\n-import '../../__test-utils__/mock-dependencies'\n-\n import { beforeEach, describe, expect, it, type Mock, vi } from 'vitest'\n import type { BlockOutput } from '@/blocks/types'\n import type { SerializedBlock } from '@/serializer/types'\n@@ -7,6 +5,19 @@ import { executeTool } from '@/tools'\n import type { ExecutionContext } from '../../types'\n import { FunctionBlockHandler } from './function-handler'\n \n+vi.mock('@/lib/logs/console-logger', () => ({\n+  createLogger: vi.fn(() => ({\n+    info: vi.fn(),\n+    error: vi.fn(),\n+    warn: vi.fn(),\n+    debug: vi.fn(),\n+  })),\n+}))\n+\n+vi.mock('@/tools', () => ({\n+  executeTool: vi.fn(),\n+}))\n+\n const mockExecuteTool = executeTool as Mock\n \n describe('FunctionBlockHandler', () => {\n@@ -58,10 +69,14 @@ describe('FunctionBlockHandler', () => {\n     const inputs = {\n       code: 'console.log(\"Hello\"); return 1 + 1;',\n       timeout: 10000,\n+      envVars: {},\n+      isCustomTool: false,\n+      workflowId: undefined,\n     }\n     const expectedToolParams = {\n       code: inputs.code,\n       timeout: inputs.timeout,\n+      envVars: {},\n       _context: { workflowId: mockContext.workflowId },\n     }\n     const expectedOutput: BlockOutput = { response: { result: 'Success' } }\n@@ -76,11 +91,15 @@ describe('FunctionBlockHandler', () => {\n     const inputs = {\n       code: [{ content: 'const x = 5;' }, { content: 'return x * 2;' }],\n       timeout: 5000,\n+      envVars: {},\n+      isCustomTool: false,\n+      workflowId: undefined,\n     }\n     const expectedCode = 'const x = 5;\\nreturn x * 2;'\n     const expectedToolParams = {\n       code: expectedCode,\n       timeout: inputs.timeout,\n+      envVars: {},\n       _context: { workflowId: mockContext.workflowId },\n     }\n     const expectedOutput: BlockOutput = { response: { result: 'Success' } }\n@@ -96,6 +115,7 @@ describe('FunctionBlockHandler', () => {\n     const expectedToolParams = {\n       code: inputs.code,\n       timeout: 5000, // Default timeout\n+      envVars: {},\n       _context: { workflowId: mockContext.workflowId },\n     }\n \ndiff --git a/apps/sim/executor/handlers/function/function-handler.ts b/apps/sim/executor/handlers/function/function-handler.ts\nindex d0d7e42e40..c63c80d2fb 100644\n--- a/apps/sim/executor/handlers/function/function-handler.ts\n+++ b/apps/sim/executor/handlers/function/function-handler.ts\n@@ -28,6 +28,7 @@ export class FunctionBlockHandler implements BlockHandler {\n     const result = await executeTool('function_execute', {\n       code: codeContent,\n       timeout: inputs.timeout || 5000,\n+      envVars: context.environmentVariables || {},\n       _context: { workflowId: context.workflowId },\n     })\n \ndiff --git a/apps/sim/providers/anthropic/index.ts b/apps/sim/providers/anthropic/index.ts\nindex 4a626e48fa..2181a3f065 100644\n--- a/apps/sim/providers/anthropic/index.ts\n+++ b/apps/sim/providers/anthropic/index.ts\n@@ -459,6 +459,7 @@ ${fieldDescriptions}\n                 ...tool.params,\n                 ...toolArgs,\n                 ...(request.workflowId ? { _context: { workflowId: request.workflowId } } : {}),\n+                ...(request.environmentVariables ? { envVars: request.environmentVariables } : {}),\n               }\n               const result = await executeTool(toolName, mergedArgs, true)\n               const toolCallEndTime = Date.now()\ndiff --git a/apps/sim/providers/deepseek/index.ts b/apps/sim/providers/deepseek/index.ts\nindex 0f3e06b053..cfeb4813f2 100644\n--- a/apps/sim/providers/deepseek/index.ts\n+++ b/apps/sim/providers/deepseek/index.ts\n@@ -288,6 +288,7 @@ export const deepseekProvider: ProviderConfig = {\n                 ...tool.params,\n                 ...toolArgs,\n                 ...(request.workflowId ? { _context: { workflowId: request.workflowId } } : {}),\n+                ...(request.environmentVariables ? { envVars: request.environmentVariables } : {}),\n               }\n               const result = await executeTool(toolName, mergedArgs, true)\n               const toolCallEndTime = Date.now()\ndiff --git a/apps/sim/providers/google/index.ts b/apps/sim/providers/google/index.ts\nindex 08f943a0be..944cd0c103 100644\n--- a/apps/sim/providers/google/index.ts\n+++ b/apps/sim/providers/google/index.ts\n@@ -368,6 +368,7 @@ export const googleProvider: ProviderConfig = {\n                 ...toolArgs, // Arguments from the model's function call\n                 ...requiredToolCallParams, // Required parameters from the tool definition (take precedence)\n                 ...(request.workflowId ? { _context: { workflowId: request.workflowId } } : {}),\n+                ...(request.environmentVariables ? { envVars: request.environmentVariables } : {}),\n               }\n \n               // For debugging only - don't log actual API keys\ndiff --git a/apps/sim/providers/openai/index.ts b/apps/sim/providers/openai/index.ts\nindex 307481aa57..ece64fd4cc 100644\n--- a/apps/sim/providers/openai/index.ts\n+++ b/apps/sim/providers/openai/index.ts\n@@ -360,6 +360,7 @@ export const openaiProvider: ProviderConfig = {\n               ...tool.params,\n               ...toolArgs,\n               ...(request.workflowId ? { _context: { workflowId: request.workflowId } } : {}),\n+              ...(request.environmentVariables ? { envVars: request.environmentVariables } : {}),\n             }\n \n             const result = await executeTool(toolName, mergedArgs, true)\ndiff --git a/apps/sim/providers/types.ts b/apps/sim/providers/types.ts\nindex 49e2da61e7..1be7cad578 100644\n--- a/apps/sim/providers/types.ts\n+++ b/apps/sim/providers/types.ts\n@@ -147,6 +147,7 @@ export interface ProviderRequest {\n   local_execution?: boolean\n   workflowId?: string // Optional workflow ID for authentication context\n   stream?: boolean\n+  environmentVariables?: Record<string, string> // Environment variables for tool execution\n }\n \n // Map of provider IDs to their configurations\ndiff --git a/apps/sim/stores/custom-tools/store.ts b/apps/sim/stores/custom-tools/store.ts\nindex 96ba92066b..7605321532 100644\n--- a/apps/sim/stores/custom-tools/store.ts\n+++ b/apps/sim/stores/custom-tools/store.ts\n@@ -32,27 +32,34 @@ export const useCustomToolsStore = create<CustomToolsStore>()(\n               throw new Error('Invalid response format')\n             }\n \n-            // Validate each tool object's structure before processing\n-            data.forEach((tool, index) => {\n+            // Filter and validate tools, skipping invalid ones instead of throwing errors\n+            const validTools = data.filter((tool, index) => {\n               if (!tool || typeof tool !== 'object') {\n-                throw new Error(`Invalid tool format at index ${index}: not an object`)\n+                logger.warn(`Skipping invalid tool at index ${index}: not an object`)\n+                return false\n               }\n               if (!tool.id || typeof tool.id !== 'string') {\n-                throw new Error(`Invalid tool format at index ${index}: missing or invalid id`)\n+                logger.warn(`Skipping invalid tool at index ${index}: missing or invalid id`)\n+                return false\n               }\n               if (!tool.title || typeof tool.title !== 'string') {\n-                throw new Error(`Invalid tool format at index ${index}: missing or invalid title`)\n+                logger.warn(`Skipping invalid tool at index ${index}: missing or invalid title`)\n+                return false\n               }\n               if (!tool.schema || typeof tool.schema !== 'object') {\n-                throw new Error(`Invalid tool format at index ${index}: missing or invalid schema`)\n+                logger.warn(`Skipping invalid tool at index ${index}: missing or invalid schema`)\n+                return false\n               }\n+              // Make code field optional - default to empty string if missing\n               if (!tool.code || typeof tool.code !== 'string') {\n-                throw new Error(`Invalid tool format at index ${index}: missing or invalid code`)\n+                logger.warn(`Tool at index ${index} missing code field, defaulting to empty string`)\n+                tool.code = ''\n               }\n+              return true\n             })\n \n             // Transform to local format and set\n-            const transformedTools = data.reduce(\n+            const transformedTools = validTools.reduce(\n               (acc, tool) => ({\n                 ...acc,\n                 [tool.id]: tool,\n@@ -60,8 +67,6 @@ export const useCustomToolsStore = create<CustomToolsStore>()(\n               {}\n             )\n \n-            logger.info(`Loaded ${data.length} custom tools from server`)\n-\n             set({\n               tools: transformedTools,\n               isLoading: false,\n@@ -72,12 +77,6 @@ export const useCustomToolsStore = create<CustomToolsStore>()(\n               error: error instanceof Error ? error.message : 'Unknown error',\n               isLoading: false,\n             })\n-\n-            // Add a delay before reloading to prevent race conditions\n-            setTimeout(() => {\n-              // Reload from server to ensure consistency\n-              get().loadCustomTools()\n-            }, 500)\n           }\n         },\n \n@@ -121,21 +120,12 @@ export const useCustomToolsStore = create<CustomToolsStore>()(\n \n             set({ isLoading: false })\n             logger.info('Successfully synced custom tools with server')\n-\n-            // Load from server to ensure consistency even after successful sync\n-            get().loadCustomTools()\n           } catch (error) {\n             logger.error('Error syncing custom tools:', error)\n             set({\n               error: error instanceof Error ? error.message : 'Unknown error',\n               isLoading: false,\n             })\n-\n-            // Add a delay before reloading to prevent race conditions\n-            setTimeout(() => {\n-              // Reload from server to ensure consistency\n-              get().loadCustomTools()\n-            }, 500)\n           }\n         },\n \ndiff --git a/apps/sim/tools/function/execute.test.ts b/apps/sim/tools/function/execute.test.ts\nindex 7ac8d7851e..80e670d840 100644\n--- a/apps/sim/tools/function/execute.test.ts\n+++ b/apps/sim/tools/function/execute.test.ts\n@@ -41,12 +41,18 @@ describe('Function Execute Tool', () => {\n     test('should format single string code correctly', () => {\n       const body = tester.getRequestBody({\n         code: 'return 42',\n+        envVars: {},\n+        isCustomTool: false,\n         timeout: 5000,\n+        workflowId: undefined,\n       })\n \n       expect(body).toEqual({\n         code: 'return 42',\n+        envVars: {},\n+        isCustomTool: false,\n         timeout: 5000,\n+        workflowId: undefined,\n       })\n     })\n \n@@ -57,11 +63,18 @@ describe('Function Execute Tool', () => {\n           { content: 'const y = 2;', id: 'block2' },\n           { content: 'return x + y;', id: 'block3' },\n         ],\n+        envVars: {},\n+        isCustomTool: false,\n+        timeout: 10000,\n+        workflowId: undefined,\n       })\n \n       expect(body).toEqual({\n         code: 'const x = 40;\\nconst y = 2;\\nreturn x + y;',\n         timeout: 10000,\n+        envVars: {},\n+        isCustomTool: false,\n+        workflowId: undefined,\n       })\n     })\n \n@@ -73,6 +86,9 @@ describe('Function Execute Tool', () => {\n       expect(body).toEqual({\n         code: 'return 42',\n         timeout: 10000,\n+        envVars: {},\n+        isCustomTool: false,\n+        workflowId: undefined,\n       })\n     })\n   })\ndiff --git a/apps/sim/tools/function/execute.ts b/apps/sim/tools/function/execute.ts\nindex 6994b50ca9..e98c00e675 100644\n--- a/apps/sim/tools/function/execute.ts\n+++ b/apps/sim/tools/function/execute.ts\n@@ -22,6 +22,12 @@ export const functionExecuteTool: ToolConfig<CodeExecutionInput, CodeExecutionOu\n       description: 'Execution timeout in milliseconds',\n       default: DEFAULT_TIMEOUT,\n     },\n+    envVars: {\n+      type: 'object',\n+      required: false,\n+      description: 'Environment variables to make available during execution',\n+      default: {},\n+    },\n   },\n \n   request: {\n@@ -38,6 +44,9 @@ export const functionExecuteTool: ToolConfig<CodeExecutionInput, CodeExecutionOu\n       return {\n         code: codeContent,\n         timeout: params.timeout || DEFAULT_TIMEOUT,\n+        envVars: params.envVars || {},\n+        workflowId: params._context?.workflowId,\n+        isCustomTool: params.isCustomTool || false,\n       }\n     },\n     isInternalRoute: true,\ndiff --git a/apps/sim/tools/function/types.ts b/apps/sim/tools/function/types.ts\nindex dfe25bfe95..dbb9555a34 100644\n--- a/apps/sim/tools/function/types.ts\n+++ b/apps/sim/tools/function/types.ts\n@@ -4,6 +4,11 @@ export interface CodeExecutionInput {\n   code: Array<{ content: string; id: string }> | string\n   timeout?: number\n   memoryLimit?: number\n+  envVars?: Record<string, string>\n+  _context?: {\n+    workflowId?: string\n+  }\n+  isCustomTool?: boolean\n }\n \n export interface CodeExecutionOutput extends ToolResponse {\ndiff --git a/apps/sim/tools/index.ts b/apps/sim/tools/index.ts\nindex f669032802..4a7df371e3 100644\n--- a/apps/sim/tools/index.ts\n+++ b/apps/sim/tools/index.ts\n@@ -495,7 +495,7 @@ function validateClientSideParams(\n   }\n \n   // Internal parameters that should be excluded from validation\n-  const internalParamSet = new Set(['_context', 'workflowId'])\n+  const internalParamSet = new Set(['_context', 'workflowId', 'envVars'])\n \n   // Check required parameters\n   if (schema.required) {\ndiff --git a/apps/sim/tools/utils.ts b/apps/sim/tools/utils.ts\nindex a6244c8552..88d86494cd 100644\n--- a/apps/sim/tools/utils.ts\n+++ b/apps/sim/tools/utils.ts\n@@ -249,8 +249,11 @@ export function createCustomToolRequestBody(\n   getStore?: () => any\n ) {\n   return (params: Record<string, any>) => {\n-    // Get environment variables - empty on server, from store on client\n-    const envVars = isClient ? getClientEnvVars(getStore) : {}\n+    // Get environment variables - try multiple sources in order of preference:\n+    // 1. envVars parameter (passed from provider/agent context)\n+    // 2. Client-side store (if running in browser)\n+    // 3. Empty object (fallback)\n+    const envVars = params.envVars || (isClient ? getClientEnvVars(getStore) : {})\n \n     // Include everything needed for execution\n     return {\n"
  },
  "CWE-191": {
    "cve": "CVE-2023-39350",
    "commit_url": "https://github.com/FreeRDP/FreeRDP/commit/e204fc8be5a372626b13f66daf2abafe71dbc2dc",
    "diff": "diff --git a/libfreerdp/codec/rfx.c b/libfreerdp/codec/rfx.c\nindex 58ab10d21b56..3716084b52e3 100644\n--- a/libfreerdp/codec/rfx.c\n+++ b/libfreerdp/codec/rfx.c\n@@ -994,6 +994,31 @@ static BOOL rfx_process_message_tileset(RFX_CONTEXT* context, RFX_MESSAGE* messa\n \t\t\tStream_Read_UINT8(sub, tile->quantIdxY);  /* quantIdxY (1 byte) */\n \t\t\tStream_Read_UINT8(sub, tile->quantIdxCb); /* quantIdxCb (1 byte) */\n \t\t\tStream_Read_UINT8(sub, tile->quantIdxCr); /* quantIdxCr (1 byte) */\n+\t\t\tif (tile->quantIdxY >= context->numQuant)\n+\t\t\t{\n+\t\t\t\tWLog_Print(context->priv->log, WLOG_ERROR,\n+\t\t\t\t           \"quantIdxY %\" PRIu8 \" >= numQuant %\" PRIu8, tile->quantIdxY,\n+\t\t\t\t           context->numQuant);\n+\t\t\t\trc = FALSE;\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tif (tile->quantIdxCb >= context->numQuant)\n+\t\t\t{\n+\t\t\t\tWLog_Print(context->priv->log, WLOG_ERROR,\n+\t\t\t\t           \"quantIdxCb %\" PRIu8 \" >= numQuant %\" PRIu8, tile->quantIdxCb,\n+\t\t\t\t           context->numQuant);\n+\t\t\t\trc = FALSE;\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tif (tile->quantIdxCr >= context->numQuant)\n+\t\t\t{\n+\t\t\t\tWLog_Print(context->priv->log, WLOG_ERROR,\n+\t\t\t\t           \"quantIdxCr %\" PRIu8 \" >= numQuant %\" PRIu8, tile->quantIdxCr,\n+\t\t\t\t           context->numQuant);\n+\t\t\t\trc = FALSE;\n+\t\t\t\tbreak;\n+\t\t\t}\n+\n \t\t\tStream_Read_UINT16(sub, tile->xIdx);      /* xIdx (2 bytes) */\n \t\t\tStream_Read_UINT16(sub, tile->yIdx);      /* yIdx (2 bytes) */\n \t\t\tStream_Read_UINT16(sub, tile->YLen);      /* YLen (2 bytes) */\n"
  },
  "CWE-122": {
    "cve": "CVE-2024-56827",
    "commit_url": "https://github.com/uclouvain/openjpeg/commit/e492644fbded4c820ca55b5e50e598d346e850e8",
    "diff": "diff --git a/src/lib/openjp2/j2k.c b/src/lib/openjp2/j2k.c\nindex 7dc389fa2..b5903a59c 100644\n--- a/src/lib/openjp2/j2k.c\n+++ b/src/lib/openjp2/j2k.c\n@@ -8456,7 +8456,8 @@ static OPJ_BOOL opj_j2k_add_tlmarker(OPJ_UINT32 tileno,\n     if (type == J2K_MS_SOT) {\n         OPJ_UINT32 l_current_tile_part = cstr_index->tile_index[tileno].current_tpsno;\n \n-        if (cstr_index->tile_index[tileno].tp_index) {\n+        if (cstr_index->tile_index[tileno].tp_index &&\n+                l_current_tile_part < cstr_index->tile_index[tileno].nb_tps) {\n             cstr_index->tile_index[tileno].tp_index[l_current_tile_part].start_pos = pos;\n         }\n \n"
  },
  "CWE-434": {
    "cve": "CVE-2025-61681",
    "commit_url": "https://github.com/xuemian168/kuno/commit/fc486b5c9091b607f82bf7b354d18f25204f7dc6",
    "diff": "diff --git a/.gitignore b/.gitignore\nindex 2daab18..45b02c6 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -2,4 +2,6 @@\n dev.sh\n blog.db\n .env\n-!/docs/CHANGELOG.md\n\\ No newline at end of file\n+backups/\n+!/docs/CHANGELOG.md\n+.DS_Store\n\\ No newline at end of file\ndiff --git a/backend/internal/api/media.go b/backend/internal/api/media.go\nindex 79c60cb..83f28e7 100644\n--- a/backend/internal/api/media.go\n+++ b/backend/internal/api/media.go\n@@ -3,6 +3,8 @@ package api\n import (\n \t\"blog-backend/internal/database\"\n \t\"blog-backend/internal/models\"\n+\t\"bytes\"\n+\t\"encoding/xml\"\n \t\"fmt\"\n \t\"github.com/gin-gonic/gin\"\n \t\"github.com/google/uuid\"\n@@ -10,21 +12,28 @@ import (\n \t\"net/http\"\n \t\"os\"\n \t\"path/filepath\"\n+\t\"regexp\"\n \t\"strconv\"\n+\t\"strings\"\n )\n \n const (\n-\tMaxFileSize = 100 * 1024 * 1024 // 100MB\n+\tMaxFileSize       = 100 * 1024 * 1024 // 100MB\n+\tMaxGIFFrames      = 1000               // Prevent GIF bombs\n+\tMaxPNGChunks      = 100                // Prevent PNG bombs\n+\tMaxImageDimension = 10000              // 10000x10000 pixels max\n+\tMaxVideoMetadata  = 10 * 1024 * 1024   // 10MB metadata limit\n )\n \n var UploadDir = getUploadDir()\n \n var allowedImageTypes = map[string]bool{\n-\t\"image/jpeg\": true,\n-\t\"image/jpg\":  true,\n-\t\"image/png\":  true,\n-\t\"image/gif\":  true,\n-\t\"image/webp\": true,\n+\t\"image/jpeg\":    true,\n+\t\"image/jpg\":     true,\n+\t\"image/png\":     true,\n+\t\"image/gif\":     true,\n+\t\"image/webp\":    true,\n+\t\"image/svg+xml\": true, // SVG support with sanitization\n }\n \n var allowedVideoTypes = map[string]bool{\n@@ -51,6 +60,349 @@ func init() {\n \t}\n }\n \n+// Security: File magic numbers for validation\n+var fileMagicNumbers = map[string][]byte{\n+\t\"image/jpeg\":    {0xFF, 0xD8, 0xFF},\n+\t\"image/png\":     {0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A},\n+\t\"image/gif\":     {0x47, 0x49, 0x46, 0x38},\n+\t\"image/webp\":    {0x52, 0x49, 0x46, 0x46},\n+\t\"image/svg+xml\": {0x3C, 0x3F, 0x78, 0x6D, 0x6C}, // <?xml or <svg\n+\t\"video/mp4\":     {0x00, 0x00, 0x00},\n+\t\"video/webm\":    {0x1A, 0x45, 0xDF, 0xA3},\n+\t\"video/ogg\":     {0x4F, 0x67, 0x67, 0x53},\n+\t\"video/avi\":     {0x52, 0x49, 0x46, 0x46},\n+}\n+\n+// Security: Dangerous SVG elements that must be removed\n+var dangerousSVGElements = []string{\n+\t\"script\", \"foreignObject\", \"iframe\", \"object\", \"embed\",\n+\t\"use\", \"animate\", \"animateTransform\", \"set\", \"animateMotion\",\n+}\n+\n+// Security: Dangerous SVG attributes that must be removed\n+var dangerousSVGAttributes = []string{\n+\t\"onload\", \"onerror\", \"onclick\", \"onmouseover\", \"onmouseout\",\n+\t\"onmousedown\", \"onmouseup\", \"onmousemove\", \"onkeydown\",\n+\t\"onkeyup\", \"onkeypress\", \"onfocus\", \"onblur\", \"onchange\",\n+\t\"onsubmit\", \"onreset\", \"onselect\", \"onabort\", \"href\", \"xlink:href\",\n+}\n+\n+// Helper function to get minimum of two integers\n+func minInt(a, b int) int {\n+\tif a < b {\n+\t\treturn a\n+\t}\n+\treturn b\n+}\n+\n+// validateJPEG checks JPEG file integrity\n+func validateJPEG(content []byte) error {\n+\tif len(content) < 4 {\n+\t\treturn fmt.Errorf(\"file too small to be a valid JPEG\")\n+\t}\n+\n+\t// Check SOI (Start of Image) marker: 0xFFD8\n+\tif content[0] != 0xFF || content[1] != 0xD8 {\n+\t\treturn fmt.Errorf(\"invalid JPEG: missing SOI marker\")\n+\t}\n+\n+\t// Check EOI (End of Image) marker: 0xFFD9\n+\tif content[len(content)-2] != 0xFF || content[len(content)-1] != 0xD9 {\n+\t\treturn fmt.Errorf(\"invalid JPEG: missing EOI marker\")\n+\t}\n+\n+\treturn nil\n+}\n+\n+// validatePNG checks PNG file integrity and prevents PNG bombs\n+func validatePNG(content []byte) error {\n+\tif len(content) < 8 {\n+\t\treturn fmt.Errorf(\"file too small to be a valid PNG\")\n+\t}\n+\n+\t// Check PNG signature\n+\tpngSignature := []byte{0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A}\n+\tif !bytes.Equal(content[:8], pngSignature) {\n+\t\treturn fmt.Errorf(\"invalid PNG signature\")\n+\t}\n+\n+\t// Check for IEND chunk (marks end of PNG)\n+\tif !bytes.Contains(content, []byte(\"IEND\")) {\n+\t\treturn fmt.Errorf(\"invalid PNG: missing IEND chunk\")\n+\t}\n+\n+\t// Prevent PNG bombs by counting chunks\n+\tchunkCount := 0\n+\tpos := 8 // Skip signature\n+\tfor pos+12 <= len(content) && chunkCount < MaxPNGChunks {\n+\t\tchunkCount++\n+\t\tlength := int(content[pos])<<24 | int(content[pos+1])<<16 | int(content[pos+2])<<8 | int(content[pos+3])\n+\t\tpos += 12 + length // length + type(4) + crc(4)\n+\t\tif pos > len(content) {\n+\t\t\tbreak\n+\t\t}\n+\t}\n+\n+\tif chunkCount >= MaxPNGChunks {\n+\t\treturn fmt.Errorf(\"suspicious PNG: too many chunks (potential PNG bomb)\")\n+\t}\n+\n+\treturn nil\n+}\n+\n+// validateGIF checks GIF file integrity and prevents GIF bombs\n+func validateGIF(content []byte) error {\n+\tif len(content) < 6 {\n+\t\treturn fmt.Errorf(\"file too small to be a valid GIF\")\n+\t}\n+\n+\t// Check GIF signature (GIF87a or GIF89a)\n+\tif !bytes.HasPrefix(content, []byte(\"GIF87a\")) && !bytes.HasPrefix(content, []byte(\"GIF89a\")) {\n+\t\treturn fmt.Errorf(\"invalid GIF signature\")\n+\t}\n+\n+\t// Check for GIF terminator (0x3B)\n+\tif content[len(content)-1] != 0x3B {\n+\t\treturn fmt.Errorf(\"invalid GIF: missing terminator\")\n+\t}\n+\n+\t// Prevent GIF bombs by counting frames\n+\tframeCount := bytes.Count(content, []byte{0x21, 0xF9, 0x04}) // Graphics Control Extension\n+\tif frameCount > MaxGIFFrames {\n+\t\treturn fmt.Errorf(\"suspicious GIF: too many frames (potential GIF bomb): %d\", frameCount)\n+\t}\n+\n+\treturn nil\n+}\n+\n+// validateWebP checks WebP file integrity\n+func validateWebP(content []byte) error {\n+\tif len(content) < 12 {\n+\t\treturn fmt.Errorf(\"file too small to be a valid WebP\")\n+\t}\n+\n+\t// Check RIFF header\n+\tif !bytes.HasPrefix(content, []byte(\"RIFF\")) {\n+\t\treturn fmt.Errorf(\"invalid WebP: missing RIFF header\")\n+\t}\n+\n+\t// Check WEBP signature at offset 8\n+\tif !bytes.Equal(content[8:12], []byte(\"WEBP\")) {\n+\t\treturn fmt.Errorf(\"invalid WebP: missing WEBP signature\")\n+\t}\n+\n+\treturn nil\n+}\n+\n+// validateMP4 checks MP4 file integrity\n+func validateMP4(content []byte) error {\n+\tif len(content) < 12 {\n+\t\treturn fmt.Errorf(\"file too small to be a valid MP4\")\n+\t}\n+\n+\t// Check for ftyp atom (file type box) in first 64 bytes or entire file if smaller\n+\tsearchLen := minInt(len(content), 64)\n+\tif !bytes.Contains(content[:searchLen], []byte(\"ftyp\")) {\n+\t\treturn fmt.Errorf(\"invalid MP4: missing ftyp atom\")\n+\t}\n+\n+\t// Common MP4 brands\n+\tvalidBrands := [][]byte{\n+\t\t[]byte(\"isom\"), []byte(\"iso2\"), []byte(\"mp41\"), []byte(\"mp42\"),\n+\t\t[]byte(\"avc1\"), []byte(\"M4V \"), []byte(\"M4A \"),\n+\t}\n+\n+\tfound := false\n+\tfor _, brand := range validBrands {\n+\t\tif bytes.Contains(content[:searchLen], brand) {\n+\t\t\tfound = true\n+\t\t\tbreak\n+\t\t}\n+\t}\n+\n+\tif !found {\n+\t\treturn fmt.Errorf(\"invalid MP4: unrecognized brand\")\n+\t}\n+\n+\treturn nil\n+}\n+\n+// validateWebM checks WebM file integrity\n+func validateWebM(content []byte) error {\n+\tif len(content) < 4 {\n+\t\treturn fmt.Errorf(\"file too small to be a valid WebM\")\n+\t}\n+\n+\t// Check EBML header (0x1A45DFA3)\n+\tif content[0] != 0x1A || content[1] != 0x45 || content[2] != 0xDF || content[3] != 0xA3 {\n+\t\treturn fmt.Errorf(\"invalid WebM: missing EBML header\")\n+\t}\n+\n+\treturn nil\n+}\n+\n+// validateOGG checks OGG file integrity\n+func validateOGG(content []byte) error {\n+\tif len(content) < 4 {\n+\t\treturn fmt.Errorf(\"file too small to be a valid OGG\")\n+\t}\n+\n+\t// Check OggS signature\n+\tif !bytes.HasPrefix(content, []byte(\"OggS\")) {\n+\t\treturn fmt.Errorf(\"invalid OGG: missing OggS signature\")\n+\t}\n+\n+\treturn nil\n+}\n+\n+// validateAVI checks AVI file integrity\n+func validateAVI(content []byte) error {\n+\tif len(content) < 12 {\n+\t\treturn fmt.Errorf(\"file too small to be a valid AVI\")\n+\t}\n+\n+\t// Check RIFF header\n+\tif !bytes.HasPrefix(content, []byte(\"RIFF\")) {\n+\t\treturn fmt.Errorf(\"invalid AVI: missing RIFF header\")\n+\t}\n+\n+\t// Check AVI signature at offset 8\n+\tif !bytes.Equal(content[8:12], []byte(\"AVI \")) {\n+\t\treturn fmt.Errorf(\"invalid AVI: missing AVI signature\")\n+\t}\n+\n+\treturn nil\n+}\n+\n+// detectPolyglot detects files that are valid in multiple formats (polyglot attacks)\n+func detectPolyglot(content []byte) bool {\n+\tif len(content) < 100 {\n+\t\treturn false\n+\t}\n+\n+\tsuspiciousPatterns := []string{\n+\t\t\"<?php\", \"#!/bin/\", \"<script\", \"javascript:\",\n+\t\t\"eval(\", \"base64_decode\", \"exec(\", \"system(\",\n+\t}\n+\n+\tcontentStr := string(content)\n+\tfor _, pattern := range suspiciousPatterns {\n+\t\tif strings.Contains(contentStr, pattern) {\n+\t\t\tfmt.Printf(\"Warning: Suspicious pattern detected in file: %s\\n\", pattern)\n+\t\t\treturn true\n+\t\t}\n+\t}\n+\n+\tmagicCount := 0\n+\tfor _, magic := range fileMagicNumbers {\n+\t\tif bytes.HasPrefix(content, magic) {\n+\t\t\tmagicCount++\n+\t\t}\n+\t}\n+\n+\treturn magicCount > 1\n+}\n+\n+// validateFileIntegrity performs comprehensive integrity checks based on file type\n+func validateFileIntegrity(content []byte, mimeType string) error {\n+\tswitch mimeType {\n+\tcase \"image/jpeg\", \"image/jpg\":\n+\t\treturn validateJPEG(content)\n+\tcase \"image/png\":\n+\t\treturn validatePNG(content)\n+\tcase \"image/gif\":\n+\t\treturn validateGIF(content)\n+\tcase \"image/webp\":\n+\t\treturn validateWebP(content)\n+\tcase \"video/mp4\":\n+\t\treturn validateMP4(content)\n+\tcase \"video/webm\":\n+\t\treturn validateWebM(content)\n+\tcase \"video/ogg\":\n+\t\treturn validateOGG(content)\n+\tcase \"video/avi\", \"video/mov\":\n+\t\treturn validateAVI(content)\n+\tcase \"image/svg+xml\":\n+\t\t// SVG validation happens in sanitizeSVG\n+\t\treturn nil\n+\tdefault:\n+\t\treturn fmt.Errorf(\"unsupported file type for integrity check: %s\", mimeType)\n+\t}\n+}\n+\n+// validateFileContent checks if file content matches declared MIME type using magic numbers\n+func validateFileContent(content []byte, declaredType string) bool {\n+\tif len(content) == 0 {\n+\t\treturn false\n+\t}\n+\n+\tmagic, exists := fileMagicNumbers[declaredType]\n+\tif !exists {\n+\t\treturn false\n+\t}\n+\n+\t// Special handling for SVG (can start with <?xml or <svg)\n+\tif declaredType == \"image/svg+xml\" {\n+\t\treturn bytes.HasPrefix(content, []byte(\"<?xml\")) ||\n+\t\t\tbytes.HasPrefix(content, []byte(\"<svg\")) ||\n+\t\t\tbytes.HasPrefix(content, magic)\n+\t}\n+\n+\t// For other formats, check if content starts with expected magic number\n+\tif len(content) < len(magic) {\n+\t\treturn false\n+\t}\n+\n+\treturn bytes.HasPrefix(content, magic)\n+}\n+\n+// sanitizeSVG removes dangerous elements and attributes from SVG content\n+func sanitizeSVG(content []byte) ([]byte, error) {\n+\tcontentStr := string(content)\n+\n+\t// Remove dangerous elements\n+\tfor _, element := range dangerousSVGElements {\n+\t\t// Remove opening and closing tags (non-greedy)\n+\t\topenTag := regexp.MustCompile(`(?is)<` + element + `[^>]*>.*?</` + element + `>`)\n+\t\tcontentStr = openTag.ReplaceAllString(contentStr, \"\")\n+\n+\t\t// Remove self-closing tags\n+\t\tselfClosing := regexp.MustCompile(`(?i)<` + element + `[^>]*/?>`)\n+\t\tcontentStr = selfClosing.ReplaceAllString(contentStr, \"\")\n+\t}\n+\n+\t// Remove dangerous attributes - improved regex patterns\n+\tfor _, attr := range dangerousSVGAttributes {\n+\t\t// Handle href specially (remove javascript:, data:, vbscript: protocols)\n+\t\tif attr == \"href\" || attr == \"xlink:href\" {\n+\t\t\t// Match and remove dangerous protocols in href/xlink:href\n+\t\t\tdangerousProtocols := regexp.MustCompile(`(?i)\\s*` + regexp.QuoteMeta(attr) + `\\s*=\\s*[\"']?(javascript|data|vbscript):[^\"'\\s>]*[\"'\\s>]?`)\n+\t\t\tcontentStr = dangerousProtocols.ReplaceAllString(contentStr, \"\")\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\t// Remove event handler attributes - match attribute with any value\n+\t\tattrPattern := regexp.MustCompile(`(?i)\\s+` + regexp.QuoteMeta(attr) + `\\s*=\\s*\"[^\"]*\"`)\n+\t\tcontentStr = attrPattern.ReplaceAllString(contentStr, \"\")\n+\n+\t\t// Also match single-quoted attributes\n+\t\tattrPattern2 := regexp.MustCompile(`(?i)\\s+` + regexp.QuoteMeta(attr) + `\\s*=\\s*'[^']*'`)\n+\t\tcontentStr = attrPattern2.ReplaceAllString(contentStr, \"\")\n+\n+\t\t// Also match unquoted attributes\n+\t\tattrPattern3 := regexp.MustCompile(`(?i)\\s+` + regexp.QuoteMeta(attr) + `\\s*=\\s*[^\\s>]+`)\n+\t\tcontentStr = attrPattern3.ReplaceAllString(contentStr, \"\")\n+\t}\n+\n+\t// Validate result is still valid XML\n+\tvar svgRoot struct{}\n+\tif err := xml.Unmarshal([]byte(contentStr), &svgRoot); err != nil {\n+\t\treturn nil, fmt.Errorf(\"sanitized SVG is not valid XML: %v\", err)\n+\t}\n+\n+\treturn []byte(contentStr), nil\n+}\n+\n func UploadMedia(c *gin.Context) {\n \tfile, header, err := c.Request.FormFile(\"file\")\n \tif err != nil {\n@@ -65,6 +417,13 @@ func UploadMedia(c *gin.Context) {\n \t\treturn\n \t}\n \n+\t// Read file content for security validation\n+\tfileContent, err := io.ReadAll(file)\n+\tif err != nil {\n+\t\tc.JSON(http.StatusInternalServerError, gin.H{\"error\": \"Failed to read file content\"})\n+\t\treturn\n+\t}\n+\n \t// Check content type\n \tcontentType := header.Header.Get(\"Content-Type\")\n \tvar mediaType models.MediaType\n@@ -81,8 +440,48 @@ func UploadMedia(c *gin.Context) {\n \t\treturn\n \t}\n \n-\t// Generate unique filename\n-\text := filepath.Ext(header.Filename)\n+\t// Security Layer 1: Validate file content matches declared MIME type (prevent Content-Type spoofing)\n+\tif !validateFileContent(fileContent, contentType) {\n+\t\tc.JSON(http.StatusBadRequest, gin.H{\"error\": \"File content does not match declared type. Possible Content-Type spoofing detected.\"})\n+\t\treturn\n+\t}\n+\n+\t// Security Layer 2: Perform comprehensive file integrity check\n+\tif err := validateFileIntegrity(fileContent, contentType); err != nil {\n+\t\tc.JSON(http.StatusBadRequest, gin.H{\"error\": fmt.Sprintf(\"File integrity check failed: %v\", err)})\n+\t\treturn\n+\t}\n+\n+\t// Security Layer 3: Detect polyglot files (files valid in multiple formats)\n+\tif detectPolyglot(fileContent) {\n+\t\tc.JSON(http.StatusBadRequest, gin.H{\"error\": \"Suspicious file detected: file contains multiple format signatures or executable content\"})\n+\t\treturn\n+\t}\n+\n+\t// Validate file extension with whitelist\n+\text := strings.ToLower(filepath.Ext(header.Filename))\n+\tallowedExtensions := map[string]bool{\n+\t\t\".jpg\": true, \".jpeg\": true, \".png\": true, \".gif\": true, \".webp\": true, \".svg\": true,\n+\t\t\".mp4\": true, \".webm\": true, \".ogg\": true, \".avi\": true, \".mov\": true,\n+\t}\n+\tif !allowedExtensions[ext] {\n+\t\tc.JSON(http.StatusBadRequest, gin.H{\"error\": \"File extension not allowed\"})\n+\t\treturn\n+\t}\n+\n+\t// Security Layer 4: Sanitize SVG content if it's an SVG file\n+\tisSVG := contentType == \"image/svg+xml\"\n+\tif isSVG {\n+\t\tsanitized, err := sanitizeSVG(fileContent)\n+\t\tif err != nil {\n+\t\t\tc.JSON(http.StatusBadRequest, gin.H{\"error\": fmt.Sprintf(\"Failed to sanitize SVG: %v\", err)})\n+\t\t\treturn\n+\t\t}\n+\t\tfileContent = sanitized\n+\t\tfmt.Printf(\"SVG file sanitized: %s\\n\", header.Filename)\n+\t}\n+\n+\t// Generate unique filename with validated extension\n \tfileName := fmt.Sprintf(\"%s%s\", uuid.New().String(), ext)\n \tfilePath := filepath.Join(UploadDir, subDir, fileName)\n \n@@ -94,17 +493,9 @@ func UploadMedia(c *gin.Context) {\n \t\treturn\n \t}\n \n-\t// Create the file\n-\tdst, err := os.Create(filePath)\n-\tif err != nil {\n-\t\tfmt.Printf(\"Failed to create file %s: %v\\n\", filePath, err)\n-\t\tc.JSON(http.StatusInternalServerError, gin.H{\"error\": \"Failed to create file\"})\n-\t\treturn\n-\t}\n-\tdefer dst.Close()\n-\n-\t// Copy file content\n-\tif _, err := io.Copy(dst, file); err != nil {\n+\t// Write sanitized content to file\n+\tif err := os.WriteFile(filePath, fileContent, 0644); err != nil {\n+\t\tfmt.Printf(\"Failed to write file %s: %v\\n\", filePath, err)\n \t\tc.JSON(http.StatusInternalServerError, gin.H{\"error\": \"Failed to save file\"})\n \t\treturn\n \t}\n@@ -115,7 +506,7 @@ func UploadMedia(c *gin.Context) {\n \t\tFileName:     fileName,\n \t\tOriginalName: header.Filename,\n \t\tFilePath:     filePath,\n-\t\tFileSize:     header.Size,\n+\t\tFileSize:     int64(len(fileContent)), // Use actual file size after sanitization\n \t\tMimeType:     contentType,\n \t\tMediaType:    mediaType,\n \t\tURL:          fmt.Sprintf(\"/uploads/%s/%s\", subDir, fileName),\n@@ -329,5 +720,21 @@ func ServeMedia(c *gin.Context) {\n \t\treturn\n \t}\n \n+\t// Security Layer 5: Set security response headers\n+\t// Prevent MIME type sniffing\n+\tc.Header(\"X-Content-Type-Options\", \"nosniff\")\n+\t// Prevent clickjacking\n+\tc.Header(\"X-Frame-Options\", \"DENY\")\n+\n+\t// Special handling for SVG files\n+\text := strings.ToLower(filepath.Ext(fileName))\n+\tif ext == \".svg\" {\n+\t\t// Strict CSP for SVG files to prevent script execution\n+\t\tc.Header(\"Content-Security-Policy\", \"default-src 'none'; style-src 'unsafe-inline'; img-src 'self' data:\")\n+\t\tc.Header(\"Content-Type\", \"image/svg+xml\")\n+\t\t// Force browser to treat as attachment (optional: could use 'inline' if you want to display)\n+\t\tc.Header(\"Content-Disposition\", fmt.Sprintf(\"inline; filename=\\\"%s\\\"\", fileName))\n+\t}\n+\n \tc.File(filePath)\n }\ndiff --git a/backend/internal/api/media_test.go b/backend/internal/api/media_test.go\nnew file mode 100644\nindex 0000000..5c74e8c\n--- /dev/null\n+++ b/backend/internal/api/media_test.go\n@@ -0,0 +1,620 @@\n+package api\n+\n+import (\n+\t\"bytes\"\n+\t\"strings\"\n+\t\"testing\"\n+)\n+\n+// Test SVG sanitization\n+func TestSanitizeSVG(t *testing.T) {\n+\ttests := []struct {\n+\t\tname        string\n+\t\tinput       string\n+\t\tshouldPass  bool\n+\t\tshouldContain []string\n+\t\tshouldNotContain []string\n+\t}{\n+\t\t{\n+\t\t\tname: \"Malicious SVG with script tag\",\n+\t\t\tinput: `<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<svg xmlns=\"http://www.w3.org/2000/svg\">\n+  <script>alert('XSS')</script>\n+  <circle cx=\"50\" cy=\"50\" r=\"40\" fill=\"red\"/>\n+</svg>`,\n+\t\t\tshouldPass: true,\n+\t\t\tshouldContain: []string{\"<circle\", \"<svg\"},\n+\t\t\tshouldNotContain: []string{\"<script\", \"alert\"},\n+\t\t},\n+\t\t{\n+\t\t\tname: \"SVG with onerror event\",\n+\t\t\tinput: `<svg xmlns=\"http://www.w3.org/2000/svg\">\n+  <image href=\"x\" onerror=\"alert('XSS')\" />\n+  <rect width=\"100\" height=\"100\" fill=\"blue\"/>\n+</svg>`,\n+\t\t\tshouldPass: true,\n+\t\t\tshouldContain: []string{\"<rect\", \"<svg\"},\n+\t\t\tshouldNotContain: []string{\"onerror\", \"alert\"},\n+\t\t},\n+\t\t{\n+\t\t\tname: \"SVG with onload event\",\n+\t\t\tinput: `<svg xmlns=\"http://www.w3.org/2000/svg\" onload=\"alert('XSS')\">\n+  <circle cx=\"50\" cy=\"50\" r=\"40\"/>\n+</svg>`,\n+\t\t\tshouldPass: true,\n+\t\t\tshouldContain: []string{\"<circle\", \"<svg\"},\n+\t\t\tshouldNotContain: []string{\"onload=\", \"alert\"},\n+\t\t},\n+\t\t{\n+\t\t\tname: \"SVG with javascript: protocol in nested element\",\n+\t\t\tinput: `<svg xmlns=\"http://www.w3.org/2000/svg\">\n+  <a href=\"javascript:alert('XSS')\">\n+    <text x=\"0\" y=\"15\">Click me</text>\n+  </a>\n+</svg>`,\n+\t\t\t// This might fail XML validation after sanitization because nested structure is broken\n+\t\t\t// That's OK - we prefer to reject than to allow potential XSS\n+\t\t\tshouldPass: false,\n+\t\t\tshouldContain: []string{},\n+\t\t\tshouldNotContain: []string{},\n+\t\t},\n+\t\t{\n+\t\t\tname: \"SVG with foreignObject\",\n+\t\t\tinput: `<svg xmlns=\"http://www.w3.org/2000/svg\">\n+  <foreignObject width=\"100\" height=\"100\">\n+    <body xmlns=\"http://www.w3.org/1999/xhtml\">\n+      <script>alert('XSS')</script>\n+    </body>\n+  </foreignObject>\n+  <circle cx=\"50\" cy=\"50\" r=\"40\"/>\n+</svg>`,\n+\t\t\tshouldPass: true,\n+\t\t\tshouldContain: []string{\"<circle\", \"<svg\"},\n+\t\t\tshouldNotContain: []string{\"<foreignObject\", \"<script\", \"alert\"},\n+\t\t},\n+\t\t{\n+\t\t\tname: \"Clean SVG should remain unchanged\",\n+\t\t\tinput: `<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"100\" height=\"100\">\n+  <circle cx=\"50\" cy=\"50\" r=\"40\" fill=\"red\"/>\n+  <rect x=\"10\" y=\"10\" width=\"30\" height=\"30\" fill=\"blue\"/>\n+</svg>`,\n+\t\t\tshouldPass: true,\n+\t\t\tshouldContain: []string{\"<circle\", \"<rect\", \"<svg\", \"<?xml\"},\n+\t\t\tshouldNotContain: []string{},\n+\t\t},\n+\t}\n+\n+\tfor _, tt := range tests {\n+\t\tt.Run(tt.name, func(t *testing.T) {\n+\t\t\tresult, err := sanitizeSVG([]byte(tt.input))\n+\n+\t\t\tif tt.shouldPass && err != nil {\n+\t\t\t\tt.Errorf(\"Expected sanitization to pass, but got error: %v\", err)\n+\t\t\t\treturn\n+\t\t\t}\n+\n+\t\t\tif !tt.shouldPass && err == nil {\n+\t\t\t\tt.Errorf(\"Expected sanitization to fail, but it passed\")\n+\t\t\t\treturn\n+\t\t\t}\n+\n+\t\t\tif tt.shouldPass {\n+\t\t\t\tresultStr := string(result)\n+\n+\t\t\t\t// Check for required content\n+\t\t\t\tfor _, required := range tt.shouldContain {\n+\t\t\t\t\tif !strings.Contains(resultStr, required) {\n+\t\t\t\t\t\tt.Errorf(\"Expected result to contain '%s', but it didn't. Result: %s\", required, resultStr)\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\n+\t\t\t\t// Check for forbidden content\n+\t\t\t\tfor _, forbidden := range tt.shouldNotContain {\n+\t\t\t\t\tif strings.Contains(resultStr, forbidden) {\n+\t\t\t\t\t\tt.Errorf(\"Expected result NOT to contain '%s', but it did. Result: %s\", forbidden, resultStr)\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t})\n+\t}\n+}\n+\n+// Test file content validation\n+func TestValidateFileContent(t *testing.T) {\n+\ttests := []struct {\n+\t\tname        string\n+\t\tcontent     []byte\n+\t\tdeclaredType string\n+\t\tshouldPass  bool\n+\t}{\n+\t\t{\n+\t\t\tname:        \"Valid JPEG file\",\n+\t\t\tcontent:     []byte{0xFF, 0xD8, 0xFF, 0xE0, 0x00, 0x10},\n+\t\t\tdeclaredType: \"image/jpeg\",\n+\t\t\tshouldPass:  true,\n+\t\t},\n+\t\t{\n+\t\t\tname:        \"Valid PNG file\",\n+\t\t\tcontent:     []byte{0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A},\n+\t\t\tdeclaredType: \"image/png\",\n+\t\t\tshouldPass:  true,\n+\t\t},\n+\t\t{\n+\t\t\tname:        \"Invalid JPEG (wrong magic number)\",\n+\t\t\tcontent:     []byte{0x00, 0x00, 0x00, 0x00},\n+\t\t\tdeclaredType: \"image/jpeg\",\n+\t\t\tshouldPass:  false,\n+\t\t},\n+\t\t{\n+\t\t\tname:        \"SVG with XML declaration\",\n+\t\t\tcontent:     []byte(\"<?xml version=\\\"1.0\\\"?>\\n<svg></svg>\"),\n+\t\t\tdeclaredType: \"image/svg+xml\",\n+\t\t\tshouldPass:  true,\n+\t\t},\n+\t\t{\n+\t\t\tname:        \"SVG starting with svg tag\",\n+\t\t\tcontent:     []byte(\"<svg xmlns=\\\"http://www.w3.org/2000/svg\\\"></svg>\"),\n+\t\t\tdeclaredType: \"image/svg+xml\",\n+\t\t\tshouldPass:  true,\n+\t\t},\n+\t\t{\n+\t\t\tname:        \"Content-Type spoofing: PNG claimed as JPEG\",\n+\t\t\tcontent:     []byte{0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A},\n+\t\t\tdeclaredType: \"image/jpeg\",\n+\t\t\tshouldPass:  false,\n+\t\t},\n+\t}\n+\n+\tfor _, tt := range tests {\n+\t\tt.Run(tt.name, func(t *testing.T) {\n+\t\t\tresult := validateFileContent(tt.content, tt.declaredType)\n+\n+\t\t\tif result != tt.shouldPass {\n+\t\t\t\tt.Errorf(\"Expected validation result to be %v, but got %v\", tt.shouldPass, result)\n+\t\t\t}\n+\t\t})\n+\t}\n+}\n+\n+// Test real-world XSS vectors\n+func TestXSSVectors(t *testing.T) {\n+\txssVectors := []string{\n+\t\t// Script injection\n+\t\t`<svg><script>alert('XSS')</script></svg>`,\n+\t\t`<svg><script xlink:href=\"data:text/javascript,alert('XSS')\"/></svg>`,\n+\n+\t\t// Event handlers\n+\t\t`<svg onload=\"alert('XSS')\"></svg>`,\n+\t\t`<svg><image onerror=\"alert('XSS')\" src=\"invalid\"/></svg>`,\n+\t\t`<svg><animate onbegin=\"alert('XSS')\"/></svg>`,\n+\n+\t\t// JavaScript protocols\n+\t\t`<svg><a href=\"javascript:alert('XSS')\"><text>Click</text></a></svg>`,\n+\t\t`<svg><use href=\"javascript:alert('XSS')\"/></svg>`,\n+\n+\t\t// ForeignObject\n+\t\t`<svg><foreignObject><body><script>alert('XSS')</script></body></foreignObject></svg>`,\n+\n+\t\t// Data URLs\n+\t\t`<svg><image href=\"data:text/html,<script>alert('XSS')</script>\"/></svg>`,\n+\t}\n+\n+\tfor i, vector := range xssVectors {\n+\t\tt.Run(string(rune(i)), func(t *testing.T) {\n+\t\t\tresult, err := sanitizeSVG([]byte(vector))\n+\n+\t\t\tif err != nil {\n+\t\t\t\t// Some vectors might produce invalid XML after sanitization - that's OK\n+\t\t\t\treturn\n+\t\t\t}\n+\n+\t\t\tresultStr := strings.ToLower(string(result))\n+\n+\t\t\t// Check that dangerous patterns are removed\n+\t\t\tdangerousPatterns := []string{\n+\t\t\t\t\"<script\", \"javascript:\", \"onerror=\", \"onload=\",\n+\t\t\t\t\"onclick=\", \"onmouseover=\", \"<foreignobject\",\n+\t\t\t\t\"alert(\", \"eval(\", \"prompt(\",\n+\t\t\t}\n+\n+\t\t\tfor _, pattern := range dangerousPatterns {\n+\t\t\t\tif strings.Contains(resultStr, pattern) {\n+\t\t\t\t\tt.Errorf(\"Sanitized SVG still contains dangerous pattern '%s': %s\", pattern, resultStr)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t})\n+\t}\n+}\n+\n+// Benchmark SVG sanitization\n+func BenchmarkSanitizeSVG(b *testing.B) {\n+\tsvgContent := []byte(`<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<svg xmlns=\"http://www.w3.org/2000/svg\">\n+  <script>alert('XSS')</script>\n+  <circle cx=\"50\" cy=\"50\" r=\"40\" fill=\"red\" onerror=\"alert('XSS')\"/>\n+  <rect x=\"10\" y=\"10\" width=\"30\" height=\"30\" fill=\"blue\"/>\n+  <a href=\"javascript:alert('XSS')\"><text>Click</text></a>\n+</svg>`)\n+\n+\tb.ResetTimer()\n+\tfor i := 0; i < b.N; i++ {\n+\t\t_, _ = sanitizeSVG(svgContent)\n+\t}\n+}\n+\n+// Benchmark file validation\n+func BenchmarkValidateFileContent(b *testing.B) {\n+\tcontent := bytes.Repeat([]byte{0xFF, 0xD8, 0xFF}, 1000)\n+\n+\tb.ResetTimer()\n+\tfor i := 0; i < b.N; i++ {\n+\t\t_ = validateFileContent(content, \"image/jpeg\")\n+\t}\n+}\n+\n+// Test JPEG file integrity validation\n+func TestValidateJPEG(t *testing.T) {\n+\ttests := []struct {\n+\t\tname    string\n+\t\tcontent []byte\n+\t\twantErr bool\n+\t}{\n+\t\t{\n+\t\t\tname:    \"Valid JPEG\",\n+\t\t\tcontent: []byte{0xFF, 0xD8, 0xFF, 0xE0, 0x00, 0x10, 0x4A, 0x46, 0x49, 0x46, 0xFF, 0xD9},\n+\t\t\twantErr: false,\n+\t\t},\n+\t\t{\n+\t\t\tname:    \"JPEG missing EOI\",\n+\t\t\tcontent: []byte{0xFF, 0xD8, 0xFF, 0xE0, 0x00, 0x10},\n+\t\t\twantErr: true,\n+\t\t},\n+\t\t{\n+\t\t\tname:    \"JPEG invalid SOI\",\n+\t\t\tcontent: []byte{0x00, 0x00, 0xFF, 0xE0, 0x00, 0x10, 0xFF, 0xD9},\n+\t\t\twantErr: true,\n+\t\t},\n+\t\t{\n+\t\t\tname:    \"File too small\",\n+\t\t\tcontent: []byte{0xFF, 0xD8},\n+\t\t\twantErr: true,\n+\t\t},\n+\t}\n+\n+\tfor _, tt := range tests {\n+\t\tt.Run(tt.name, func(t *testing.T) {\n+\t\t\terr := validateJPEG(tt.content)\n+\t\t\tif (err != nil) != tt.wantErr {\n+\t\t\t\tt.Errorf(\"validateJPEG() error = %v, wantErr %v\", err, tt.wantErr)\n+\t\t\t}\n+\t\t})\n+\t}\n+}\n+\n+// Test PNG file integrity validation\n+func TestValidatePNG(t *testing.T) {\n+\ttests := []struct {\n+\t\tname    string\n+\t\tcontent []byte\n+\t\twantErr bool\n+\t}{\n+\t\t{\n+\t\t\tname: \"Valid PNG\",\n+\t\t\tcontent: append(append(\n+\t\t\t\t[]byte{0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A}, // PNG signature\n+\t\t\t\tmake([]byte, 20)...), // IHDR chunk placeholder\n+\t\t\t\t[]byte{0x49, 0x45, 0x4E, 0x44, 0xAE, 0x42, 0x60, 0x82}...), // IEND chunk\n+\t\t\twantErr: false,\n+\t\t},\n+\t\t{\n+\t\t\tname:    \"PNG invalid signature\",\n+\t\t\tcontent: []byte{0x00, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A},\n+\t\t\twantErr: true,\n+\t\t},\n+\t\t{\n+\t\t\tname: \"PNG missing IEND\",\n+\t\t\tcontent: append(\n+\t\t\t\t[]byte{0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A},\n+\t\t\t\tmake([]byte, 30)...),\n+\t\t\twantErr: true,\n+\t\t},\n+\t}\n+\n+\tfor _, tt := range tests {\n+\t\tt.Run(tt.name, func(t *testing.T) {\n+\t\t\terr := validatePNG(tt.content)\n+\t\t\tif (err != nil) != tt.wantErr {\n+\t\t\t\tt.Errorf(\"validatePNG() error = %v, wantErr %v\", err, tt.wantErr)\n+\t\t\t}\n+\t\t})\n+\t}\n+}\n+\n+// Test GIF file integrity validation\n+func TestValidateGIF(t *testing.T) {\n+\ttests := []struct {\n+\t\tname    string\n+\t\tcontent []byte\n+\t\twantErr bool\n+\t}{\n+\t\t{\n+\t\t\tname: \"Valid GIF89a\",\n+\t\t\tcontent: append(append(\n+\t\t\t\t[]byte(\"GIF89a\"),\n+\t\t\t\tmake([]byte, 7)...), // Logical screen descriptor\n+\t\t\t\t0x3B), // Terminator\n+\t\t\twantErr: false,\n+\t\t},\n+\t\t{\n+\t\t\tname: \"Valid GIF87a\",\n+\t\t\tcontent: append(append(\n+\t\t\t\t[]byte(\"GIF87a\"),\n+\t\t\t\tmake([]byte, 7)...), // Logical screen descriptor\n+\t\t\t\t0x3B), // Terminator\n+\t\t\twantErr: false,\n+\t\t},\n+\t\t{\n+\t\t\tname:    \"Invalid GIF signature\",\n+\t\t\tcontent: []byte(\"PNG89a\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x3B\"),\n+\t\t\twantErr: true,\n+\t\t},\n+\t\t{\n+\t\t\tname:    \"GIF missing terminator\",\n+\t\t\tcontent: append([]byte(\"GIF89a\"), make([]byte, 10)...),\n+\t\t\twantErr: true,\n+\t\t},\n+\t}\n+\n+\tfor _, tt := range tests {\n+\t\tt.Run(tt.name, func(t *testing.T) {\n+\t\t\terr := validateGIF(tt.content)\n+\t\t\tif (err != nil) != tt.wantErr {\n+\t\t\t\tt.Errorf(\"validateGIF() error = %v, wantErr %v\", err, tt.wantErr)\n+\t\t\t}\n+\t\t})\n+\t}\n+}\n+\n+// Test WebP file integrity validation\n+func TestValidateWebP(t *testing.T) {\n+\ttests := []struct {\n+\t\tname    string\n+\t\tcontent []byte\n+\t\twantErr bool\n+\t}{\n+\t\t{\n+\t\t\tname: \"Valid WebP\",\n+\t\t\tcontent: append(append(\n+\t\t\t\t[]byte(\"RIFF\"),\n+\t\t\t\t[]byte{0x00, 0x00, 0x00, 0x00}...), // Size placeholder\n+\t\t\t\t[]byte(\"WEBP\")...),\n+\t\t\twantErr: false,\n+\t\t},\n+\t\t{\n+\t\t\tname:    \"Invalid RIFF header\",\n+\t\t\tcontent: []byte(\"PNG \\x00\\x00\\x00\\x00WEBP\"),\n+\t\t\twantErr: true,\n+\t\t},\n+\t\t{\n+\t\t\tname:    \"Invalid WEBP signature\",\n+\t\t\tcontent: []byte(\"RIFF\\x00\\x00\\x00\\x00JPEG\"),\n+\t\t\twantErr: true,\n+\t\t},\n+\t}\n+\n+\tfor _, tt := range tests {\n+\t\tt.Run(tt.name, func(t *testing.T) {\n+\t\t\terr := validateWebP(tt.content)\n+\t\t\tif (err != nil) != tt.wantErr {\n+\t\t\t\tt.Errorf(\"validateWebP() error = %v, wantErr %v\", err, tt.wantErr)\n+\t\t\t}\n+\t\t})\n+\t}\n+}\n+\n+// Test MP4 file integrity validation\n+func TestValidateMP4(t *testing.T) {\n+\ttests := []struct {\n+\t\tname    string\n+\t\tcontent []byte\n+\t\twantErr bool\n+\t}{\n+\t\t{\n+\t\t\tname:    \"Valid MP4 with isom brand\",\n+\t\t\tcontent: []byte{0x00, 0x00, 0x00, 0x20, 0x66, 0x74, 0x79, 0x70, 0x69, 0x73, 0x6F, 0x6D},\n+\t\t\twantErr: false,\n+\t\t},\n+\t\t{\n+\t\t\tname:    \"Valid MP4 with mp42 brand\",\n+\t\t\tcontent: []byte{0x00, 0x00, 0x00, 0x20, 0x66, 0x74, 0x79, 0x70, 0x6D, 0x70, 0x34, 0x32},\n+\t\t\twantErr: false,\n+\t\t},\n+\t\t{\n+\t\t\tname:    \"Invalid MP4 - missing ftyp\",\n+\t\t\tcontent: []byte{0x00, 0x00, 0x00, 0x20, 0x6D, 0x64, 0x61, 0x74},\n+\t\t\twantErr: true,\n+\t\t},\n+\t\t{\n+\t\t\tname:    \"File too small\",\n+\t\t\tcontent: []byte{0x00, 0x00},\n+\t\t\twantErr: true,\n+\t\t},\n+\t}\n+\n+\tfor _, tt := range tests {\n+\t\tt.Run(tt.name, func(t *testing.T) {\n+\t\t\terr := validateMP4(tt.content)\n+\t\t\tif (err != nil) != tt.wantErr {\n+\t\t\t\tt.Errorf(\"validateMP4() error = %v, wantErr %v\", err, tt.wantErr)\n+\t\t\t}\n+\t\t})\n+\t}\n+}\n+\n+// Test video file validations\n+func TestVideoValidations(t *testing.T) {\n+\tt.Run(\"WebM validation\", func(t *testing.T) {\n+\t\tvalidWebM := []byte{0x1A, 0x45, 0xDF, 0xA3, 0x00, 0x00}\n+\t\tif err := validateWebM(validWebM); err != nil {\n+\t\t\tt.Errorf(\"validateWebM() failed for valid WebM: %v\", err)\n+\t\t}\n+\n+\t\tinvalidWebM := []byte{0x00, 0x00, 0x00, 0x00}\n+\t\tif err := validateWebM(invalidWebM); err == nil {\n+\t\t\tt.Error(\"validateWebM() should fail for invalid WebM\")\n+\t\t}\n+\t})\n+\n+\tt.Run(\"OGG validation\", func(t *testing.T) {\n+\t\tvalidOGG := []byte(\"OggS\\x00\\x02\")\n+\t\tif err := validateOGG(validOGG); err != nil {\n+\t\t\tt.Errorf(\"validateOGG() failed for valid OGG: %v\", err)\n+\t\t}\n+\n+\t\tinvalidOGG := []byte(\"MPEG\")\n+\t\tif err := validateOGG(invalidOGG); err == nil {\n+\t\t\tt.Error(\"validateOGG() should fail for invalid OGG\")\n+\t\t}\n+\t})\n+\n+\tt.Run(\"AVI validation\", func(t *testing.T) {\n+\t\tvalidAVI := []byte(\"RIFF\\x00\\x00\\x00\\x00AVI \")\n+\t\tif err := validateAVI(validAVI); err != nil {\n+\t\t\tt.Errorf(\"validateAVI() failed for valid AVI: %v\", err)\n+\t\t}\n+\n+\t\tinvalidAVI := []byte(\"RIFF\\x00\\x00\\x00\\x00MP4 \")\n+\t\tif err := validateAVI(invalidAVI); err == nil {\n+\t\t\tt.Error(\"validateAVI() should fail for invalid AVI\")\n+\t\t}\n+\t})\n+}\n+\n+// Test polyglot file detection\n+func TestDetectPolyglot(t *testing.T) {\n+\ttests := []struct {\n+\t\tname       string\n+\t\tcontent    []byte\n+\t\tshouldFlag bool\n+\t}{\n+\t\t{\n+\t\t\tname:       \"Clean JPEG\",\n+\t\t\tcontent:    append([]byte{0xFF, 0xD8, 0xFF}, make([]byte, 200)...),\n+\t\t\tshouldFlag: false,\n+\t\t},\n+\t\t{\n+\t\t\tname:       \"JPEG with embedded script tag\",\n+\t\t\tcontent:    append(append([]byte{0xFF, 0xD8, 0xFF}, make([]byte, 100)...), []byte(\"<script>alert('xss')</script>\")...),\n+\t\t\tshouldFlag: true,\n+\t\t},\n+\t\t{\n+\t\t\tname:       \"File with PHP code\",\n+\t\t\tcontent:    append([]byte(\"<?php system($_GET['cmd']); ?>\"), make([]byte, 80)...),\n+\t\t\tshouldFlag: true,\n+\t\t},\n+\t\t{\n+\t\t\tname:       \"File with shell script\",\n+\t\t\tcontent:    append([]byte(\"#!/bin/bash\\nrm -rf /\"), make([]byte, 80)...),\n+\t\t\tshouldFlag: true,\n+\t\t},\n+\t\t{\n+\t\t\tname:       \"File with JavaScript\",\n+\t\t\tcontent:    append(append([]byte{0xFF, 0xD8, 0xFF}, make([]byte, 100)...), []byte(\"javascript:alert(1)\")...),\n+\t\t\tshouldFlag: true,\n+\t\t},\n+\t\t{\n+\t\t\tname:       \"File with eval\",\n+\t\t\tcontent:    append(make([]byte, 100), []byte(\"eval(atob('YWxlcnQoMSk='))\")...),\n+\t\t\tshouldFlag: true,\n+\t\t},\n+\t}\n+\n+\tfor _, tt := range tests {\n+\t\tt.Run(tt.name, func(t *testing.T) {\n+\t\t\tresult := detectPolyglot(tt.content)\n+\t\t\tif result != tt.shouldFlag {\n+\t\t\t\tt.Errorf(\"detectPolyglot() = %v, want %v\", result, tt.shouldFlag)\n+\t\t\t}\n+\t\t})\n+\t}\n+}\n+\n+// Test file integrity function dispatcher\n+func TestValidateFileIntegrity(t *testing.T) {\n+\ttests := []struct {\n+\t\tname     string\n+\t\tcontent  []byte\n+\t\tmimeType string\n+\t\twantErr  bool\n+\t}{\n+\t\t{\n+\t\t\tname:     \"Valid JPEG\",\n+\t\t\tcontent:  []byte{0xFF, 0xD8, 0xFF, 0xE0, 0x00, 0x10, 0xFF, 0xD9},\n+\t\t\tmimeType: \"image/jpeg\",\n+\t\t\twantErr:  false,\n+\t\t},\n+\t\t{\n+\t\t\tname:     \"Invalid JPEG\",\n+\t\t\tcontent:  []byte{0x00, 0x00, 0x00, 0x00},\n+\t\t\tmimeType: \"image/jpeg\",\n+\t\t\twantErr:  true,\n+\t\t},\n+\t\t{\n+\t\t\tname: \"Valid PNG\",\n+\t\t\tcontent: append(append(\n+\t\t\t\t[]byte{0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A},\n+\t\t\t\tmake([]byte, 20)...),\n+\t\t\t\t[]byte{0x49, 0x45, 0x4E, 0x44, 0xAE, 0x42, 0x60, 0x82}...),\n+\t\t\tmimeType: \"image/png\",\n+\t\t\twantErr:  false,\n+\t\t},\n+\t\t{\n+\t\t\tname:     \"SVG (no validation needed)\",\n+\t\t\tcontent:  []byte(\"<svg></svg>\"),\n+\t\t\tmimeType: \"image/svg+xml\",\n+\t\t\twantErr:  false,\n+\t\t},\n+\t}\n+\n+\tfor _, tt := range tests {\n+\t\tt.Run(tt.name, func(t *testing.T) {\n+\t\t\terr := validateFileIntegrity(tt.content, tt.mimeType)\n+\t\t\tif (err != nil) != tt.wantErr {\n+\t\t\t\tt.Errorf(\"validateFileIntegrity() error = %v, wantErr %v\", err, tt.wantErr)\n+\t\t\t}\n+\t\t})\n+\t}\n+}\n+\n+// Benchmark integrity validations\n+func BenchmarkValidateJPEG(b *testing.B) {\n+\tcontent := append([]byte{0xFF, 0xD8, 0xFF, 0xE0}, make([]byte, 10000)...)\n+\tcontent = append(content, []byte{0xFF, 0xD9}...)\n+\n+\tb.ResetTimer()\n+\tfor i := 0; i < b.N; i++ {\n+\t\t_ = validateJPEG(content)\n+\t}\n+}\n+\n+func BenchmarkValidatePNG(b *testing.B) {\n+\tcontent := append(append(\n+\t\t[]byte{0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A},\n+\t\tmake([]byte, 10000)...),\n+\t\t[]byte{0x49, 0x45, 0x4E, 0x44, 0xAE, 0x42, 0x60, 0x82}...)\n+\n+\tb.ResetTimer()\n+\tfor i := 0; i < b.N; i++ {\n+\t\t_ = validatePNG(content)\n+\t}\n+}\n+\n+func BenchmarkDetectPolyglot(b *testing.B) {\n+\tcontent := append([]byte{0xFF, 0xD8, 0xFF}, make([]byte, 1000)...)\n+\n+\tb.ResetTimer()\n+\tfor i := 0; i < b.N; i++ {\n+\t\t_ = detectPolyglot(content)\n+\t}\n+}\ndiff --git a/docs/CHANGELOG.md b/docs/CHANGELOG.md\nindex 56a721e..9b7dd88 100644\n--- a/docs/CHANGELOG.md\n+++ b/docs/CHANGELOG.md\n@@ -5,6 +5,48 @@ All notable changes to KUNO Blog Platform will be documented in this file.\n The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\n and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n \n+## [1.3.14] - 2025-10-03\n+\n+### Security\n+- **Fixed critical XSS vulnerability** in file upload functionality (Stored XSS)\n+- Implemented 7-layer defense-in-depth security architecture for media uploads\n+- Added magic number validation to prevent Content-Type spoofing attacks\n+- Implemented comprehensive file integrity checks for all supported formats:\n+  - JPEG: SOI/EOI marker validation\n+  - PNG: Signature validation with PNG bomb protection (100 chunk limit)\n+  - GIF: Signature validation with GIF bomb protection (1000 frame limit)\n+  - WebP: RIFF/WEBP signature validation\n+  - MP4, WebM, OGG, AVI: Video format signature validation\n+- Added polyglot file detection to prevent multi-format hybrid attacks\n+- Implemented SVG content sanitization:\n+  - Removes all dangerous elements (script, foreignObject, iframe, object, embed, animations)\n+  - Strips all event handlers (onload, onerror, onclick, etc.)\n+  - Sanitizes dangerous protocols in href/xlink:href (javascript:, data:, vbscript:)\n+- Added strict security response headers:\n+  - X-Content-Type-Options: nosniff\n+  - X-Frame-Options: DENY\n+  - Content-Security-Policy for SVG files (prevents script execution)\n+- File extension whitelist enforcement\n+\n+### Added\n+- SVG file upload support with automatic sanitization\n+- 48 comprehensive security test cases (100% pass rate)\n+- Security warning in frontend for SVG uploads\n+- Helper function minInt() for safe integer operations\n+\n+### Changed\n+- Enhanced UploadMedia endpoint with 4-layer validation\n+- Enhanced ServeMedia endpoint with security headers\n+- Improved frontend media upload component with SVG support\n+\n+### Technical Details\n+- Complies with OWASP Top 10 - A03:2021 Injection\n+- Complies with OWASP ASVS Level 2 - File Upload Validation\n+- Addresses CWE-79 (Cross-Site Scripting)\n+- Addresses CWE-434 (Unrestricted File Upload)\n+- Test coverage: 48 test cases validating all security measures\n+- Files modified: media.go (730 lines), media_test.go (new), media-upload.tsx\n+\n ## [1.3.9] - 2025-09-06\n \n ### Fixed\ndiff --git a/frontend/src/components/admin/media-upload.tsx b/frontend/src/components/admin/media-upload.tsx\nindex fa2ba5b..6331a72 100644\n--- a/frontend/src/components/admin/media-upload.tsx\n+++ b/frontend/src/components/admin/media-upload.tsx\n@@ -36,11 +36,11 @@ export default function MediaUpload({\n   const getAcceptString = () => {\n     switch (acceptedTypes) {\n       case 'image':\n-        return 'image/jpeg,image/jpg,image/png,image/gif,image/webp'\n+        return 'image/jpeg,image/jpg,image/png,image/gif,image/webp,image/svg+xml'\n       case 'video':\n         return 'video/mp4,video/webm,video/ogg,video/avi,video/mov'\n       default:\n-        return 'image/jpeg,image/jpg,image/png,image/gif,image/webp,video/mp4,video/webm,video/ogg,video/avi,video/mov'\n+        return 'image/jpeg,image/jpg,image/png,image/gif,image/webp,image/svg+xml,video/mp4,video/webm,video/ogg,video/avi,video/mov'\n     }\n   }\n \n@@ -278,6 +278,14 @@ export default function MediaUpload({\n               </div>\n             )}\n \n+            {selectedFile.type === 'image/svg+xml' && (\n+              <Alert>\n+                <AlertDescription className=\"text-sm\">\n+                  \u26a0\ufe0f SVG files will be automatically sanitized to remove any scripts for security. Only safe SVG graphics elements will be preserved.\n+                </AlertDescription>\n+              </Alert>\n+            )}\n+\n             {uploading && (\n               <div className=\"space-y-2\">\n                 <div className=\"flex justify-between text-sm\">\n"
  },
  "CWE-416": {
    "cve": "CVE-2024-43374",
    "commit_url": "https://github.com/vim/vim/commit/0a6e57b09bc8c76691b367a5babfb79b31b770e8",
    "diff": "diff --git a/src/arglist.c b/src/arglist.c\nindex 187e16e8354b14..8825c8e252ccc5 100644\n--- a/src/arglist.c\n+++ b/src/arglist.c\n@@ -184,6 +184,8 @@ alist_set(\n /*\n  * Add file \"fname\" to argument list \"al\".\n  * \"fname\" must have been allocated and \"al\" must have been checked for room.\n+ *\n+ * May trigger Buf* autocommands\n  */\n     void\n alist_add(\n@@ -196,6 +198,7 @@ alist_add(\n     if (check_arglist_locked() == FAIL)\n \treturn;\n     arglist_locked = TRUE;\n+    curwin->w_locked = TRUE;\n \n #ifdef BACKSLASH_IN_FILENAME\n     slash_adjust(fname);\n@@ -207,6 +210,7 @@ alist_add(\n     ++al->al_ga.ga_len;\n \n     arglist_locked = FALSE;\n+    curwin->w_locked = FALSE;\n }\n \n #if defined(BACKSLASH_IN_FILENAME) || defined(PROTO)\n@@ -365,6 +369,7 @@ alist_add_list(\n \t    mch_memmove(&(ARGLIST[after + count]), &(ARGLIST[after]),\n \t\t\t\t       (ARGCOUNT - after) * sizeof(aentry_T));\n \targlist_locked = TRUE;\n+\tcurwin->w_locked = TRUE;\n \tfor (i = 0; i < count; ++i)\n \t{\n \t    int flags = BLN_LISTED | (will_edit ? BLN_CURBUF : 0);\n@@ -373,6 +378,7 @@ alist_add_list(\n \t    ARGLIST[after + i].ae_fnum = buflist_add(files[i], flags);\n \t}\n \targlist_locked = FALSE;\n+\tcurwin->w_locked = FALSE;\n \tALIST(curwin)->al_ga.ga_len += count;\n \tif (old_argcount > 0 && curwin->w_arg_idx >= after)\n \t    curwin->w_arg_idx += count;\ndiff --git a/src/buffer.c b/src/buffer.c\nindex 447ce76d49a328..34500e4abc2821 100644\n--- a/src/buffer.c\n+++ b/src/buffer.c\n@@ -1484,7 +1484,7 @@ do_buffer_ext(\n \t// (unless it's the only window).  Repeat this so long as we end up in\n \t// a window with this buffer.\n \twhile (buf == curbuf\n-\t\t   && !(curwin->w_closing || curwin->w_buffer->b_locked > 0)\n+\t\t   && !(win_locked(curwin) || curwin->w_buffer->b_locked > 0)\n \t\t   && (!ONE_WINDOW || first_tabpage->tp_next != NULL))\n \t{\n \t    if (win_close(curwin, FALSE) == FAIL)\n@@ -5470,7 +5470,7 @@ ex_buffer_all(exarg_T *eap)\n \t\t\t    : wp->w_width != Columns)\n \t\t\t|| (had_tab > 0 && wp != firstwin))\n \t\t    && !ONE_WINDOW\n-\t\t    && !(wp->w_closing || wp->w_buffer->b_locked > 0)\n+\t\t    && !(win_locked(wp) || wp->w_buffer->b_locked > 0)\n \t\t    && !win_unlisted(wp))\n \t    {\n \t\tif (win_close(wp, FALSE) == FAIL)\ndiff --git a/src/ex_cmds.c b/src/ex_cmds.c\nindex 05778c8fd8b9c6..349269a2bb8b69 100644\n--- a/src/ex_cmds.c\n+++ b/src/ex_cmds.c\n@@ -2840,7 +2840,7 @@ do_ecmd(\n \n \t\t// Set the w_closing flag to avoid that autocommands close the\n \t\t// window.  And set b_locked for the same reason.\n-\t\tthe_curwin->w_closing = TRUE;\n+\t\tthe_curwin->w_locked = TRUE;\n \t\t++buf->b_locked;\n \n \t\tif (curbuf == old_curbuf.br_buf)\n@@ -2854,7 +2854,7 @@ do_ecmd(\n \n \t\t// Autocommands may have closed the window.\n \t\tif (win_valid(the_curwin))\n-\t\t    the_curwin->w_closing = FALSE;\n+\t\t    the_curwin->w_locked = FALSE;\n \t\t--buf->b_locked;\n \n #ifdef FEAT_EVAL\ndiff --git a/src/proto/window.pro b/src/proto/window.pro\nindex 26c7040b8a1b4d..441070ebfcb8e8 100644\n--- a/src/proto/window.pro\n+++ b/src/proto/window.pro\n@@ -100,4 +100,5 @@ int get_win_number(win_T *wp, win_T *first_win);\n int get_tab_number(tabpage_T *tp);\n char *check_colorcolumn(win_T *wp);\n int get_last_winid(void);\n+int win_locked(win_T *wp);\n /* vim: set ft=c : */\ndiff --git a/src/structs.h b/src/structs.h\nindex fe4704a367949c..abda3a0c38b4ea 100644\n--- a/src/structs.h\n+++ b/src/structs.h\n@@ -3785,7 +3785,7 @@ struct window_S\n     synblock_T\t*w_s;\t\t    // for :ownsyntax\n #endif\n \n-    int\t\tw_closing;\t    // window is being closed, don't let\n+    int\t\tw_locked;\t    // window is being closed, don't let\n \t\t\t\t    // autocommands close it too.\n \n     frame_T\t*w_frame;\t    // frame containing this window\ndiff --git a/src/terminal.c b/src/terminal.c\nindex 1fc0ef96881f95..f80196096df49e 100644\n--- a/src/terminal.c\n+++ b/src/terminal.c\n@@ -3680,10 +3680,10 @@ term_after_channel_closed(term_T *term)\n \t\tif (is_aucmd_win(curwin))\n \t\t    do_set_w_closing = TRUE;\n \t\tif (do_set_w_closing)\n-\t\t    curwin->w_closing = TRUE;\n+\t\t    curwin->w_locked = TRUE;\n \t\tdo_bufdel(DOBUF_WIPE, (char_u *)\"\", 1, fnum, fnum, FALSE);\n \t\tif (do_set_w_closing)\n-\t\t    curwin->w_closing = FALSE;\n+\t\t    curwin->w_locked = FALSE;\n \t\taucmd_restbuf(&aco);\n \t    }\n #ifdef FEAT_PROP_POPUP\ndiff --git a/src/testdir/test_arglist.vim b/src/testdir/test_arglist.vim\nindex edc8b77429e20a..8d81a828b3e03d 100644\n--- a/src/testdir/test_arglist.vim\n+++ b/src/testdir/test_arglist.vim\n@@ -359,6 +359,7 @@ func Test_argv()\n   call assert_equal('', argv(1, 100))\n   call assert_equal([], argv(-1, 100))\n   call assert_equal('', argv(10, -1))\n+  %argdelete\n endfunc\n \n \" Test for the :argedit command\n@@ -744,4 +745,26 @@ func Test_all_command()\n   %bw!\n endfunc\n \n+\" Test for deleting buffer when creating an arglist. This was accessing freed\n+\" memory\n+func Test_crash_arglist_uaf()\n+  \"%argdelete\n+  new one\n+  au BufAdd XUAFlocal :bw\n+  \"call assert_fails(':arglocal XUAFlocal', 'E163:')\n+  arglocal XUAFlocal\n+  au! BufAdd\n+  bw! XUAFlocal\n+\n+  au BufAdd XUAFlocal2 :bw\n+  new two\n+  new three\n+  arglocal\n+  argadd XUAFlocal2 Xfoobar\n+  bw! XUAFlocal2\n+  bw! two\n+\n+  au! BufAdd\n+endfunc\n+\n \" vim: shiftwidth=2 sts=2 expandtab\ndiff --git a/src/version.c b/src/version.c\nindex 78ccd20e5a1aa6..08b3fafd7b4527 100644\n--- a/src/version.c\n+++ b/src/version.c\n@@ -704,6 +704,8 @@ static char *(features[]) =\n \n static int included_patches[] =\n {   /* Add new patch number below this line */\n+/**/\n+    678,\n /**/\n     677,\n /**/\ndiff --git a/src/window.c b/src/window.c\nindex 43a15e0561f2cb..b2c90c7d64114d 100644\n--- a/src/window.c\n+++ b/src/window.c\n@@ -2511,7 +2511,7 @@ close_windows(\n     for (wp = firstwin; wp != NULL && !ONE_WINDOW; )\n     {\n \tif (wp->w_buffer == buf && (!keep_curwin || wp != curwin)\n-\t\t&& !(wp->w_closing || wp->w_buffer->b_locked > 0))\n+\t\t&& !(win_locked(wp) || wp->w_buffer->b_locked > 0))\n \t{\n \t    if (win_close(wp, FALSE) == FAIL)\n \t\t// If closing the window fails give up, to avoid looping\n@@ -2532,7 +2532,7 @@ close_windows(\n \tif (tp != curtab)\n \t    FOR_ALL_WINDOWS_IN_TAB(tp, wp)\n \t\tif (wp->w_buffer == buf\n-\t\t    && !(wp->w_closing || wp->w_buffer->b_locked > 0))\n+\t\t    && !(win_locked(wp) || wp->w_buffer->b_locked > 0))\n \t\t{\n \t\t    win_close_othertab(wp, FALSE, tp);\n \n@@ -2654,10 +2654,10 @@ win_close_buffer(win_T *win, int action, int abort_if_last)\n \tbufref_T    bufref;\n \n \tset_bufref(&bufref, curbuf);\n-\twin->w_closing = TRUE;\n+\twin->w_locked = TRUE;\n \tclose_buffer(win, win->w_buffer, action, abort_if_last, TRUE);\n \tif (win_valid_any_tab(win))\n-\t    win->w_closing = FALSE;\n+\t    win->w_locked = FALSE;\n \t// Make sure curbuf is valid. It can become invalid if 'bufhidden' is\n \t// \"wipe\".\n \tif (!bufref_valid(&bufref))\n@@ -2705,7 +2705,7 @@ win_close(win_T *win, int free_buf)\n     if (window_layout_locked(CMD_close))\n \treturn FAIL;\n \n-    if (win->w_closing || (win->w_buffer != NULL\n+    if (win_locked(win) || (win->w_buffer != NULL\n \t\t\t\t\t       && win->w_buffer->b_locked > 0))\n \treturn FAIL; // window is already being closed\n     if (win_unlisted(win))\n@@ -2754,19 +2754,19 @@ win_close(win_T *win, int free_buf)\n \t    other_buffer = TRUE;\n \t    if (!win_valid(win))\n \t\treturn FAIL;\n-\t    win->w_closing = TRUE;\n+\t    win->w_locked = TRUE;\n \t    apply_autocmds(EVENT_BUFLEAVE, NULL, NULL, FALSE, curbuf);\n \t    if (!win_valid(win))\n \t\treturn FAIL;\n-\t    win->w_closing = FALSE;\n+\t    win->w_locked = FALSE;\n \t    if (last_window())\n \t\treturn FAIL;\n \t}\n-\twin->w_closing = TRUE;\n+\twin->w_locked = TRUE;\n \tapply_autocmds(EVENT_WINLEAVE, NULL, NULL, FALSE, curbuf);\n \tif (!win_valid(win))\n \t    return FAIL;\n-\twin->w_closing = FALSE;\n+\twin->w_locked = FALSE;\n \tif (last_window())\n \t    return FAIL;\n #ifdef FEAT_EVAL\n@@ -3346,7 +3346,7 @@ win_close_othertab(win_T *win, int free_buf, tabpage_T *tp)\n \n     // Get here with win->w_buffer == NULL when win_close() detects the tab\n     // page changed.\n-    if (win->w_closing || (win->w_buffer != NULL\n+    if (win_locked(win) || (win->w_buffer != NULL\n \t\t\t\t\t       && win->w_buffer->b_locked > 0))\n \treturn; // window is already being closed\n \n@@ -8000,3 +8000,12 @@ get_last_winid(void)\n {\n     return last_win_id;\n }\n+\n+/*\n+ * Don't let autocommands close the given window\n+ */\n+   int\n+win_locked(win_T *wp)\n+{\n+    return wp->w_locked;\n+}\n"
  },
  "CWE-284": {
    "cve": "CVE-2024-30261",
    "commit_url": "https://github.com/nodejs/undici/commit/2b39440bd9ded841c93dd72138f3b1763ae26055",
    "diff": "diff --git a/benchmarks/fetch/bytes-match.mjs b/benchmarks/fetch/bytes-match.mjs\nnew file mode 100644\nindex 00000000000..6c6b263499d\n--- /dev/null\n+++ b/benchmarks/fetch/bytes-match.mjs\n@@ -0,0 +1,24 @@\n+import { createHash } from 'node:crypto'\n+import { bench, run } from 'mitata'\n+import { bytesMatch } from '../../lib/web/fetch/util.js'\n+\n+const body = Buffer.from('Hello world!')\n+const validSha256Base64 = `sha256-${createHash('sha256').update(body).digest('base64')}`\n+const invalidSha256Base64 = `sha256-${createHash('sha256').update(body).digest('base64')}`\n+const validSha256Base64Url = `sha256-${createHash('sha256').update(body).digest('base64url')}`\n+const invalidSha256Base64Url = `sha256-${createHash('sha256').update(body).digest('base64url')}`\n+\n+bench('bytesMatch valid sha256 and base64', () => {\n+  bytesMatch(body, validSha256Base64)\n+})\n+bench('bytesMatch invalid sha256 and base64', () => {\n+  bytesMatch(body, invalidSha256Base64)\n+})\n+bench('bytesMatch valid sha256 and base64url', () => {\n+  bytesMatch(body, validSha256Base64Url)\n+})\n+bench('bytesMatch invalid sha256 and base64url', () => {\n+  bytesMatch(body, invalidSha256Base64Url)\n+})\n+\n+await run()\ndiff --git a/lib/fetch/util.js b/lib/fetch/util.js\nindex b12142c7f42..ede4ec4897a 100644\n--- a/lib/fetch/util.js\n+++ b/lib/fetch/util.js\n@@ -7,14 +7,18 @@ const { isBlobLike, toUSVString, ReadableStreamFrom } = require('../core/util')\n const assert = require('assert')\n const { isUint8Array } = require('util/types')\n \n+let supportedHashes = []\n+\n // https://nodejs.org/api/crypto.html#determining-if-crypto-support-is-unavailable\n /** @type {import('crypto')|undefined} */\n let crypto\n \n try {\n   crypto = require('crypto')\n+  const possibleRelevantHashes = ['sha256', 'sha384', 'sha512']\n+  supportedHashes = crypto.getHashes().filter((hash) => possibleRelevantHashes.includes(hash))\n+/* c8 ignore next 3 */\n } catch {\n-\n }\n \n function responseURL (response) {\n@@ -542,66 +546,56 @@ function bytesMatch (bytes, metadataList) {\n     return true\n   }\n \n-  // 3. If parsedMetadata is the empty set, return true.\n+  // 3. If response is not eligible for integrity validation, return false.\n+  // TODO\n+\n+  // 4. If parsedMetadata is the empty set, return true.\n   if (parsedMetadata.length === 0) {\n     return true\n   }\n \n-  // 4. Let metadata be the result of getting the strongest\n+  // 5. Let metadata be the result of getting the strongest\n   //    metadata from parsedMetadata.\n-  const list = parsedMetadata.sort((c, d) => d.algo.localeCompare(c.algo))\n-  // get the strongest algorithm\n-  const strongest = list[0].algo\n-  // get all entries that use the strongest algorithm; ignore weaker\n-  const metadata = list.filter((item) => item.algo === strongest)\n+  const strongest = getStrongestMetadata(parsedMetadata)\n+  const metadata = filterMetadataListByAlgorithm(parsedMetadata, strongest)\n \n-  // 5. For each item in metadata:\n+  // 6. For each item in metadata:\n   for (const item of metadata) {\n     // 1. Let algorithm be the alg component of item.\n     const algorithm = item.algo\n \n     // 2. Let expectedValue be the val component of item.\n-    let expectedValue = item.hash\n+    const expectedValue = item.hash\n \n     // See https://github.com/web-platform-tests/wpt/commit/e4c5cc7a5e48093220528dfdd1c4012dc3837a0e\n     // \"be liberal with padding\". This is annoying, and it's not even in the spec.\n \n-    if (expectedValue.endsWith('==')) {\n-      expectedValue = expectedValue.slice(0, -2)\n-    }\n-\n     // 3. Let actualValue be the result of applying algorithm to bytes.\n     let actualValue = crypto.createHash(algorithm).update(bytes).digest('base64')\n \n-    if (actualValue.endsWith('==')) {\n-      actualValue = actualValue.slice(0, -2)\n+    if (actualValue[actualValue.length - 1] === '=') {\n+      if (actualValue[actualValue.length - 2] === '=') {\n+        actualValue = actualValue.slice(0, -2)\n+      } else {\n+        actualValue = actualValue.slice(0, -1)\n+      }\n     }\n \n     // 4. If actualValue is a case-sensitive match for expectedValue,\n     //    return true.\n-    if (actualValue === expectedValue) {\n-      return true\n-    }\n-\n-    let actualBase64URL = crypto.createHash(algorithm).update(bytes).digest('base64url')\n-\n-    if (actualBase64URL.endsWith('==')) {\n-      actualBase64URL = actualBase64URL.slice(0, -2)\n-    }\n-\n-    if (actualBase64URL === expectedValue) {\n+    if (compareBase64Mixed(actualValue, expectedValue)) {\n       return true\n     }\n   }\n \n-  // 6. Return false.\n+  // 7. Return false.\n   return false\n }\n \n // https://w3c.github.io/webappsec-subresource-integrity/#grammardef-hash-with-options\n // https://www.w3.org/TR/CSP2/#source-list-syntax\n // https://www.rfc-editor.org/rfc/rfc5234#appendix-B.1\n-const parseHashWithOptions = /((?<algo>sha256|sha384|sha512)-(?<hash>[A-z0-9+/]{1}.*={0,2}))( +[\\x21-\\x7e]?)?/i\n+const parseHashWithOptions = /(?<algo>sha256|sha384|sha512)-((?<hash>[A-Za-z0-9+/]+|[A-Za-z0-9_-]+)={0,2}(?:\\s|$)( +[!-~]*)?)?/i\n \n /**\n  * @see https://w3c.github.io/webappsec-subresource-integrity/#parse-metadata\n@@ -615,8 +609,6 @@ function parseMetadata (metadata) {\n   // 2. Let empty be equal to true.\n   let empty = true\n \n-  const supportedHashes = crypto.getHashes()\n-\n   // 3. For each token returned by splitting metadata on spaces:\n   for (const token of metadata.split(' ')) {\n     // 1. Set empty to false.\n@@ -626,7 +618,11 @@ function parseMetadata (metadata) {\n     const parsedToken = parseHashWithOptions.exec(token)\n \n     // 3. If token does not parse, continue to the next token.\n-    if (parsedToken === null || parsedToken.groups === undefined) {\n+    if (\n+      parsedToken === null ||\n+      parsedToken.groups === undefined ||\n+      parsedToken.groups.algo === undefined\n+    ) {\n       // Note: Chromium blocks the request at this point, but Firefox\n       // gives a warning that an invalid integrity was given. The\n       // correct behavior is to ignore these, and subsequently not\n@@ -635,11 +631,11 @@ function parseMetadata (metadata) {\n     }\n \n     // 4. Let algorithm be the hash-algo component of token.\n-    const algorithm = parsedToken.groups.algo\n+    const algorithm = parsedToken.groups.algo.toLowerCase()\n \n     // 5. If algorithm is a hash function recognized by the user\n     //    agent, add the parsed token to result.\n-    if (supportedHashes.includes(algorithm.toLowerCase())) {\n+    if (supportedHashes.includes(algorithm)) {\n       result.push(parsedToken.groups)\n     }\n   }\n@@ -652,6 +648,82 @@ function parseMetadata (metadata) {\n   return result\n }\n \n+/**\n+ * @param {{ algo: 'sha256' | 'sha384' | 'sha512' }[]} metadataList\n+ */\n+function getStrongestMetadata (metadataList) {\n+  // Let algorithm be the algo component of the first item in metadataList.\n+  // Can be sha256\n+  let algorithm = metadataList[0].algo\n+  // If the algorithm is sha512, then it is the strongest\n+  // and we can return immediately\n+  if (algorithm[3] === '5') {\n+    return algorithm\n+  }\n+\n+  for (let i = 1; i < metadataList.length; ++i) {\n+    const metadata = metadataList[i]\n+    // If the algorithm is sha512, then it is the strongest\n+    // and we can break the loop immediately\n+    if (metadata.algo[3] === '5') {\n+      algorithm = 'sha512'\n+      break\n+    // If the algorithm is sha384, then a potential sha256 or sha384 is ignored\n+    } else if (algorithm[3] === '3') {\n+      continue\n+    // algorithm is sha256, check if algorithm is sha384 and if so, set it as\n+    // the strongest\n+    } else if (metadata.algo[3] === '3') {\n+      algorithm = 'sha384'\n+    }\n+  }\n+  return algorithm\n+}\n+\n+function filterMetadataListByAlgorithm (metadataList, algorithm) {\n+  if (metadataList.length === 1) {\n+    return metadataList\n+  }\n+\n+  let pos = 0\n+  for (let i = 0; i < metadataList.length; ++i) {\n+    if (metadataList[i].algo === algorithm) {\n+      metadataList[pos++] = metadataList[i]\n+    }\n+  }\n+\n+  metadataList.length = pos\n+\n+  return metadataList\n+}\n+\n+/**\n+ * Compares two base64 strings, allowing for base64url\n+ * in the second string.\n+ *\n+* @param {string} actualValue always base64\n+ * @param {string} expectedValue base64 or base64url\n+ * @returns {boolean}\n+ */\n+function compareBase64Mixed (actualValue, expectedValue) {\n+  if (actualValue.length !== expectedValue.length) {\n+    return false\n+  }\n+  for (let i = 0; i < actualValue.length; ++i) {\n+    if (actualValue[i] !== expectedValue[i]) {\n+      if (\n+        (actualValue[i] === '+' && expectedValue[i] === '-') ||\n+        (actualValue[i] === '/' && expectedValue[i] === '_')\n+      ) {\n+        continue\n+      }\n+      return false\n+    }\n+  }\n+\n+  return true\n+}\n+\n // https://w3c.github.io/webappsec-upgrade-insecure-requests/#upgrade-request\n function tryUpgradeRequestToAPotentiallyTrustworthyURL (request) {\n   // TODO\n@@ -1067,5 +1139,6 @@ module.exports = {\n   urlHasHttpsScheme,\n   urlIsHttpHttpsScheme,\n   readAllBytes,\n-  normalizeMethodRecord\n+  normalizeMethodRecord,\n+  parseMetadata\n }\ndiff --git a/test/fetch/integrity.js b/test/fetch/integrity.js\nindex f91f69314ee..90af8918cc5 100644\n--- a/test/fetch/integrity.js\n+++ b/test/fetch/integrity.js\n@@ -4,6 +4,16 @@ const { test } = require('tap')\n const { createServer } = require('http')\n const { createHash, getHashes } = require('crypto')\n const { gzipSync } = require('zlib')\n+/*\n+=======\n+const { test, after } = require('node:test')\n+const { tspl } = require('@matteo.collina/tspl')\n+const assert = require('node:assert')\n+const { createServer } = require('node:http')\n+const { createHash, getHashes } = require('node:crypto')\n+const { gzipSync } = require('node:zlib')\n+>>>>>>> d542b8cd (Merge pull request from GHSA-9qxr-qj54-h672)\n+*/\n const { fetch, setGlobalDispatcher, Agent } = require('../..')\n const { once } = require('events')\n \n@@ -148,3 +158,190 @@ test('request with sha512 hash', { skip: !supportedHashes.includes('sha512') },\n     integrity: 'sha512-ypeBEsobvcr6wjGzmiPcTaeG7/gUfE5yuYB3ha/uSLs='\n   }))\n })\n+\n+test('request with correct integrity checksum (base64url)', (t) => {\n+  t.plan(1)\n+  const body = 'Hello world!'\n+  const hash = createHash('sha256').update(body).digest('base64url')\n+\n+  const server = createServer((req, res) => {\n+    res.end(body)\n+  })\n+\n+  t.teardown(server.close.bind(server))\n+\n+  server.listen(0, async () => {\n+    const response = await fetch(`http://localhost:${server.address().port}`, {\n+      integrity: `sha256-${hash}`\n+    })\n+    t.equal(body, await response.text())\n+  })\n+})\n+\n+test('request with incorrect integrity checksum (base64url)', (t) => {\n+  t.plan(1)\n+\n+  const body = 'Hello world!'\n+  const hash = createHash('sha256').update('invalid').digest('base64url')\n+\n+  const server = createServer((req, res) => {\n+    res.end(body)\n+  })\n+\n+  t.teardown(server.close.bind(server))\n+\n+  server.listen(0, async () => {\n+    await t.rejects(fetch(`http://localhost:${server.address().port}`, {\n+      integrity: `sha256-${hash}`\n+    }))\n+  })\n+})\n+\n+test('request with incorrect integrity checksum (only dash)', (t) => {\n+  t.plan(1)\n+\n+  const body = 'Hello world!'\n+\n+  const server = createServer((req, res) => {\n+    res.end(body)\n+  })\n+\n+  t.teardown(server.close.bind(server))\n+\n+  server.listen(0, async () => {\n+    await t.rejects(fetch(`http://localhost:${server.address().port}`, {\n+      integrity: 'sha256--'\n+    }))\n+  })\n+})\n+\n+test('request with incorrect integrity checksum (non-ascii character)', (t) => {\n+  t.plan(1)\n+\n+  const body = 'Hello world!'\n+\n+  const server = createServer((req, res) => {\n+    res.end(body)\n+  })\n+\n+  t.teardown(server.close.bind(server))\n+\n+  server.listen(0, async () => {\n+    await t.rejects(() => fetch(`http://localhost:${server.address().port}`, {\n+      integrity: 'sha256-\u00e4'\n+    }))\n+  })\n+})\n+\n+test('request with incorrect stronger integrity checksum (non-ascii character)', (t) => {\n+  t.plan(2)\n+\n+  const body = 'Hello world!'\n+  const sha256 = createHash('sha256').update(body).digest('base64')\n+  const sha384 = '\u00e4'\n+\n+  const server = createServer((req, res) => {\n+    res.end(body)\n+  })\n+\n+  t.teardown(server.close.bind(server))\n+\n+  server.listen(0, async () => {\n+    await t.rejects(() => fetch(`http://localhost:${server.address().port}`, {\n+      integrity: `sha256-${sha256} sha384-${sha384}`\n+    }))\n+    await t.rejects(() => fetch(`http://localhost:${server.address().port}`, {\n+      integrity: `sha384-${sha384} sha256-${sha256}`\n+    }))\n+  })\n+})\n+\n+test('request with correct integrity checksum (base64). mixed', (t) => {\n+  t.plan(6)\n+\n+  const body = 'Hello world!'\n+  const sha256 = createHash('sha256').update(body).digest('base64')\n+  const sha384 = createHash('sha384').update(body).digest('base64')\n+  const sha512 = createHash('sha512').update(body).digest('base64')\n+\n+  const server = createServer((req, res) => {\n+    res.end(body)\n+  })\n+\n+  t.teardown(server.close.bind(server))\n+\n+  server.listen(0, async () => {\n+    let response\n+    response = await fetch(`http://localhost:${server.address().port}`, {\n+      integrity: `sha256-${sha256} sha512-${sha512}`\n+    })\n+    t.equal(body, await response.text())\n+    response = await fetch(`http://localhost:${server.address().port}`, {\n+      integrity: `sha512-${sha512} sha256-${sha256}`\n+    })\n+\n+    t.equal(body, await response.text())\n+    response = await fetch(`http://localhost:${server.address().port}`, {\n+      integrity: `sha384-${sha384} sha512-${sha512}`\n+    })\n+    t.equal(body, await response.text())\n+    response = await fetch(`http://localhost:${server.address().port}`, {\n+      integrity: `sha384-${sha384} sha512-${sha512}`\n+    })\n+    t.equal(body, await response.text())\n+\n+    response = await fetch(`http://localhost:${server.address().port}`, {\n+      integrity: `sha256-${sha256} sha384-${sha384}`\n+    })\n+    t.equal(body, await response.text())\n+    response = await fetch(`http://localhost:${server.address().port}`, {\n+      integrity: `sha384-${sha384} sha256-${sha256}`\n+    })\n+    t.equal(body, await response.text())\n+  })\n+})\n+\n+test('request with correct integrity checksum (base64url). mixed', (t) => {\n+  t.plan(6)\n+\n+  const body = 'Hello world!'\n+  const sha256 = createHash('sha256').update(body).digest('base64url')\n+  const sha384 = createHash('sha384').update(body).digest('base64url')\n+  const sha512 = createHash('sha512').update(body).digest('base64url')\n+\n+  const server = createServer((req, res) => {\n+    res.end(body)\n+  })\n+\n+  t.teardown(server.close.bind(server))\n+\n+  server.listen(0, async () => {\n+    let response\n+    response = await fetch(`http://localhost:${server.address().port}`, {\n+      integrity: `sha256-${sha256} sha512-${sha512}`\n+    })\n+    t.equal(body, await response.text())\n+    response = await fetch(`http://localhost:${server.address().port}`, {\n+      integrity: `sha512-${sha512} sha256-${sha256}`\n+    })\n+\n+    t.equal(body, await response.text())\n+    response = await fetch(`http://localhost:${server.address().port}`, {\n+      integrity: `sha384-${sha384} sha512-${sha512}`\n+    })\n+    t.equal(body, await response.text())\n+    response = await fetch(`http://localhost:${server.address().port}`, {\n+      integrity: `sha384-${sha384} sha512-${sha512}`\n+    })\n+    t.equal(body, await response.text())\n+\n+    response = await fetch(`http://localhost:${server.address().port}`, {\n+      integrity: `sha256-${sha256} sha384-${sha384}`\n+    })\n+    t.equal(body, await response.text())\n+    response = await fetch(`http://localhost:${server.address().port}`, {\n+      integrity: `sha384-${sha384} sha256-${sha256}`\n+    })\n+    t.equal(body, await response.text())\n+  })\n+})\ndiff --git a/test/fetch/util.js b/test/fetch/util.js\nindex 02b75bc7783..f728d8673ac 100644\n--- a/test/fetch/util.js\n+++ b/test/fetch/util.js\n@@ -5,6 +5,7 @@ const { test } = t\n \n const util = require('../../lib/fetch/util')\n const { HeadersList } = require('../../lib/fetch/headers')\n+const { createHash } = require('crypto')\n \n test('responseURL', (t) => {\n   t.plan(2)\n@@ -279,3 +280,75 @@ test('setRequestReferrerPolicyOnRedirect', nested => {\n     t.equal(request.referrerPolicy, initial)\n   })\n })\n+\n+test('parseMetadata', (t) => {\n+  t.test('should parse valid metadata with option', (t) => {\n+    const body = 'Hello world!'\n+    const hash256 = createHash('sha256').update(body).digest('base64')\n+    const hash384 = createHash('sha384').update(body).digest('base64')\n+    const hash512 = createHash('sha512').update(body).digest('base64')\n+\n+    const validMetadata = `sha256-${hash256} !@ sha384-${hash384} !@ sha512-${hash512} !@`\n+    const result = util.parseMetadata(validMetadata)\n+\n+    t.same(result, [\n+      { algo: 'sha256', hash: hash256.replace(/=/g, '') },\n+      { algo: 'sha384', hash: hash384.replace(/=/g, '') },\n+      { algo: 'sha512', hash: hash512.replace(/=/g, '') }\n+    ])\n+    t.end()\n+  })\n+\n+  t.test('should parse valid metadata with non ASCII chars option', (t) => {\n+    const body = 'Hello world!'\n+    const hash256 = createHash('sha256').update(body).digest('base64')\n+    const hash384 = createHash('sha384').update(body).digest('base64')\n+    const hash512 = createHash('sha512').update(body).digest('base64')\n+\n+    const validMetadata = `sha256-${hash256} !\u00a9 sha384-${hash384} !\u20ac sha512-${hash512} !\u00b5`\n+    const result = util.parseMetadata(validMetadata)\n+\n+    t.same(result, [\n+      { algo: 'sha256', hash: hash256.replace(/=/g, '') },\n+      { algo: 'sha384', hash: hash384.replace(/=/g, '') },\n+      { algo: 'sha512', hash: hash512.replace(/=/g, '') }\n+    ])\n+    t.end()\n+  })\n+\n+  t.test('should parse valid metadata without option', (t) => {\n+    const body = 'Hello world!'\n+    const hash256 = createHash('sha256').update(body).digest('base64')\n+    const hash384 = createHash('sha384').update(body).digest('base64')\n+    const hash512 = createHash('sha512').update(body).digest('base64')\n+\n+    const validMetadata = `sha256-${hash256} sha384-${hash384} sha512-${hash512}`\n+    const result = util.parseMetadata(validMetadata)\n+\n+    t.same(result, [\n+      { algo: 'sha256', hash: hash256.replace(/=/g, '') },\n+      { algo: 'sha384', hash: hash384.replace(/=/g, '') },\n+      { algo: 'sha512', hash: hash512.replace(/=/g, '') }\n+    ])\n+    t.end()\n+  })\n+\n+  t.test('should set hash as undefined when invalid base64 chars are provided', (t) => {\n+    const body = 'Hello world!'\n+    const hash256 = createHash('sha256').update(body).digest('base64')\n+    const invalidHash384 = 'zifp5hE1Xl5LQQqQz[]Bq/iaq9Wb6jVb//T7EfTmbXD2aEP5c2ZdJr9YTDfcTE1ZH+'\n+    const hash512 = createHash('sha512').update(body).digest('base64')\n+\n+    const validMetadata = `sha256-${hash256} sha384-${invalidHash384} sha512-${hash512}`\n+    const result = util.parseMetadata(validMetadata)\n+\n+    t.same(result, [\n+      { algo: 'sha256', hash: hash256.replace(/=/g, '') },\n+      { algo: 'sha384', hash: undefined },\n+      { algo: 'sha512', hash: hash512.replace(/=/g, '') }\n+    ])\n+    t.end()\n+  })\n+\n+  t.end()\n+})\n"
  },
  "CWE-287": {
    "cve": "CVE-2023-28609",
    "commit_url": "https://github.com/ansible-semaphore/semaphore/commit/3e4a62b7f2b1ef0660c9fb839818a53c80a5a8b1",
    "diff": "diff --git a/api/auth.go b/api/auth.go\nindex a69e58186..4a7082190 100644\n--- a/api/auth.go\n+++ b/api/auth.go\n@@ -11,7 +11,7 @@ import (\n \t\"time\"\n )\n \n-func authenticationHandler(w http.ResponseWriter, r *http.Request) {\n+func authenticationHandler(w http.ResponseWriter, r *http.Request) bool {\n \tvar userID int\n \n \tauthHeader := strings.ToLower(r.Header.Get(\"authorization\"))\n@@ -25,7 +25,7 @@ func authenticationHandler(w http.ResponseWriter, r *http.Request) {\n \t\t\t}\n \n \t\t\tw.WriteHeader(http.StatusUnauthorized)\n-\t\t\treturn\n+\t\t\treturn false\n \t\t}\n \n \t\tuserID = token.UserID\n@@ -34,20 +34,20 @@ func authenticationHandler(w http.ResponseWriter, r *http.Request) {\n \t\tcookie, err := r.Cookie(\"semaphore\")\n \t\tif err != nil {\n \t\t\tw.WriteHeader(http.StatusUnauthorized)\n-\t\t\treturn\n+\t\t\treturn false\n \t\t}\n \n \t\tvalue := make(map[string]interface{})\n \t\tif err = util.Cookie.Decode(\"semaphore\", cookie.Value, &value); err != nil {\n \t\t\tw.WriteHeader(http.StatusUnauthorized)\n-\t\t\treturn\n+\t\t\treturn false\n \t\t}\n \n \t\tuser, ok := value[\"user\"]\n \t\tsessionVal, okSession := value[\"session\"]\n \t\tif !ok || !okSession {\n \t\t\tw.WriteHeader(http.StatusUnauthorized)\n-\t\t\treturn\n+\t\t\treturn false\n \t\t}\n \n \t\tuserID = user.(int)\n@@ -58,7 +58,7 @@ func authenticationHandler(w http.ResponseWriter, r *http.Request) {\n \n \t\tif err != nil {\n \t\t\tw.WriteHeader(http.StatusUnauthorized)\n-\t\t\treturn\n+\t\t\treturn false\n \t\t}\n \n \t\tif time.Since(session.LastActive).Hours() > 7*24 {\n@@ -70,13 +70,13 @@ func authenticationHandler(w http.ResponseWriter, r *http.Request) {\n \t\t\t}\n \n \t\t\tw.WriteHeader(http.StatusUnauthorized)\n-\t\t\treturn\n+\t\t\treturn false\n \t\t}\n \n \t\tif err := helpers.Store(r).TouchSession(userID, sessionID); err != nil {\n \t\t\tlog.Error(err)\n \t\t\tw.WriteHeader(http.StatusUnauthorized)\n-\t\t\treturn\n+\t\t\treturn false\n \t\t}\n \t}\n \n@@ -87,7 +87,7 @@ func authenticationHandler(w http.ResponseWriter, r *http.Request) {\n \t\t\tlog.Error(err)\n \t\t}\n \t\tw.WriteHeader(http.StatusUnauthorized)\n-\t\treturn\n+\t\treturn false\n \t}\n \n \tif util.Config.DemoMode {\n@@ -95,18 +95,21 @@ func authenticationHandler(w http.ResponseWriter, r *http.Request) {\n \t\t\t!strings.HasSuffix(r.URL.Path, \"/tasks\") &&\n \t\t\t!strings.HasSuffix(r.URL.Path, \"/stop\") {\n \t\t\tw.WriteHeader(http.StatusUnauthorized)\n-\t\t\treturn\n+\t\t\treturn false\n \t\t}\n \t}\n \n \tcontext.Set(r, \"user\", &user)\n+\treturn true\n }\n \n // nolint: gocyclo\n func authentication(next http.Handler) http.Handler {\n \treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n-\t\tauthenticationHandler(w, r)\n-\t\tnext.ServeHTTP(w, r)\n+\t\tok := authenticationHandler(w, r)\n+\t\tif ok {\n+\t\t\tnext.ServeHTTP(w, r)\n+\t\t}\n \t})\n }\n \n@@ -115,10 +118,14 @@ func authenticationWithStore(next http.Handler) http.Handler {\n \treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n \t\tstore := helpers.Store(r)\n \n+\t\tvar ok bool\n+\t\t\n \t\tdb.StoreSession(store, r.URL.String(), func() {\n-\t\t\tauthenticationHandler(w, r)\n+\t\t\tok = authenticationHandler(w, r)\n \t\t})\n \n-\t\tnext.ServeHTTP(w, r)\n+\t\tif ok {\n+\t\t\tnext.ServeHTTP(w, r)\n+\t\t}\n \t})\n }\ndiff --git a/api/router.go b/api/router.go\nindex b5a3d8e66..1a51c31db 100644\n--- a/api/router.go\n+++ b/api/router.go\n@@ -348,16 +348,8 @@ func servePublic(w http.ResponseWriter, r *http.Request) {\n }\n \n func getSystemInfo(w http.ResponseWriter, r *http.Request) {\n-\t//updateAvailable, err := util.CheckUpdate()\n-\n-\t//if err != nil {\n-\t//\thelpers.WriteError(w, err)\n-\t//\treturn\n-\t//}\n-\n \tbody := map[string]interface{}{\n \t\t\"version\": util.Version,\n-\t\t//\"update\":  updateAvailable,\n \t\t\"ansible\": util.AnsibleVersion(),\n \t\t\"demo\":    util.Config.DemoMode,\n \t}\n"
  },
  "CWE-200": {
    "cve": "CVE-2024-52513",
    "commit_url": "https://github.com/nextcloud/text/commit/ca24b25c93b81626b4e457c260243edeab5f1548",
    "diff": "diff --git a/lib/Middleware/SessionMiddleware.php b/lib/Middleware/SessionMiddleware.php\nindex 1ef4bc6fc4a..6ed807ce4bc 100644\n--- a/lib/Middleware/SessionMiddleware.php\n+++ b/lib/Middleware/SessionMiddleware.php\n@@ -21,10 +21,12 @@\n use OCP\\AppFramework\\Http\\JSONResponse;\n use OCP\\AppFramework\\Http\\Response;\n use OCP\\AppFramework\\Middleware;\n+use OCP\\Constants;\n use OCP\\Files\\IRootFolder;\n use OCP\\Files\\NotPermittedException;\n use OCP\\IL10N;\n use OCP\\IRequest;\n+use OCP\\ISession;\n use OCP\\IUserSession;\n use OCP\\Share\\Exceptions\\ShareNotFound;\n use OCP\\Share\\IManager as ShareManager;\n@@ -36,6 +38,7 @@ public function __construct(\n \t\tprivate IRequest $request,\n \t\tprivate SessionService $sessionService,\n \t\tprivate DocumentService $documentService,\n+\t\tprivate ISession $session,\n \t\tprivate IUserSession $userSession,\n \t\tprivate IRootFolder $rootFolder,\n \t\tprivate ShareManager $shareManager,\n@@ -131,8 +134,25 @@ private function assertUserOrShareToken(ISessionAwareController $controller): vo\n \t\t\t} catch (ShareNotFound) {\n \t\t\t\tthrow new InvalidSessionException();\n \t\t\t}\n-\t\t\t// Check if shareToken has access to document\n-\t\t\tif ($this->rootFolder->getUserFolder($share->getShareOwner())->getFirstNodeById($documentId) === null) {\n+\n+\t\t\t$node = $this->rootFolder->getUserFolder($share->getShareOwner())->getFirstNodeById($documentId);\n+\t\t\tif ($node === null) {\n+\t\t\t\tthrow new InvalidSessionException();\n+\t\t\t}\n+\n+\t\t\tif ($share->getPassword() !== null) {\n+\t\t\t\t$shareId = $this->session->get('public_link_authenticated');\n+\t\t\t\tif ($share->getId() !== $shareId) {\n+\t\t\t\t\tthrow new InvalidSessionException();\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\tif (($share->getPermissions() & Constants::PERMISSION_READ) !== Constants::PERMISSION_READ) {\n+\t\t\t\tthrow new InvalidSessionException();\n+\t\t\t}\n+\n+\t\t\t$attributes = $share->getAttributes();\n+\t\t\tif ($attributes !== null && $attributes->getAttribute('permissions', 'download') === false) {\n \t\t\t\tthrow new InvalidSessionException();\n \t\t\t}\n \t\t} else {\n"
  },
  "CWE-94": {
    "cve": "CVE-2024-6345",
    "commit_url": "https://github.com/pypa/setuptools/commit/88807c7062788254f654ea8c03427adc859321f0",
    "diff": "diff --git a/newsfragments/4332.feature.rst b/newsfragments/4332.feature.rst\nnew file mode 100644\nindex 0000000000..9f46298adc\n--- /dev/null\n+++ b/newsfragments/4332.feature.rst\n@@ -0,0 +1 @@\n+Modernized and refactored VCS handling in package_index.\n\\ No newline at end of file\ndiff --git a/setup.cfg b/setup.cfg\nindex 68be6c8e7c..1226c940fc 100644\n--- a/setup.cfg\n+++ b/setup.cfg\n@@ -76,6 +76,7 @@ testing =\n \ttomli\n \t# No Python 3.12 dependencies require importlib_metadata, but needed for type-checking since we import it directly\n \timportlib_metadata\n+\tpytest-subprocess\n \n \t# workaround for pypa/setuptools#4333\n \tpyproject-hooks!=1.1\ndiff --git a/setuptools/package_index.py b/setuptools/package_index.py\nindex f5a7d77eed..c3ffee41a7 100644\n--- a/setuptools/package_index.py\n+++ b/setuptools/package_index.py\n@@ -1,6 +1,7 @@\n \"\"\"PyPI and direct package downloading.\"\"\"\n \n import sys\n+import subprocess\n import os\n import re\n import io\n@@ -587,7 +588,7 @@ def download(self, spec, tmpdir):\n             scheme = URL_SCHEME(spec)\n             if scheme:\n                 # It's a url, download it to tmpdir\n-                found = self._download_url(scheme.group(1), spec, tmpdir)\n+                found = self._download_url(spec, tmpdir)\n                 base, fragment = egg_info_for_url(spec)\n                 if base.endswith('.py'):\n                     found = self.gen_setup(found, fragment, tmpdir)\n@@ -816,7 +817,7 @@ def open_url(self, url, warning=None):  # noqa: C901  # is too complex (12)\n             else:\n                 raise DistutilsError(\"Download error for %s: %s\" % (url, v)) from v\n \n-    def _download_url(self, scheme, url, tmpdir):\n+    def _download_url(self, url, tmpdir):\n         # Determine download filename\n         #\n         name, fragment = egg_info_for_url(url)\n@@ -831,19 +832,59 @@ def _download_url(self, scheme, url, tmpdir):\n \n         filename = os.path.join(tmpdir, name)\n \n-        # Download the file\n-        #\n-        if scheme == 'svn' or scheme.startswith('svn+'):\n-            return self._download_svn(url, filename)\n-        elif scheme == 'git' or scheme.startswith('git+'):\n-            return self._download_git(url, filename)\n-        elif scheme.startswith('hg+'):\n-            return self._download_hg(url, filename)\n-        elif scheme == 'file':\n-            return urllib.request.url2pathname(urllib.parse.urlparse(url)[2])\n-        else:\n-            self.url_ok(url, True)  # raises error if not allowed\n-            return self._attempt_download(url, filename)\n+        return self._download_vcs(url, filename) or self._download_other(url, filename)\n+\n+    @staticmethod\n+    def _resolve_vcs(url):\n+        \"\"\"\n+        >>> rvcs = PackageIndex._resolve_vcs\n+        >>> rvcs('git+http://foo/bar')\n+        'git'\n+        >>> rvcs('hg+https://foo/bar')\n+        'hg'\n+        >>> rvcs('git:myhost')\n+        'git'\n+        >>> rvcs('hg:myhost')\n+        >>> rvcs('http://foo/bar')\n+        \"\"\"\n+        scheme = urllib.parse.urlsplit(url).scheme\n+        pre, sep, post = scheme.partition('+')\n+        # svn and git have their own protocol; hg does not\n+        allowed = set(['svn', 'git'] + ['hg'] * bool(sep))\n+        return next(iter({pre} & allowed), None)\n+\n+    def _download_vcs(self, url, spec_filename):\n+        vcs = self._resolve_vcs(url)\n+        if not vcs:\n+            return\n+        if vcs == 'svn':\n+            raise DistutilsError(\n+                f\"Invalid config, SVN download is not supported: {url}\"\n+            )\n+\n+        filename, _, _ = spec_filename.partition('#')\n+        url, rev = self._vcs_split_rev_from_url(url)\n+\n+        self.info(f\"Doing {vcs} clone from {url} to {filename}\")\n+        subprocess.check_call([vcs, 'clone', '--quiet', url, filename])\n+\n+        co_commands = dict(\n+            git=[vcs, '-C', filename, 'checkout', '--quiet', rev],\n+            hg=[vcs, '--cwd', filename, 'up', '-C', '-r', rev, '-q'],\n+        )\n+        if rev is not None:\n+            self.info(f\"Checking out {rev}\")\n+            subprocess.check_call(co_commands[vcs])\n+\n+        return filename\n+\n+    def _download_other(self, url, filename):\n+        scheme = urllib.parse.urlsplit(url).scheme\n+        if scheme == 'file':  # pragma: no cover\n+            return urllib.request.url2pathname(urllib.parse.urlparse(url).path)\n+        # raise error if not allowed\n+        self.url_ok(url, True)\n+        return self._attempt_download(url, filename)\n \n     def scan_url(self, url):\n         self.process_url(url, True)\n@@ -859,64 +900,37 @@ def _invalid_download_html(self, url, headers, filename):\n         os.unlink(filename)\n         raise DistutilsError(f\"Unexpected HTML page found at {url}\")\n \n-    def _download_svn(self, url, _filename):\n-        raise DistutilsError(f\"Invalid config, SVN download is not supported: {url}\")\n-\n     @staticmethod\n-    def _vcs_split_rev_from_url(url, pop_prefix=False):\n-        scheme, netloc, path, query, frag = urllib.parse.urlsplit(url)\n+    def _vcs_split_rev_from_url(url):\n+        \"\"\"\n+        Given a possible VCS URL, return a clean URL and resolved revision if any.\n+\n+        >>> vsrfu = PackageIndex._vcs_split_rev_from_url\n+        >>> vsrfu('git+https://github.com/pypa/setuptools@v69.0.0#egg-info=setuptools')\n+        ('https://github.com/pypa/setuptools', 'v69.0.0')\n+        >>> vsrfu('git+https://github.com/pypa/setuptools#egg-info=setuptools')\n+        ('https://github.com/pypa/setuptools', None)\n+        >>> vsrfu('http://foo/bar')\n+        ('http://foo/bar', None)\n+        \"\"\"\n+        parts = urllib.parse.urlsplit(url)\n \n-        scheme = scheme.split('+', 1)[-1]\n+        clean_scheme = parts.scheme.split('+', 1)[-1]\n \n         # Some fragment identification fails\n-        path = path.split('#', 1)[0]\n-\n-        rev = None\n-        if '@' in path:\n-            path, rev = path.rsplit('@', 1)\n-\n-        # Also, discard fragment\n-        url = urllib.parse.urlunsplit((scheme, netloc, path, query, ''))\n-\n-        return url, rev\n-\n-    def _download_git(self, url, filename):\n-        filename = filename.split('#', 1)[0]\n-        url, rev = self._vcs_split_rev_from_url(url, pop_prefix=True)\n-\n-        self.info(\"Doing git clone from %s to %s\", url, filename)\n-        os.system(\"git clone --quiet %s %s\" % (url, filename))\n-\n-        if rev is not None:\n-            self.info(\"Checking out %s\", rev)\n-            os.system(\n-                \"git -C %s checkout --quiet %s\"\n-                % (\n-                    filename,\n-                    rev,\n-                )\n-            )\n+        no_fragment_path, _, _ = parts.path.partition('#')\n \n-        return filename\n+        pre, sep, post = no_fragment_path.rpartition('@')\n+        clean_path, rev = (pre, post) if sep else (post, None)\n \n-    def _download_hg(self, url, filename):\n-        filename = filename.split('#', 1)[0]\n-        url, rev = self._vcs_split_rev_from_url(url, pop_prefix=True)\n+        resolved = parts._replace(\n+            scheme=clean_scheme,\n+            path=clean_path,\n+            # discard the fragment\n+            fragment='',\n+        ).geturl()\n \n-        self.info(\"Doing hg clone from %s to %s\", url, filename)\n-        os.system(\"hg clone --quiet %s %s\" % (url, filename))\n-\n-        if rev is not None:\n-            self.info(\"Updating to %s\", rev)\n-            os.system(\n-                \"hg --cwd %s up -C -r %s -q\"\n-                % (\n-                    filename,\n-                    rev,\n-                )\n-            )\n-\n-        return filename\n+        return resolved, rev\n \n     def debug(self, msg, *args):\n         log.debug(msg, *args)\ndiff --git a/setuptools/tests/test_packageindex.py b/setuptools/tests/test_packageindex.py\nindex 93474ae5af..f5f37e0563 100644\n--- a/setuptools/tests/test_packageindex.py\n+++ b/setuptools/tests/test_packageindex.py\n@@ -3,7 +3,6 @@\n import urllib.error\n import http.client\n from inspect import cleandoc\n-from unittest import mock\n \n import pytest\n \n@@ -171,49 +170,46 @@ def test_egg_fragment(self):\n             assert dists[0].version == ''\n             assert dists[1].version == vc\n \n-    def test_download_git_with_rev(self, tmpdir):\n+    def test_download_git_with_rev(self, tmp_path, fp):\n         url = 'git+https://github.example/group/project@master#egg=foo'\n         index = setuptools.package_index.PackageIndex()\n \n-        with mock.patch(\"os.system\") as os_system_mock:\n-            result = index.download(url, str(tmpdir))\n+        expected_dir = tmp_path / 'project@master'\n+        fp.register([\n+            'git',\n+            'clone',\n+            '--quiet',\n+            'https://github.example/group/project',\n+            expected_dir,\n+        ])\n+        fp.register(['git', '-C', expected_dir, 'checkout', '--quiet', 'master'])\n \n-        os_system_mock.assert_called()\n+        result = index.download(url, tmp_path)\n \n-        expected_dir = str(tmpdir / 'project@master')\n-        expected = (\n-            'git clone --quiet ' 'https://github.example/group/project {expected_dir}'\n-        ).format(**locals())\n-        first_call_args = os_system_mock.call_args_list[0][0]\n-        assert first_call_args == (expected,)\n+        assert result == str(expected_dir)\n+        assert len(fp.calls) == 2\n \n-        tmpl = 'git -C {expected_dir} checkout --quiet master'\n-        expected = tmpl.format(**locals())\n-        assert os_system_mock.call_args_list[1][0] == (expected,)\n-        assert result == expected_dir\n-\n-    def test_download_git_no_rev(self, tmpdir):\n+    def test_download_git_no_rev(self, tmp_path, fp):\n         url = 'git+https://github.example/group/project#egg=foo'\n         index = setuptools.package_index.PackageIndex()\n \n-        with mock.patch(\"os.system\") as os_system_mock:\n-            result = index.download(url, str(tmpdir))\n-\n-        os_system_mock.assert_called()\n-\n-        expected_dir = str(tmpdir / 'project')\n-        expected = (\n-            'git clone --quiet ' 'https://github.example/group/project {expected_dir}'\n-        ).format(**locals())\n-        os_system_mock.assert_called_once_with(expected)\n-\n-    def test_download_svn(self, tmpdir):\n+        expected_dir = tmp_path / 'project'\n+        fp.register([\n+            'git',\n+            'clone',\n+            '--quiet',\n+            'https://github.example/group/project',\n+            expected_dir,\n+        ])\n+        index.download(url, tmp_path)\n+\n+    def test_download_svn(self, tmp_path):\n         url = 'svn+https://svn.example/project#egg=foo'\n         index = setuptools.package_index.PackageIndex()\n \n         msg = r\".*SVN download is not supported.*\"\n         with pytest.raises(distutils.errors.DistutilsError, match=msg):\n-            index.download(url, str(tmpdir))\n+            index.download(url, tmp_path)\n \n \n class TestContentCheckers:\n"
  },
  "CWE-400": {
    "cve": "CVE-2024-23835",
    "commit_url": "https://github.com/OISF/suricata/commit/86de7cffa7e8f06fe9d600127e7dabe89c7e81dd",
    "diff": "diff --git a/rust/src/pgsql/parser.rs b/rust/src/pgsql/parser.rs\nindex 4dbb2915c236..886ee4c5dca8 100644\n--- a/rust/src/pgsql/parser.rs\n+++ b/rust/src/pgsql/parser.rs\n@@ -23,7 +23,7 @@ use crate::common::nom7::take_until_and_consume;\n use nom7::branch::alt;\n use nom7::bytes::streaming::{tag, take, take_until, take_until1};\n use nom7::character::streaming::{alphanumeric1, char};\n-use nom7::combinator::{all_consuming, cond, eof, map_parser, opt, peek, rest, verify};\n+use nom7::combinator::{all_consuming, cond, eof, map_parser, opt, peek, verify};\n use nom7::error::{make_error, ErrorKind};\n use nom7::multi::{many1, many_m_n, many_till};\n use nom7::number::streaming::{be_i16, be_i32};\n@@ -1078,10 +1078,12 @@ pub fn pgsql_parse_response(i: &[u8]) -> IResult<&[u8], PgsqlBEMessage> {\n                 b'A' => parse_notification_response(i)?,\n                 b'D' => parse_consolidated_data_row(i)?,\n                 _ => {\n-                    let (i, payload) = rest(i)?;\n+                    let (i, identifier) = be_u8(i)?;\n+                    let (i, length) = verify(be_u32, |&x| x > PGSQL_LENGTH_FIELD)(i)?;\n+                    let (i, payload) = take(length - PGSQL_LENGTH_FIELD)(i)?;\n                     let unknown = PgsqlBEMessage::UnknownMessageType (RegularPacket{\n-                        identifier: pseudo_header.0,\n-                        length: pseudo_header.1,\n+                        identifier,\n+                        length,\n                         payload: payload.to_vec(),\n                     });\n                     (i, unknown)\n@@ -1918,7 +1920,7 @@ mod tests {\n         let res = PgsqlBEMessage::UnknownMessageType(RegularPacket {\n             identifier: b'`',\n             length: 54,\n-            payload: bad_buf.to_vec(),\n+            payload: bad_buf[5..].to_vec(),\n         });\n         assert_eq!(result, res);\n         assert!(remainder.is_empty());\n"
  },
  "CWE-120": {
    "cve": "CVE-2024-40130",
    "commit_url": "https://github.com/open5gs/open5gs/commit/2f8ae91b0b9467f94f128090c88cae91bd73e008",
    "diff": "diff --git a/src/main.c b/src/main.c\nindex 329d5b1087..0f993a6a67 100644\n--- a/src/main.c\n+++ b/src/main.c\n@@ -111,7 +111,7 @@ int main(int argc, const char *const argv[])\n         bool enable_debug;\n         bool enable_trace;\n     } optarg;\n-    const char *argv_out[argc];\n+    const char *argv_out[argc+1];\n \n     memset(&optarg, 0, sizeof(optarg));\n \ndiff --git a/tests/app/5gc-init.c b/tests/app/5gc-init.c\nindex 9589115d12..dfdcac093a 100644\n--- a/tests/app/5gc-init.c\n+++ b/tests/app/5gc-init.c\n@@ -37,7 +37,7 @@ int app_initialize(const char *const argv[])\n     bool user_config = false;\n     int i = 0;\n \n-    for (i = 0; argv[i]; i++) {\n+    for (i = 0; argv[i] && i < OGS_ARG_MAX-3; i++) {\n         if (strcmp(\"-c\", argv[i]) == 0) {\n             user_config = true; \n         }\ndiff --git a/tests/app/app-init.c b/tests/app/app-init.c\nindex 040448d53f..d53601ec4b 100644\n--- a/tests/app/app-init.c\n+++ b/tests/app/app-init.c\n@@ -42,7 +42,7 @@ int app_initialize(const char *const argv[])\n     bool user_config = false;\n     int i = 0;\n \n-    for (i = 0; argv[i]; i++) {\n+    for (i = 0; argv[i] && i < OGS_ARG_MAX-3; i++) {\n         if (strcmp(\"-c\", argv[i]) == 0) {\n             user_config = true;\n         }\ndiff --git a/tests/app/epc-init.c b/tests/app/epc-init.c\nindex c84f0ffe2a..7d27ad5941 100644\n--- a/tests/app/epc-init.c\n+++ b/tests/app/epc-init.c\n@@ -33,7 +33,7 @@ int app_initialize(const char *const argv[])\n     bool user_config = false;\n     int i = 0;\n \n-    for (i = 0; argv[i]; i++) {\n+    for (i = 0; argv[i] && i < OGS_ARG_MAX-3; i++) {\n         if (strcmp(\"-c\", argv[i]) == 0) {\n             user_config = true; \n         }\ndiff --git a/tests/common/application.c b/tests/common/application.c\nindex 1bf1a0528b..f934fe5145 100644\n--- a/tests/common/application.c\n+++ b/tests/common/application.c\n@@ -27,7 +27,7 @@ static void run(int argc, const char *const argv[],\n     bool user_config;\n \n     /* '-f sample-XXXX.conf -e error' is always added */\n-    const char *argv_out[argc+4], *new_argv[argc+4];\n+    const char *argv_out[argc+5], *new_argv[argc+5];\n     int argc_out;\n \n     char conf_file[OGS_MAX_FILEPATH_LEN];\ndiff --git a/tests/common/context.c b/tests/common/context.c\nindex c1cf82cbf3..1f1bfa5c5e 100644\n--- a/tests/common/context.c\n+++ b/tests/common/context.c\n@@ -127,7 +127,8 @@ static int test_context_validation(void)\n \n     if (test_self()->nr_served_tai[index].list2.num) {\n         memcpy(&test_self()->nr_tai,\n-            &test_self()->nr_served_tai[index].list2.tai[0], sizeof(ogs_5gs_tai_t));\n+            &test_self()->nr_served_tai[index].list2.tai[0],\n+            sizeof(ogs_5gs_tai_t));\n     } else if (test_self()->nr_served_tai[index].list1.tai[0].num) {\n         test_self()->nr_tai.tac =\n             test_self()->nr_served_tai[index].list1.tai[0].tac;\ndiff --git a/tests/common/context.h b/tests/common/context.h\nindex 422a6b2ab1..c54eff9fee 100644\n--- a/tests/common/context.h\n+++ b/tests/common/context.h\n@@ -477,6 +477,13 @@ typedef struct test_bearer_s {\n \n     uint32_t        sgw_s1u_teid;   /* SGW-S1U TEID */\n     ogs_ip_t        sgw_s1u_ip;     /* SGW-S1U IPv4/IPv6 */\n+    struct {\n+        /* Indirect Forwarding */\n+        uint32_t dl_teid;\n+        ogs_ip_t dl_ip;\n+        uint32_t ul_teid;\n+        ogs_ip_t ul_ip;\n+    } handover;\n \n     uint32_t        enb_s1u_teid;   /* eNB-S1U TEID */\n     ogs_sockaddr_t  *enb_s1u_addr;  /* eNB-S1U IPv4 */\ndiff --git a/tests/common/gtpu.c b/tests/common/gtpu.c\nindex a5c7ce2d36..091e216db3 100644\n--- a/tests/common/gtpu.c\n+++ b/tests/common/gtpu.c\n@@ -535,9 +535,7 @@ int test_gtpu_send_indirect_data_forwarding(\n         ext_hdesc.qos_flow_identifier = bearer->qfi;\n \n     } else if (bearer->ebi) {\n-\n-        ogs_fatal(\"Not implmented EPC Indirect Tunnel\");\n-        ogs_assert_if_reached();\n+        gtp_hdesc.teid = bearer->handover.ul_teid;\n \n     } else {\n         ogs_fatal(\"No QFI[%d] and EBI[%d]\", bearer->qfi, bearer->ebi);\ndiff --git a/tests/common/ngap-build.c b/tests/common/ngap-build.c\nindex 1f2a654364..84432701f1 100644\n--- a/tests/common/ngap-build.c\n+++ b/tests/common/ngap-build.c\n@@ -37,7 +37,7 @@ static ogs_pkbuf_t *testngap_build_handover_request_ack_transfer(\n ogs_pkbuf_t *testngap_build_ng_setup_request(uint32_t gnb_id, uint8_t bitsize)\n {\n     ogs_pkbuf_t *pkbuf = NULL;\n-    int i, j;\n+    int i, j, k, num = 0;\n     ogs_plmn_id_t *plmn_id = NULL;\n     const char *ran_node_name = \"5G gNB-CU\";\n \n@@ -118,48 +118,58 @@ ogs_pkbuf_t *testngap_build_ng_setup_request(uint32_t gnb_id, uint8_t bitsize)\n     ogs_asn_buffer_to_OCTET_STRING((char*)ran_node_name,\n             strlen(ran_node_name), RANNodeName);\n \n-    SupportedTAItem = CALLOC(1, sizeof(NGAP_SupportedTAItem_t));\n     if (test_self()->nr_served_tai[0].list2.num)\n-        ogs_asn_uint24_to_OCTET_STRING(\n-            test_self()->nr_served_tai[0].list2.tai[0].tac,\n-            &SupportedTAItem->tAC);\n+        num = test_self()->nr_served_tai[0].list2.num;\n     else if (test_self()->nr_served_tai[0].list0.tai[0].num)\n-        ogs_asn_uint24_to_OCTET_STRING(\n-            test_self()->nr_served_tai[0].list0.tai[0].tac[0],\n-                &SupportedTAItem->tAC);\n+        num = test_self()->nr_served_tai[0].list0.tai[0].num;\n     else\n         ogs_assert_if_reached();\n \n-    for (i = 0; i < test_self()->num_of_plmn_support; i++) {\n-        plmn_id = &test_self()->plmn_support[i].plmn_id;\n+    for (i = 0; i < num; i++) {\n+        SupportedTAItem = CALLOC(1, sizeof(NGAP_SupportedTAItem_t));\n+        if (test_self()->nr_served_tai[0].list2.num)\n+            ogs_asn_uint24_to_OCTET_STRING(\n+                test_self()->nr_served_tai[0].list2.tai[i].tac,\n+                &SupportedTAItem->tAC);\n+        else if (test_self()->nr_served_tai[0].list0.tai[0].num)\n+            ogs_asn_uint24_to_OCTET_STRING(\n+                test_self()->nr_served_tai[0].list0.tai[0].tac[i],\n+                    &SupportedTAItem->tAC);\n+        else\n+            ogs_assert_if_reached();\n \n-        BroadcastPLMNItem = CALLOC(1, sizeof(NGAP_BroadcastPLMNItem_t));\n+        for (j = 0; j < test_self()->num_of_plmn_support; j++) {\n+            plmn_id = &test_self()->plmn_support[j].plmn_id;\n \n-        ogs_asn_buffer_to_OCTET_STRING(\n-                plmn_id, OGS_PLMN_ID_LEN, &BroadcastPLMNItem->pLMNIdentity);\n+            BroadcastPLMNItem = CALLOC(1, sizeof(NGAP_BroadcastPLMNItem_t));\n \n-        for (j = 0; j < test_self()->plmn_support[i].num_of_s_nssai; j++) {\n-            ogs_s_nssai_t *s_nssai = &test_self()->plmn_support[i].s_nssai[j];\n+            ogs_asn_buffer_to_OCTET_STRING(\n+                    plmn_id, OGS_PLMN_ID_LEN, &BroadcastPLMNItem->pLMNIdentity);\n \n-            SliceSupportItem = CALLOC(1, sizeof(NGAP_SliceSupportItem_t));\n-            ogs_asn_uint8_to_OCTET_STRING(s_nssai->sst,\n-                    &SliceSupportItem->s_NSSAI.sST);\n-            if (s_nssai->sd.v != OGS_S_NSSAI_NO_SD_VALUE) {\n-                SliceSupportItem->s_NSSAI.sD = CALLOC(1, sizeof(NGAP_SD_t));\n-                ogs_asn_uint24_to_OCTET_STRING(\n-                        s_nssai->sd, SliceSupportItem->s_NSSAI.sD);\n+            for (k = 0; k < test_self()->plmn_support[j].num_of_s_nssai; k++) {\n+                ogs_s_nssai_t *s_nssai =\n+                    &test_self()->plmn_support[j].s_nssai[k];\n+\n+                SliceSupportItem = CALLOC(1, sizeof(NGAP_SliceSupportItem_t));\n+                ogs_asn_uint8_to_OCTET_STRING(s_nssai->sst,\n+                        &SliceSupportItem->s_NSSAI.sST);\n+                if (s_nssai->sd.v != OGS_S_NSSAI_NO_SD_VALUE) {\n+                    SliceSupportItem->s_NSSAI.sD = CALLOC(1, sizeof(NGAP_SD_t));\n+                    ogs_asn_uint24_to_OCTET_STRING(\n+                            s_nssai->sd, SliceSupportItem->s_NSSAI.sD);\n+                }\n+\n+                ASN_SEQUENCE_ADD(&BroadcastPLMNItem->tAISliceSupportList.list,\n+                                SliceSupportItem);\n             }\n \n-            ASN_SEQUENCE_ADD(&BroadcastPLMNItem->tAISliceSupportList.list,\n-                            SliceSupportItem);\n+            ASN_SEQUENCE_ADD(&SupportedTAItem->broadcastPLMNList.list,\n+                    BroadcastPLMNItem);\n         }\n \n-        ASN_SEQUENCE_ADD(&SupportedTAItem->broadcastPLMNList.list,\n-                BroadcastPLMNItem);\n+        ASN_SEQUENCE_ADD(&SupportedTAList->list, SupportedTAItem);\n     }\n \n-    ASN_SEQUENCE_ADD(&SupportedTAList->list, SupportedTAItem);\n-\n     *PagingDRX = NGAP_PagingDRX_v32;\n \n     return ogs_ngap_encode(&pdu);\n@@ -168,7 +178,7 @@ ogs_pkbuf_t *testngap_build_ng_setup_request(uint32_t gnb_id, uint8_t bitsize)\n ogs_pkbuf_t *testngap_build_ran_configuration_update(bool supported_ta_list)\n {\n     ogs_pkbuf_t *pkbuf = NULL;\n-    int i, j;\n+    int i, j, k, num;\n     ogs_plmn_id_t *plmn_id = NULL;\n \n     NGAP_NGAP_PDU_t pdu;\n@@ -210,48 +220,62 @@ ogs_pkbuf_t *testngap_build_ran_configuration_update(bool supported_ta_list)\n \n         SupportedTAList = &ie->value.choice.SupportedTAList;\n \n-        SupportedTAItem = CALLOC(1, sizeof(NGAP_SupportedTAItem_t));\n         if (test_self()->nr_served_tai[0].list2.num)\n-            ogs_asn_uint24_to_OCTET_STRING(\n-                test_self()->nr_served_tai[0].list2.tai[0].tac,\n-                &SupportedTAItem->tAC);\n+            num = test_self()->nr_served_tai[0].list2.num;\n         else if (test_self()->nr_served_tai[0].list0.tai[0].num)\n-            ogs_asn_uint24_to_OCTET_STRING(\n-                test_self()->nr_served_tai[0].list0.tai[0].tac[0],\n-                    &SupportedTAItem->tAC);\n+            num = test_self()->nr_served_tai[0].list0.tai[0].num;\n         else\n             ogs_assert_if_reached();\n \n-        for (i = 0; i < test_self()->num_of_plmn_support; i++) {\n-            plmn_id = &test_self()->plmn_support[i].plmn_id;\n-\n-            BroadcastPLMNItem = CALLOC(1, sizeof(NGAP_BroadcastPLMNItem_t));\n-\n-            ogs_asn_buffer_to_OCTET_STRING(\n-                    plmn_id, OGS_PLMN_ID_LEN, &BroadcastPLMNItem->pLMNIdentity);\n-\n-            for (j = 0; j < test_self()->plmn_support[i].num_of_s_nssai; j++) {\n-                ogs_s_nssai_t *s_nssai =\n-                    &test_self()->plmn_support[i].s_nssai[j];\n-\n-                SliceSupportItem = CALLOC(1, sizeof(NGAP_SliceSupportItem_t));\n-                ogs_asn_uint8_to_OCTET_STRING(s_nssai->sst,\n-                        &SliceSupportItem->s_NSSAI.sST);\n-                if (s_nssai->sd.v != OGS_S_NSSAI_NO_SD_VALUE) {\n-                    SliceSupportItem->s_NSSAI.sD = CALLOC(1, sizeof(NGAP_SD_t));\n-                    ogs_asn_uint24_to_OCTET_STRING(\n-                            s_nssai->sd, SliceSupportItem->s_NSSAI.sD);\n+        for (i = 0; i < num; i++) {\n+            SupportedTAItem = CALLOC(1, sizeof(NGAP_SupportedTAItem_t));\n+            if (test_self()->nr_served_tai[0].list2.num)\n+                ogs_asn_uint24_to_OCTET_STRING(\n+                    test_self()->nr_served_tai[0].list2.tai[i].tac,\n+                    &SupportedTAItem->tAC);\n+            else if (test_self()->nr_served_tai[0].list0.tai[0].num)\n+                ogs_asn_uint24_to_OCTET_STRING(\n+                    test_self()->nr_served_tai[0].list0.tai[0].tac[i],\n+                        &SupportedTAItem->tAC);\n+            else\n+                ogs_assert_if_reached();\n+\n+            for (j = 0; j < test_self()->num_of_plmn_support; j++) {\n+                plmn_id = &test_self()->plmn_support[j].plmn_id;\n+\n+                BroadcastPLMNItem = CALLOC(1, sizeof(NGAP_BroadcastPLMNItem_t));\n+\n+                ogs_asn_buffer_to_OCTET_STRING(\n+                        plmn_id, OGS_PLMN_ID_LEN,\n+                        &BroadcastPLMNItem->pLMNIdentity);\n+\n+                for (k = 0; k < test_self()->plmn_support[j].num_of_s_nssai;\n+                        k++) {\n+                    ogs_s_nssai_t *s_nssai =\n+                        &test_self()->plmn_support[j].s_nssai[k];\n+\n+                    SliceSupportItem = CALLOC(1,\n+                            sizeof(NGAP_SliceSupportItem_t));\n+                    ogs_asn_uint8_to_OCTET_STRING(s_nssai->sst,\n+                            &SliceSupportItem->s_NSSAI.sST);\n+                    if (s_nssai->sd.v != OGS_S_NSSAI_NO_SD_VALUE) {\n+                        SliceSupportItem->s_NSSAI.sD = CALLOC(\n+                                1, sizeof(NGAP_SD_t));\n+                        ogs_asn_uint24_to_OCTET_STRING(\n+                                s_nssai->sd, SliceSupportItem->s_NSSAI.sD);\n+                    }\n+\n+                    ASN_SEQUENCE_ADD(\n+                            &BroadcastPLMNItem->tAISliceSupportList.list,\n+                            SliceSupportItem);\n                 }\n \n-                ASN_SEQUENCE_ADD(&BroadcastPLMNItem->tAISliceSupportList.list,\n-                                SliceSupportItem);\n+                ASN_SEQUENCE_ADD(&SupportedTAItem->broadcastPLMNList.list,\n+                        BroadcastPLMNItem);\n             }\n \n-            ASN_SEQUENCE_ADD(&SupportedTAItem->broadcastPLMNList.list,\n-                    BroadcastPLMNItem);\n+            ASN_SEQUENCE_ADD(&SupportedTAList->list, SupportedTAItem);\n         }\n-\n-        ASN_SEQUENCE_ADD(&SupportedTAList->list, SupportedTAItem);\n     }\n \n     return ogs_ngap_encode(&pdu);\ndiff --git a/tests/common/s1ap-handler.c b/tests/common/s1ap-handler.c\nindex 5281794eb4..91f5e601b2 100644\n--- a/tests/common/s1ap-handler.c\n+++ b/tests/common/s1ap-handler.c\n@@ -479,6 +479,7 @@ void tests1ap_handle_handover_command(\n     char buf[OGS_ADDRSTRLEN];\n \n     test_sess_t *sess = NULL;\n+    test_bearer_t *bearer = NULL;\n \n     S1AP_S1AP_PDU_t pdu;\n     S1AP_SuccessfulOutcome_t *successfulOutcome = NULL;\n@@ -487,6 +488,8 @@ void tests1ap_handle_handover_command(\n     S1AP_HandoverCommandIEs_t *ie = NULL;\n     S1AP_MME_UE_S1AP_ID_t *MME_UE_S1AP_ID = NULL;\n     S1AP_ENB_UE_S1AP_ID_t *ENB_UE_S1AP_ID = NULL;\n+    S1AP_E_RABSubjecttoDataForwardingList_t\n+        *E_RABSubjecttoDataForwardingList = NULL;\n \n     ogs_assert(test_ue);\n     ogs_assert(message);\n@@ -505,6 +508,10 @@ void tests1ap_handle_handover_command(\n         case S1AP_ProtocolIE_ID_id_eNB_UE_S1AP_ID:\n             ENB_UE_S1AP_ID = &ie->value.choice.ENB_UE_S1AP_ID;\n             break;\n+        case S1AP_ProtocolIE_ID_id_E_RABSubjecttoDataForwardingList:\n+            E_RABSubjecttoDataForwardingList =\n+                &ie->value.choice.E_RABSubjecttoDataForwardingList;\n+            break;\n         default:\n             break;\n         }\n@@ -514,6 +521,44 @@ void tests1ap_handle_handover_command(\n         test_ue->mme_ue_s1ap_id = *MME_UE_S1AP_ID;\n     if (ENB_UE_S1AP_ID)\n         test_ue->enb_ue_s1ap_id = *ENB_UE_S1AP_ID;\n+\n+    if (E_RABSubjecttoDataForwardingList) {\n+        for (i = 0; i < E_RABSubjecttoDataForwardingList->list.count; i++) {\n+            S1AP_E_RABDataForwardingItemIEs_t *ie = NULL;\n+            S1AP_E_RABDataForwardingItem_t *e_rab = NULL;\n+\n+            ie = (S1AP_E_RABDataForwardingItemIEs_t *)\n+                    E_RABSubjecttoDataForwardingList->list.array[i];\n+            ogs_assert(ie);\n+            e_rab = &ie->value.choice.E_RABDataForwardingItem;\n+\n+            bearer = test_bearer_find_by_ue_ebi(test_ue, e_rab->e_RAB_ID);\n+            ogs_assert(bearer);\n+\n+            if (e_rab->dL_gTP_TEID) {\n+                memcpy(&bearer->handover.dl_teid, e_rab->dL_gTP_TEID->buf,\n+                        sizeof(bearer->handover.dl_teid));\n+                bearer->handover.dl_teid = be32toh(bearer->handover.dl_teid);\n+            }\n+            if (e_rab->dL_transportLayerAddress) {\n+                ogs_assert(OGS_OK ==\n+                        ogs_asn_BIT_STRING_to_ip(\n+                            e_rab->dL_transportLayerAddress,\n+                            &bearer->handover.dl_ip));\n+            }\n+            if (e_rab->uL_GTP_TEID) {\n+                memcpy(&bearer->handover.ul_teid, e_rab->uL_GTP_TEID->buf,\n+                        sizeof(bearer->handover.ul_teid));\n+                bearer->handover.ul_teid = be32toh(bearer->handover.ul_teid);\n+            }\n+            if (e_rab->uL_TransportLayerAddress) {\n+                ogs_assert(OGS_OK ==\n+                        ogs_asn_BIT_STRING_to_ip(\n+                            e_rab->uL_TransportLayerAddress,\n+                            &bearer->handover.ul_ip));\n+            }\n+        }\n+    }\n }\n \n void tests1ap_handle_handover_preparation_failure(\ndiff --git a/tests/core/abts-main.c b/tests/core/abts-main.c\nindex aa6c187c7f..49e2139194 100644\n--- a/tests/core/abts-main.c\n+++ b/tests/core/abts-main.c\n@@ -77,7 +77,7 @@ int main(int argc, const char *const argv[])\n         char *log_level;\n         char *domain_mask;\n     } optarg;\n-    const char *argv_out[argc+2]; /* '-e error' is always added */\n+    const char *argv_out[argc+3]; /* '-e error' is always added */\n     \n     abts_suite *suite = NULL;\n     ogs_pkbuf_config_t config;\ndiff --git a/tests/crypt/abts-main.c b/tests/crypt/abts-main.c\nindex ae4a0e8b99..852786ca47 100644\n--- a/tests/crypt/abts-main.c\n+++ b/tests/crypt/abts-main.c\n@@ -50,7 +50,7 @@ int main(int argc, const char *const argv[])\n         char *log_level;\n         char *domain_mask;\n     } optarg;\n-    const char *argv_out[argc+2]; /* '-e error' is always added */\n+    const char *argv_out[argc+3]; /* '-e error' is always added */\n     \n     abts_suite *suite = NULL;\n     ogs_pkbuf_config_t config;\ndiff --git a/tests/handover/epc-s1-test.c b/tests/handover/epc-s1-test.c\nindex 13b226896d..becee46ec9 100644\n--- a/tests/handover/epc-s1-test.c\n+++ b/tests/handover/epc-s1-test.c\n@@ -28,6 +28,7 @@ static void test1_func(abts_case *tc, void *data)\n     ogs_pkbuf_t *esmbuf;\n     ogs_pkbuf_t *sendbuf;\n     ogs_pkbuf_t *recvbuf;\n+    ogs_pkbuf_t *pkbuf;\n     ogs_s1ap_message_t message;\n \n     ogs_nas_5gs_mobile_identity_suci_t mobile_identity_suci;\n@@ -329,6 +330,35 @@ static void test1_func(abts_case *tc, void *data)\n     ABTS_PTR_NOTNULL(tc, recvbuf);\n     tests1ap_recv(test_ue, recvbuf);\n \n+    /* Send GTP-U ICMP Packet */\n+    bearer = test_bearer_find_by_ue_ebi(test_ue, 5);\n+    ogs_assert(bearer);\n+    rv = test_gtpu_send_ping(gtpu1, bearer, TEST_PING_IPV4);\n+    ABTS_INT_EQUAL(tc, OGS_OK, rv);\n+\n+    /* Receive GTP-U ICMP Packet */\n+    recvbuf = test_gtpu_read(gtpu1);\n+    ABTS_PTR_NOTNULL(tc, recvbuf);\n+\n+    /* Copy ICMP Packet */\n+    pkbuf = ogs_pkbuf_alloc(NULL, 200);\n+    ogs_assert(pkbuf);\n+    ogs_pkbuf_reserve(pkbuf, OGS_GTPV1U_5GC_HEADER_LEN);\n+    ogs_pkbuf_put(pkbuf, 200-OGS_GTPV1U_5GC_HEADER_LEN);\n+    memset(pkbuf->data, 0, pkbuf->len);\n+    memcpy(pkbuf->data, recvbuf->data + 8, recvbuf->len - 8);\n+\n+    ogs_pkbuf_free(recvbuf);\n+\n+    /* Send GTP-U Packet with Indirect Data Forwarding */\n+    rv = test_gtpu_send_indirect_data_forwarding(gtpu1, bearer, pkbuf);\n+    ABTS_INT_EQUAL(tc, OGS_OK, rv);\n+\n+    /* Receive GTP-U ICMP Packet */\n+    recvbuf = testgnb_gtpu_read(gtpu2);\n+    ABTS_PTR_NOTNULL(tc, recvbuf);\n+    ogs_pkbuf_free(recvbuf);\n+\n     /* Send eNB Status Transfer */\n     sendbuf = test_s1ap_build_enb_status_transfer(test_ue);\n     ABTS_PTR_NOTNULL(tc, sendbuf);\ndiff --git a/tests/sctp/abts-main.c b/tests/sctp/abts-main.c\nindex 7092f8a266..2224ad3505 100644\n--- a/tests/sctp/abts-main.c\n+++ b/tests/sctp/abts-main.c\n@@ -45,7 +45,7 @@ int main(int argc, const char *const argv[])\n         char *log_level;\n         char *domain_mask;\n     } optarg;\n-    const char *argv_out[argc+2]; /* '-e error' is always added */\n+    const char *argv_out[argc+3]; /* '-e error' is always added */\n     \n     abts_suite *suite = NULL;\n     ogs_pkbuf_config_t config;\ndiff --git a/tests/unit/abts-main.c b/tests/unit/abts-main.c\nindex 9afbf8c4a1..299f3939f0 100644\n--- a/tests/unit/abts-main.c\n+++ b/tests/unit/abts-main.c\n@@ -67,7 +67,7 @@ int main(int argc, const char *const argv[])\n         char *log_level;\n         char *domain_mask;\n     } optarg;\n-    const char *argv_out[argc+2]; /* '-e error' is always added */\n+    const char *argv_out[argc+3]; /* '-e error' is always added */\n     \n     abts_suite *suite = NULL;\n     ogs_pkbuf_config_t config;\n"
  },
  "CWE-787": {
    "cve": "CVE-2023-4863",
    "commit_url": "https://github.com/webmproject/libwebp/commit/902bc9190331343b2017211debcec8d2ab87e17a",
    "diff": "diff --git a/src/dec/vp8l_dec.c b/src/dec/vp8l_dec.c\nindex 450121629..5ab34f56c 100644\n--- a/src/dec/vp8l_dec.c\n+++ b/src/dec/vp8l_dec.c\n@@ -262,11 +262,11 @@ static int ReadHuffmanCodeLengths(\n   int symbol;\n   int max_symbol;\n   int prev_code_len = DEFAULT_CODE_LENGTH;\n-  HuffmanCode table[1 << LENGTHS_TABLE_BITS];\n+  HuffmanTables tables;\n \n-  if (!VP8LBuildHuffmanTable(table, LENGTHS_TABLE_BITS,\n-                             code_length_code_lengths,\n-                             NUM_CODE_LENGTH_CODES)) {\n+  if (!VP8LHuffmanTablesAllocate(1 << LENGTHS_TABLE_BITS, &tables) ||\n+      !VP8LBuildHuffmanTable(&tables, LENGTHS_TABLE_BITS,\n+                             code_length_code_lengths, NUM_CODE_LENGTH_CODES)) {\n     goto End;\n   }\n \n@@ -286,7 +286,7 @@ static int ReadHuffmanCodeLengths(\n     int code_len;\n     if (max_symbol-- == 0) break;\n     VP8LFillBitWindow(br);\n-    p = &table[VP8LPrefetchBits(br) & LENGTHS_TABLE_MASK];\n+    p = &tables.curr_segment->start[VP8LPrefetchBits(br) & LENGTHS_TABLE_MASK];\n     VP8LSetBitPos(br, br->bit_pos_ + p->bits);\n     code_len = p->value;\n     if (code_len < kCodeLengthLiterals) {\n@@ -309,6 +309,7 @@ static int ReadHuffmanCodeLengths(\n   ok = 1;\n \n  End:\n+  VP8LHuffmanTablesDeallocate(&tables);\n   if (!ok) return VP8LSetError(dec, VP8_STATUS_BITSTREAM_ERROR);\n   return ok;\n }\n@@ -316,7 +317,8 @@ static int ReadHuffmanCodeLengths(\n // 'code_lengths' is pre-allocated temporary buffer, used for creating Huffman\n // tree.\n static int ReadHuffmanCode(int alphabet_size, VP8LDecoder* const dec,\n-                           int* const code_lengths, HuffmanCode* const table) {\n+                           int* const code_lengths,\n+                           HuffmanTables* const table) {\n   int ok = 0;\n   int size = 0;\n   VP8LBitReader* const br = &dec->br_;\n@@ -367,8 +369,7 @@ static int ReadHuffmanCodes(VP8LDecoder* const dec, int xsize, int ysize,\n   VP8LMetadata* const hdr = &dec->hdr_;\n   uint32_t* huffman_image = NULL;\n   HTreeGroup* htree_groups = NULL;\n-  HuffmanCode* huffman_tables = NULL;\n-  HuffmanCode* huffman_table = NULL;\n+  HuffmanTables* huffman_tables = &hdr->huffman_tables_;\n   int num_htree_groups = 1;\n   int num_htree_groups_max = 1;\n   const int max_alphabet_size =\n@@ -378,6 +379,10 @@ static int ReadHuffmanCodes(VP8LDecoder* const dec, int xsize, int ysize,\n   int* mapping = NULL;\n   int ok = 0;\n \n+  // Check the table has been 0 initialized (through InitMetadata).\n+  assert(huffman_tables->root.start == NULL);\n+  assert(huffman_tables->curr_segment == NULL);\n+\n   if (allow_recursion && VP8LReadBits(br, 1)) {\n     // use meta Huffman codes.\n     const int huffman_precision = VP8LReadBits(br, 3) + 2;\n@@ -429,16 +434,15 @@ static int ReadHuffmanCodes(VP8LDecoder* const dec, int xsize, int ysize,\n \n   code_lengths = (int*)WebPSafeCalloc((uint64_t)max_alphabet_size,\n                                       sizeof(*code_lengths));\n-  huffman_tables = (HuffmanCode*)WebPSafeMalloc(num_htree_groups * table_size,\n-                                                sizeof(*huffman_tables));\n   htree_groups = VP8LHtreeGroupsNew(num_htree_groups);\n \n-  if (htree_groups == NULL || code_lengths == NULL || huffman_tables == NULL) {\n+  if (htree_groups == NULL || code_lengths == NULL ||\n+      !VP8LHuffmanTablesAllocate(num_htree_groups * table_size,\n+                                 huffman_tables)) {\n     VP8LSetError(dec, VP8_STATUS_OUT_OF_MEMORY);\n     goto Error;\n   }\n \n-  huffman_table = huffman_tables;\n   for (i = 0; i < num_htree_groups_max; ++i) {\n     // If the index \"i\" is unused in the Huffman image, just make sure the\n     // coefficients are valid but do not store them.\n@@ -463,19 +467,20 @@ static int ReadHuffmanCodes(VP8LDecoder* const dec, int xsize, int ysize,\n       int max_bits = 0;\n       for (j = 0; j < HUFFMAN_CODES_PER_META_CODE; ++j) {\n         int alphabet_size = kAlphabetSize[j];\n-        htrees[j] = huffman_table;\n         if (j == 0 && color_cache_bits > 0) {\n           alphabet_size += (1 << color_cache_bits);\n         }\n-        size = ReadHuffmanCode(alphabet_size, dec, code_lengths, huffman_table);\n+        size =\n+            ReadHuffmanCode(alphabet_size, dec, code_lengths, huffman_tables);\n+        htrees[j] = huffman_tables->curr_segment->curr_table;\n         if (size == 0) {\n           goto Error;\n         }\n         if (is_trivial_literal && kLiteralMap[j] == 1) {\n-          is_trivial_literal = (huffman_table->bits == 0);\n+          is_trivial_literal = (htrees[j]->bits == 0);\n         }\n-        total_size += huffman_table->bits;\n-        huffman_table += size;\n+        total_size += htrees[j]->bits;\n+        huffman_tables->curr_segment->curr_table += size;\n         if (j <= ALPHA) {\n           int local_max_bits = code_lengths[0];\n           int k;\n@@ -510,14 +515,13 @@ static int ReadHuffmanCodes(VP8LDecoder* const dec, int xsize, int ysize,\n   hdr->huffman_image_ = huffman_image;\n   hdr->num_htree_groups_ = num_htree_groups;\n   hdr->htree_groups_ = htree_groups;\n-  hdr->huffman_tables_ = huffman_tables;\n \n  Error:\n   WebPSafeFree(code_lengths);\n   WebPSafeFree(mapping);\n   if (!ok) {\n     WebPSafeFree(huffman_image);\n-    WebPSafeFree(huffman_tables);\n+    VP8LHuffmanTablesDeallocate(huffman_tables);\n     VP8LHtreeGroupsFree(htree_groups);\n   }\n   return ok;\n@@ -1352,7 +1356,7 @@ static void ClearMetadata(VP8LMetadata* const hdr) {\n   assert(hdr != NULL);\n \n   WebPSafeFree(hdr->huffman_image_);\n-  WebPSafeFree(hdr->huffman_tables_);\n+  VP8LHuffmanTablesDeallocate(&hdr->huffman_tables_);\n   VP8LHtreeGroupsFree(hdr->htree_groups_);\n   VP8LColorCacheClear(&hdr->color_cache_);\n   VP8LColorCacheClear(&hdr->saved_color_cache_);\n@@ -1666,7 +1670,7 @@ int VP8LDecodeImage(VP8LDecoder* const dec) {\n \n   if (dec == NULL) return 0;\n \n-  assert(dec->hdr_.huffman_tables_ != NULL);\n+  assert(dec->hdr_.huffman_tables_.root.start != NULL);\n   assert(dec->hdr_.htree_groups_ != NULL);\n   assert(dec->hdr_.num_htree_groups_ > 0);\n \ndiff --git a/src/dec/vp8li_dec.h b/src/dec/vp8li_dec.h\nindex 72b2e8612..32540a4b8 100644\n--- a/src/dec/vp8li_dec.h\n+++ b/src/dec/vp8li_dec.h\n@@ -51,7 +51,7 @@ typedef struct {\n   uint32_t*       huffman_image_;\n   int             num_htree_groups_;\n   HTreeGroup*     htree_groups_;\n-  HuffmanCode*    huffman_tables_;\n+  HuffmanTables   huffman_tables_;\n } VP8LMetadata;\n \n typedef struct VP8LDecoder VP8LDecoder;\ndiff --git a/src/utils/huffman_utils.c b/src/utils/huffman_utils.c\nindex 90c2fbf7c..cf73abd43 100644\n--- a/src/utils/huffman_utils.c\n+++ b/src/utils/huffman_utils.c\n@@ -177,21 +177,24 @@ static int BuildHuffmanTable(HuffmanCode* const root_table, int root_bits,\n       if (num_open < 0) {\n         return 0;\n       }\n-      if (root_table == NULL) continue;\n       for (; count[len] > 0; --count[len]) {\n         HuffmanCode code;\n         if ((key & mask) != low) {\n-          table += table_size;\n+          if (root_table != NULL) table += table_size;\n           table_bits = NextTableBitSize(count, len, root_bits);\n           table_size = 1 << table_bits;\n           total_size += table_size;\n           low = key & mask;\n-          root_table[low].bits = (uint8_t)(table_bits + root_bits);\n-          root_table[low].value = (uint16_t)((table - root_table) - low);\n+          if (root_table != NULL) {\n+            root_table[low].bits = (uint8_t)(table_bits + root_bits);\n+            root_table[low].value = (uint16_t)((table - root_table) - low);\n+          }\n+        }\n+        if (root_table != NULL) {\n+          code.bits = (uint8_t)(len - root_bits);\n+          code.value = (uint16_t)sorted[symbol++];\n+          ReplicateValue(&table[key >> root_bits], step, table_size, code);\n         }\n-        code.bits = (uint8_t)(len - root_bits);\n-        code.value = (uint16_t)sorted[symbol++];\n-        ReplicateValue(&table[key >> root_bits], step, table_size, code);\n         key = GetNextKey(key, len);\n       }\n     }\n@@ -211,25 +214,83 @@ static int BuildHuffmanTable(HuffmanCode* const root_table, int root_bits,\n   ((1 << MAX_CACHE_BITS) + NUM_LITERAL_CODES + NUM_LENGTH_CODES)\n // Cut-off value for switching between heap and stack allocation.\n #define SORTED_SIZE_CUTOFF 512\n-int VP8LBuildHuffmanTable(HuffmanCode* const root_table, int root_bits,\n+int VP8LBuildHuffmanTable(HuffmanTables* const root_table, int root_bits,\n                           const int code_lengths[], int code_lengths_size) {\n-  int total_size;\n+  const int total_size =\n+      BuildHuffmanTable(NULL, root_bits, code_lengths, code_lengths_size, NULL);\n   assert(code_lengths_size <= MAX_CODE_LENGTHS_SIZE);\n-  if (root_table == NULL) {\n-    total_size = BuildHuffmanTable(NULL, root_bits,\n-                                   code_lengths, code_lengths_size, NULL);\n-  } else if (code_lengths_size <= SORTED_SIZE_CUTOFF) {\n+  if (total_size == 0 || root_table == NULL) return total_size;\n+\n+  if (root_table->curr_segment->curr_table + total_size >=\n+      root_table->curr_segment->start + root_table->curr_segment->size) {\n+    // If 'root_table' does not have enough memory, allocate a new segment.\n+    // The available part of root_table->curr_segment is left unused because we\n+    // need a contiguous buffer.\n+    const int segment_size = root_table->curr_segment->size;\n+    struct HuffmanTablesSegment* next =\n+        (HuffmanTablesSegment*)WebPSafeMalloc(1, sizeof(*next));\n+    if (next == NULL) return 0;\n+    // Fill the new segment.\n+    // We need at least 'total_size' but if that value is small, it is better to\n+    // allocate a big chunk to prevent more allocations later. 'segment_size' is\n+    // therefore chosen (any other arbitrary value could be chosen).\n+    next->size = total_size > segment_size ? total_size : segment_size;\n+    next->start =\n+        (HuffmanCode*)WebPSafeMalloc(next->size, sizeof(*next->start));\n+    if (next->start == NULL) {\n+      WebPSafeFree(next);\n+      return 0;\n+    }\n+    next->curr_table = next->start;\n+    next->next = NULL;\n+    // Point to the new segment.\n+    root_table->curr_segment->next = next;\n+    root_table->curr_segment = next;\n+  }\n+  if (code_lengths_size <= SORTED_SIZE_CUTOFF) {\n     // use local stack-allocated array.\n     uint16_t sorted[SORTED_SIZE_CUTOFF];\n-    total_size = BuildHuffmanTable(root_table, root_bits,\n-                                   code_lengths, code_lengths_size, sorted);\n-  } else {   // rare case. Use heap allocation.\n+    BuildHuffmanTable(root_table->curr_segment->curr_table, root_bits,\n+                      code_lengths, code_lengths_size, sorted);\n+  } else {  // rare case. Use heap allocation.\n     uint16_t* const sorted =\n         (uint16_t*)WebPSafeMalloc(code_lengths_size, sizeof(*sorted));\n     if (sorted == NULL) return 0;\n-    total_size = BuildHuffmanTable(root_table, root_bits,\n-                                   code_lengths, code_lengths_size, sorted);\n+    BuildHuffmanTable(root_table->curr_segment->curr_table, root_bits,\n+                      code_lengths, code_lengths_size, sorted);\n     WebPSafeFree(sorted);\n   }\n   return total_size;\n }\n+\n+int VP8LHuffmanTablesAllocate(int size, HuffmanTables* huffman_tables) {\n+  // Have 'segment' point to the first segment for now, 'root'.\n+  HuffmanTablesSegment* const root = &huffman_tables->root;\n+  huffman_tables->curr_segment = root;\n+  // Allocate root.\n+  root->start = (HuffmanCode*)WebPSafeMalloc(size, sizeof(*root->start));\n+  if (root->start == NULL) return 0;\n+  root->curr_table = root->start;\n+  root->next = NULL;\n+  root->size = size;\n+  return 1;\n+}\n+\n+void VP8LHuffmanTablesDeallocate(HuffmanTables* const huffman_tables) {\n+  HuffmanTablesSegment *current, *next;\n+  if (huffman_tables == NULL) return;\n+  // Free the root node.\n+  current = &huffman_tables->root;\n+  next = current->next;\n+  WebPSafeFree(current->start);\n+  current->start = NULL;\n+  current->next = NULL;\n+  current = next;\n+  // Free the following nodes.\n+  while (current != NULL) {\n+    next = current->next;\n+    WebPSafeFree(current->start);\n+    WebPSafeFree(current);\n+    current = next;\n+  }\n+}\ndiff --git a/src/utils/huffman_utils.h b/src/utils/huffman_utils.h\nindex 13b7ad1ac..98415c532 100644\n--- a/src/utils/huffman_utils.h\n+++ b/src/utils/huffman_utils.h\n@@ -43,6 +43,29 @@ typedef struct {\n                     // or non-literal symbol otherwise\n } HuffmanCode32;\n \n+// Contiguous memory segment of HuffmanCodes.\n+typedef struct HuffmanTablesSegment {\n+  HuffmanCode* start;\n+  // Pointer to where we are writing into the segment. Starts at 'start' and\n+  // cannot go beyond 'start' + 'size'.\n+  HuffmanCode* curr_table;\n+  // Pointer to the next segment in the chain.\n+  struct HuffmanTablesSegment* next;\n+  int size;\n+} HuffmanTablesSegment;\n+\n+// Chained memory segments of HuffmanCodes.\n+typedef struct HuffmanTables {\n+  HuffmanTablesSegment root;\n+  // Currently processed segment. At first, this is 'root'.\n+  HuffmanTablesSegment* curr_segment;\n+} HuffmanTables;\n+\n+// Allocates a HuffmanTables with 'size' contiguous HuffmanCodes. Returns 0 on\n+// memory allocation error, 1 otherwise.\n+int VP8LHuffmanTablesAllocate(int size, HuffmanTables* huffman_tables);\n+void VP8LHuffmanTablesDeallocate(HuffmanTables* const huffman_tables);\n+\n #define HUFFMAN_PACKED_BITS 6\n #define HUFFMAN_PACKED_TABLE_SIZE (1u << HUFFMAN_PACKED_BITS)\n \n@@ -78,9 +101,7 @@ void VP8LHtreeGroupsFree(HTreeGroup* const htree_groups);\n // the huffman table.\n // Returns built table size or 0 in case of error (invalid tree or\n // memory error).\n-// If root_table is NULL, it returns 0 if a lookup cannot be built, something\n-// > 0 otherwise (but not the table size).\n-int VP8LBuildHuffmanTable(HuffmanCode* const root_table, int root_bits,\n+int VP8LBuildHuffmanTable(HuffmanTables* const root_table, int root_bits,\n                           const int code_lengths[], int code_lengths_size);\n \n #ifdef __cplusplus\n"
  },
  "CWE-754": {
    "cve": "CVE-2025-53359",
    "commit_url": "https://github.com/rust-ethereum/ethereum/commit/2dd9d1d5d0936ec7350093ff3a5a7169a349db77",
    "diff": "diff --git a/src/block.rs b/src/block.rs\nindex 416e147..031468d 100644\n--- a/src/block.rs\n+++ b/src/block.rs\n@@ -126,10 +126,16 @@ impl From<BlockV1> for BlockV3 {\n mod tests {\n \tuse super::*;\n \tuse crate::transaction::{\n-\t\tAuthorizationListItem, EIP7702Transaction, TransactionAction, TransactionV3,\n+\t\teip2930, eip7702::AuthorizationListItem, legacy::TransactionAction, EIP7702Transaction,\n+\t\tTransactionV3,\n \t};\n \tuse ethereum_types::{H160, H256, U256};\n \n+\tconst ONE: H256 = H256([\n+\t\t0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n+\t\t0, 1,\n+\t]);\n+\n \t#[test]\n \tfn block_v3_with_eip7702_transaction() {\n \t\t// Create an EIP-7702 transaction\n@@ -147,13 +153,13 @@ mod tests {\n \t\t\t\tchain_id: 1,\n \t\t\t\taddress: H160::zero(),\n \t\t\t\tnonce: U256::zero(),\n-\t\t\t\ty_parity: false,\n-\t\t\t\tr: H256::zero(),\n-\t\t\t\ts: H256::zero(),\n+\t\t\t\tsignature: eip2930::MalleableTransactionSignature {\n+\t\t\t\t\todd_y_parity: false,\n+\t\t\t\t\tr: ONE,\n+\t\t\t\t\ts: ONE,\n+\t\t\t\t},\n \t\t\t}],\n-\t\t\todd_y_parity: false,\n-\t\t\tr: H256::zero(),\n-\t\t\ts: H256::zero(),\n+\t\t\tsignature: eip2930::TransactionSignature::new(false, ONE, ONE).unwrap(),\n \t\t});\n \n \t\t// Create a block with the EIP-7702 transaction\n@@ -207,9 +213,7 @@ mod tests {\n \t\t\tvalue: U256::zero(),\n \t\t\tinput: vec![],\n \t\t\taccess_list: vec![],\n-\t\t\todd_y_parity: false,\n-\t\t\tr: H256::zero(),\n-\t\t\ts: H256::zero(),\n+\t\t\tsignature: eip2930::TransactionSignature::new(false, ONE, ONE).unwrap(),\n \t\t});\n \n \t\tlet partial_header = PartialHeader {\ndiff --git a/src/transaction/eip1559.rs b/src/transaction/eip1559.rs\nindex 2526f01..2e009c6 100644\n--- a/src/transaction/eip1559.rs\n+++ b/src/transaction/eip1559.rs\n@@ -2,10 +2,9 @@ use ethereum_types::{H256, U256};\n use rlp::{DecoderError, Rlp, RlpStream};\n use sha3::{Digest, Keccak256};\n \n-use crate::{\n-\ttransaction::{AccessList, TransactionAction},\n-\tBytes,\n-};\n+use crate::Bytes;\n+\n+pub use super::eip2930::{AccessList, TransactionAction, TransactionSignature};\n \n #[derive(Clone, Debug, PartialEq, Eq)]\n #[cfg_attr(\n@@ -28,9 +27,7 @@ pub struct EIP1559Transaction {\n \tpub value: U256,\n \tpub input: Bytes,\n \tpub access_list: AccessList,\n-\tpub odd_y_parity: bool,\n-\tpub r: H256,\n-\tpub s: H256,\n+\tpub signature: TransactionSignature,\n }\n \n impl EIP1559Transaction {\n@@ -69,9 +66,9 @@ impl rlp::Encodable for EIP1559Transaction {\n \t\ts.append(&self.value);\n \t\ts.append(&self.input);\n \t\ts.append_list(&self.access_list);\n-\t\ts.append(&self.odd_y_parity);\n-\t\ts.append(&U256::from_big_endian(&self.r[..]));\n-\t\ts.append(&U256::from_big_endian(&self.s[..]));\n+\t\ts.append(&self.signature.odd_y_parity());\n+\t\ts.append(&U256::from_big_endian(&self.signature.r()[..]));\n+\t\ts.append(&U256::from_big_endian(&self.signature.s()[..]));\n \t}\n }\n \n@@ -91,9 +88,13 @@ impl rlp::Decodable for EIP1559Transaction {\n \t\t\tvalue: rlp.val_at(6)?,\n \t\t\tinput: rlp.val_at(7)?,\n \t\t\taccess_list: rlp.list_at(8)?,\n-\t\t\todd_y_parity: rlp.val_at(9)?,\n-\t\t\tr: H256::from(rlp.val_at::<U256>(10)?.to_big_endian()),\n-\t\t\ts: H256::from(rlp.val_at::<U256>(11)?.to_big_endian()),\n+\t\t\tsignature: {\n+\t\t\t\tlet odd_y_parity = rlp.val_at(9)?;\n+\t\t\t\tlet r = H256::from(rlp.val_at::<U256>(10)?.to_big_endian());\n+\t\t\t\tlet s = H256::from(rlp.val_at::<U256>(11)?.to_big_endian());\n+\t\t\t\tTransactionSignature::new(odd_y_parity, r, s)\n+\t\t\t\t\t.ok_or(DecoderError::Custom(\"Invalid transaction signature format\"))?\n+\t\t\t},\n \t\t})\n \t}\n }\ndiff --git a/src/transaction/eip2930.rs b/src/transaction/eip2930.rs\nindex e080ed1..d23162b 100644\n--- a/src/transaction/eip2930.rs\n+++ b/src/transaction/eip2930.rs\n@@ -4,7 +4,117 @@ use ethereum_types::{Address, H256, U256};\n use rlp::{DecoderError, Rlp, RlpStream};\n use sha3::{Digest, Keccak256};\n \n-use crate::{transaction::TransactionAction, Bytes};\n+use crate::Bytes;\n+\n+pub use super::legacy::TransactionAction;\n+\n+#[derive(Clone, Debug, PartialEq, Eq)]\n+#[cfg_attr(\n+\tfeature = \"with-scale\",\n+\tderive(\n+\t\tscale_info::TypeInfo,\n+\t\tscale_codec::Encode,\n+\t\tscale_codec::Decode,\n+\t\tscale_codec::DecodeWithMemTracking\n+\t)\n+)]\n+#[cfg_attr(feature = \"with-serde\", derive(serde::Serialize, serde::Deserialize))]\n+pub struct MalleableTransactionSignature {\n+\tpub odd_y_parity: bool,\n+\tpub r: H256,\n+\tpub s: H256,\n+}\n+\n+#[derive(Clone, Debug, PartialEq, Eq)]\n+#[cfg_attr(\n+\tfeature = \"with-scale\",\n+\tderive(\n+\t\tscale_info::TypeInfo,\n+\t\tscale_codec::Encode,\n+\t\tscale_codec::DecodeWithMemTracking\n+\t)\n+)]\n+#[cfg_attr(feature = \"with-serde\", derive(serde::Serialize))]\n+pub struct TransactionSignature {\n+\todd_y_parity: bool,\n+\tr: H256,\n+\ts: H256,\n+}\n+\n+impl TransactionSignature {\n+\t#[must_use]\n+\tpub fn new(odd_y_parity: bool, r: H256, s: H256) -> Option<Self> {\n+\t\tconst LOWER: H256 = H256([\n+\t\t\t0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n+\t\t\t0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n+\t\t\t0x00, 0x00, 0x00, 0x01,\n+\t\t]);\n+\t\tconst UPPER: H256 = H256([\n+\t\t\t0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,\n+\t\t\t0xff, 0xfe, 0xba, 0xae, 0xdc, 0xe6, 0xaf, 0x48, 0xa0, 0x3b, 0xbf, 0xd2, 0x5e, 0x8c,\n+\t\t\t0xd0, 0x36, 0x41, 0x41,\n+\t\t]);\n+\n+\t\tlet is_valid = r < UPPER && r >= LOWER && s < UPPER && s >= LOWER;\n+\n+\t\tif is_valid {\n+\t\t\tSome(Self { odd_y_parity, r, s })\n+\t\t} else {\n+\t\t\tNone\n+\t\t}\n+\t}\n+\n+\t#[must_use]\n+\tpub fn odd_y_parity(&self) -> bool {\n+\t\tself.odd_y_parity\n+\t}\n+\n+\t#[must_use]\n+\tpub fn r(&self) -> &H256 {\n+\t\t&self.r\n+\t}\n+\n+\t#[must_use]\n+\tpub fn s(&self) -> &H256 {\n+\t\t&self.s\n+\t}\n+\n+\t#[must_use]\n+\tpub fn is_low_s(&self) -> bool {\n+\t\tconst LOWER: H256 = H256([\n+\t\t\t0x7f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,\n+\t\t\t0xff, 0xff, 0x5d, 0x57, 0x6e, 0x73, 0x57, 0xa4, 0x50, 0x1d, 0xdf, 0xe9, 0x2f, 0x46,\n+\t\t\t0x68, 0x1b, 0x20, 0xa0,\n+\t\t]);\n+\n+\t\tself.s <= LOWER\n+\t}\n+}\n+\n+#[cfg(feature = \"with-scale\")]\n+impl scale_codec::Decode for TransactionSignature {\n+\tfn decode<I: scale_codec::Input>(value: &mut I) -> Result<Self, scale_codec::Error> {\n+\t\tlet unchecked = MalleableTransactionSignature::decode(value)?;\n+\t\tmatch Self::new(unchecked.odd_y_parity, unchecked.r, unchecked.s) {\n+\t\t\tSome(signature) => Ok(signature),\n+\t\t\tNone => Err(scale_codec::Error::from(\"Invalid signature\")),\n+\t\t}\n+\t}\n+}\n+\n+#[cfg(feature = \"with-serde\")]\n+impl<'de> serde::Deserialize<'de> for TransactionSignature {\n+\tfn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n+\twhere\n+\t\tD: serde::de::Deserializer<'de>,\n+\t{\n+\t\tlet unchecked = MalleableTransactionSignature::deserialize(deserializer)?;\n+\t\tOk(\n+\t\t\tTransactionSignature::new(unchecked.odd_y_parity, unchecked.r, unchecked.s)\n+\t\t\t\t.ok_or(serde::de::Error::custom(\"invalid signature\"))?,\n+\t\t)\n+\t}\n+}\n \n #[derive(Clone, Debug, PartialEq, Eq)]\n #[cfg_attr(\n@@ -61,9 +171,7 @@ pub struct EIP2930Transaction {\n \tpub value: U256,\n \tpub input: Bytes,\n \tpub access_list: AccessList,\n-\tpub odd_y_parity: bool,\n-\tpub r: H256,\n-\tpub s: H256,\n+\tpub signature: TransactionSignature,\n }\n \n impl EIP2930Transaction {\n@@ -100,9 +208,9 @@ impl rlp::Encodable for EIP2930Transaction {\n \t\ts.append(&self.value);\n \t\ts.append(&self.input);\n \t\ts.append_list(&self.access_list);\n-\t\ts.append(&self.odd_y_parity);\n-\t\ts.append(&U256::from_big_endian(&self.r[..]));\n-\t\ts.append(&U256::from_big_endian(&self.s[..]));\n+\t\ts.append(&self.signature.odd_y_parity());\n+\t\ts.append(&U256::from_big_endian(&self.signature.r()[..]));\n+\t\ts.append(&U256::from_big_endian(&self.signature.s()[..]));\n \t}\n }\n \n@@ -121,9 +229,13 @@ impl rlp::Decodable for EIP2930Transaction {\n \t\t\tvalue: rlp.val_at(5)?,\n \t\t\tinput: rlp.val_at(6)?,\n \t\t\taccess_list: rlp.list_at(7)?,\n-\t\t\todd_y_parity: rlp.val_at(8)?,\n-\t\t\tr: H256::from(rlp.val_at::<U256>(9)?.to_big_endian()),\n-\t\t\ts: H256::from(rlp.val_at::<U256>(10)?.to_big_endian()),\n+\t\t\tsignature: {\n+\t\t\t\tlet odd_y_parity = rlp.val_at(8)?;\n+\t\t\t\tlet r = H256::from(rlp.val_at::<U256>(9)?.to_big_endian());\n+\t\t\t\tlet s = H256::from(rlp.val_at::<U256>(10)?.to_big_endian());\n+\t\t\t\tTransactionSignature::new(odd_y_parity, r, s)\n+\t\t\t\t\t.ok_or(DecoderError::Custom(\"Invalid transaction signature format\"))?\n+\t\t\t},\n \t\t})\n \t}\n }\ndiff --git a/src/transaction/eip7702.rs b/src/transaction/eip7702.rs\nindex 616c4ee..ab13b26 100644\n--- a/src/transaction/eip7702.rs\n+++ b/src/transaction/eip7702.rs\n@@ -5,9 +5,10 @@ use k256::ecdsa::{RecoveryId, Signature, VerifyingKey};\n use rlp::{DecoderError, Rlp, RlpStream};\n use sha3::{Digest, Keccak256};\n \n-use crate::{\n-\ttransaction::{AccessList, TransactionAction},\n-\tBytes,\n+use crate::Bytes;\n+\n+pub use super::eip2930::{\n+\tAccessList, MalleableTransactionSignature, TransactionAction, TransactionSignature,\n };\n \n /// Error type for EIP-7702 authorization signature recovery\n@@ -45,9 +46,7 @@ pub struct AuthorizationListItem {\n \tpub chain_id: u64,\n \tpub address: Address,\n \tpub nonce: U256,\n-\tpub y_parity: bool,\n-\tpub r: H256,\n-\tpub s: H256,\n+\tpub signature: MalleableTransactionSignature,\n }\n \n impl rlp::Encodable for AuthorizationListItem {\n@@ -56,9 +55,9 @@ impl rlp::Encodable for AuthorizationListItem {\n \t\ts.append(&self.chain_id);\n \t\ts.append(&self.address);\n \t\ts.append(&self.nonce);\n-\t\ts.append(&self.y_parity);\n-\t\ts.append(&U256::from_big_endian(&self.r[..]));\n-\t\ts.append(&U256::from_big_endian(&self.s[..]));\n+\t\ts.append(&self.signature.odd_y_parity);\n+\t\ts.append(&U256::from_big_endian(&self.signature.r[..]));\n+\t\ts.append(&U256::from_big_endian(&self.signature.s[..]));\n \t}\n }\n \n@@ -72,29 +71,48 @@ impl rlp::Decodable for AuthorizationListItem {\n \t\t\tchain_id: rlp.val_at(0)?,\n \t\t\taddress: rlp.val_at(1)?,\n \t\t\tnonce: rlp.val_at(2)?,\n-\t\t\ty_parity: rlp.val_at(3)?,\n-\t\t\tr: H256::from(rlp.val_at::<U256>(4)?.to_big_endian()),\n-\t\t\ts: H256::from(rlp.val_at::<U256>(5)?.to_big_endian()),\n+\t\t\tsignature: {\n+\t\t\t\tlet odd_y_parity = rlp.val_at(3)?;\n+\t\t\t\tlet r = H256::from(rlp.val_at::<U256>(4)?.to_big_endian());\n+\t\t\t\tlet s = H256::from(rlp.val_at::<U256>(5)?.to_big_endian());\n+\t\t\t\tMalleableTransactionSignature { odd_y_parity, r, s }\n+\t\t\t},\n \t\t})\n \t}\n }\n \n impl AuthorizationListItem {\n+\t/// Check and get the signature.\n+\t///\n+\t/// This checks that the signature is not malleable, but does not otherwise check or recover\n+\t/// the public key.\n+\tpub fn signature(&self) -> Option<TransactionSignature> {\n+\t\tTransactionSignature::new(\n+\t\t\tself.signature.odd_y_parity,\n+\t\t\tself.signature.r,\n+\t\t\tself.signature.s,\n+\t\t)\n+\t}\n+\n \t/// Recover the authorizing address from the authorization signature according to EIP-7702\n \tpub fn authorizing_address(&self) -> Result<Address, AuthorizationError> {\n \t\t// Create the authorization message hash according to EIP-7702\n \t\tlet message_hash = self.authorization_message_hash();\n \n+\t\tlet sigv = self\n+\t\t\t.signature()\n+\t\t\t.ok_or(AuthorizationError::InvalidSignature)?;\n+\n \t\t// Create signature from r and s components\n \t\tlet mut signature_bytes = [0u8; 64];\n-\t\tsignature_bytes[0..32].copy_from_slice(&self.r[..]);\n-\t\tsignature_bytes[32..64].copy_from_slice(&self.s[..]);\n+\t\tsignature_bytes[0..32].copy_from_slice(&sigv.r()[..]);\n+\t\tsignature_bytes[32..64].copy_from_slice(&sigv.s()[..]);\n \n \t\t// Create the signature and recovery ID\n \t\tlet signature = Signature::from_bytes(&signature_bytes.into())\n \t\t\t.map_err(|_| AuthorizationError::InvalidSignature)?;\n \n-\t\tlet recovery_id = RecoveryId::try_from(if self.y_parity { 1u8 } else { 0u8 })\n+\t\tlet recovery_id = RecoveryId::try_from(if sigv.odd_y_parity() { 1u8 } else { 0u8 })\n \t\t\t.map_err(|_| AuthorizationError::InvalidRecoveryId)?;\n \n \t\t// Recover the verifying key using VerifyingKey::recover_from_prehash\n@@ -169,9 +187,7 @@ pub struct EIP7702Transaction {\n \tpub data: Bytes,\n \tpub access_list: AccessList,\n \tpub authorization_list: AuthorizationList,\n-\tpub odd_y_parity: bool,\n-\tpub r: H256,\n-\tpub s: H256,\n+\tpub signature: TransactionSignature,\n }\n \n impl EIP7702Transaction {\n@@ -212,9 +228,9 @@ impl rlp::Encodable for EIP7702Transaction {\n \t\ts.append(&self.data);\n \t\ts.append_list(&self.access_list);\n \t\ts.append_list(&self.authorization_list);\n-\t\ts.append(&self.odd_y_parity);\n-\t\ts.append(&U256::from_big_endian(&self.r[..]));\n-\t\ts.append(&U256::from_big_endian(&self.s[..]));\n+\t\ts.append(&self.signature.odd_y_parity());\n+\t\ts.append(&U256::from_big_endian(&self.signature.r()[..]));\n+\t\ts.append(&U256::from_big_endian(&self.signature.s()[..]));\n \t}\n }\n \n@@ -235,9 +251,13 @@ impl rlp::Decodable for EIP7702Transaction {\n \t\t\tdata: rlp.val_at(7)?,\n \t\t\taccess_list: rlp.list_at(8)?,\n \t\t\tauthorization_list: rlp.list_at(9)?,\n-\t\t\todd_y_parity: rlp.val_at(10)?,\n-\t\t\tr: H256::from(rlp.val_at::<U256>(11)?.to_big_endian()),\n-\t\t\ts: H256::from(rlp.val_at::<U256>(12)?.to_big_endian()),\n+\t\t\tsignature: {\n+\t\t\t\tlet odd_y_parity = rlp.val_at(10)?;\n+\t\t\t\tlet r = H256::from(rlp.val_at::<U256>(11)?.to_big_endian());\n+\t\t\t\tlet s = H256::from(rlp.val_at::<U256>(12)?.to_big_endian());\n+\t\t\t\tTransactionSignature::new(odd_y_parity, r, s)\n+\t\t\t\t\t.ok_or(DecoderError::Custom(\"Invalid transaction signature format\"))?\n+\t\t\t},\n \t\t})\n \t}\n }\n@@ -340,9 +360,11 @@ mod tests {\n \t\t\tchain_id,\n \t\t\taddress,\n \t\t\tnonce,\n-\t\t\ty_parity,\n-\t\t\tr,\n-\t\t\ts,\n+\t\t\tsignature: MalleableTransactionSignature {\n+\t\t\t\todd_y_parity: y_parity,\n+\t\t\t\tr,\n+\t\t\t\ts,\n+\t\t\t},\n \t\t};\n \n \t\t// Recover the authorizing address\n@@ -371,34 +393,21 @@ mod tests {\n \t#[test]\n \tfn test_authorizing_address_error_handling() {\n \t\t// Test with invalid signature components (zero values are invalid in ECDSA)\n-\t\tlet auth_item = AuthorizationListItem {\n-\t\t\tchain_id: 1,\n-\t\t\taddress: Address::from_slice(&[0x42u8; 20]),\n-\t\t\tnonce: U256::zero(),\n-\t\t\ty_parity: false,\n-\t\t\tr: H256::zero(), // Invalid r value (r cannot be zero)\n-\t\t\ts: H256::zero(), // Invalid s value (s cannot be zero)\n-\t\t};\n-\n-\t\t// This should return an error due to invalid signature\n-\t\tlet result = auth_item.authorizing_address();\n-\t\tassert!(result.is_err());\n-\t\tassert_eq!(result.unwrap_err(), AuthorizationError::InvalidSignature);\n+\t\tassert!(TransactionSignature::new(\n+\t\t\tfalse,\n+\t\t\tH256::zero(), // Invalid r value (r cannot be zero)\n+\t\t\tH256::zero(), // Invalid s value (s cannot be zero)\n+\t\t)\n+\t\t.is_none());\n \n \t\t// Test with values that are too high (greater than secp256k1 curve order)\n \t\t// secp256k1 curve order is FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141\n-\t\tlet auth_item_high_values = AuthorizationListItem {\n-\t\t\tchain_id: 1,\n-\t\t\taddress: Address::from_slice(&[0x42u8; 20]),\n-\t\t\tnonce: U256::zero(),\n-\t\t\ty_parity: false,\n+\t\tassert!(TransactionSignature::new(\n+\t\t\tfalse,\n \t\t\t// Use maximum possible values which exceed the curve order\n-\t\t\tr: H256::from_slice(&[0xFF; 32]),\n-\t\t\ts: H256::from_slice(&[0xFF; 32]),\n-\t\t};\n-\n-\t\tlet result = auth_item_high_values.authorizing_address();\n-\t\tassert!(result.is_err());\n-\t\tassert_eq!(result.unwrap_err(), AuthorizationError::InvalidSignature);\n+\t\t\tH256::from_slice(&[0xFF; 32]),\n+\t\t\tH256::from_slice(&[0xFF; 32]),\n+\t\t)\n+\t\t.is_none());\n \t}\n }\ndiff --git a/src/transaction/legacy.rs b/src/transaction/legacy.rs\nindex 5b09f2c..6ed5704 100644\n--- a/src/transaction/legacy.rs\n+++ b/src/transaction/legacy.rs\n@@ -91,7 +91,7 @@ impl TransactionRecoveryId {\n \tfeature = \"with-scale\",\n \tderive(scale_info::TypeInfo, scale_codec::DecodeWithMemTracking)\n )]\n-#[cfg_attr(feature = \"with-serde\", derive(serde::Serialize, serde::Deserialize))]\n+#[cfg_attr(feature = \"with-serde\", derive(serde::Serialize))]\n pub struct TransactionSignature {\n \tv: TransactionRecoveryId,\n \tr: H256,\n@@ -181,6 +181,29 @@ impl scale_codec::Decode for TransactionSignature {\n \t}\n }\n \n+#[cfg(feature = \"with-serde\")]\n+#[derive(serde::Deserialize)]\n+struct TransactionSignatureUnchecked {\n+\tv: u64,\n+\tr: H256,\n+\ts: H256,\n+}\n+\n+#[cfg(feature = \"with-serde\")]\n+impl<'de> serde::Deserialize<'de> for TransactionSignature {\n+\tfn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n+\twhere\n+\t\tD: serde::de::Deserializer<'de>,\n+\t{\n+\t\tlet unchecked = TransactionSignatureUnchecked::deserialize(deserializer)?;\n+\n+\t\tOk(\n+\t\t\tTransactionSignature::new(unchecked.v, unchecked.r, unchecked.s)\n+\t\t\t\t.ok_or(serde::de::Error::custom(\"invalid signature\"))?,\n+\t\t)\n+\t}\n+}\n+\n #[derive(Clone, Debug, PartialEq, Eq)]\n #[cfg_attr(\n \tfeature = \"with-scale\",\ndiff --git a/src/transaction/mod.rs b/src/transaction/mod.rs\nindex 04989ec..8ac6a02 100644\n--- a/src/transaction/mod.rs\n+++ b/src/transaction/mod.rs\n@@ -1,23 +1,15 @@\n-mod eip1559;\n-mod eip2930;\n-mod eip7702;\n-mod legacy;\n+pub mod eip1559;\n+pub mod eip2930;\n+pub mod eip7702;\n+pub mod legacy;\n \n use bytes::BytesMut;\n use ethereum_types::H256;\n use rlp::{DecoderError, Rlp};\n \n pub use self::{\n-\teip1559::{EIP1559Transaction, EIP1559TransactionMessage},\n-\teip2930::{AccessList, AccessListItem, EIP2930Transaction, EIP2930TransactionMessage},\n-\teip7702::{\n-\t\tAuthorizationList, AuthorizationListItem, EIP7702Transaction, EIP7702TransactionMessage,\n-\t\tAUTHORIZATION_MAGIC, SET_CODE_TX_TYPE,\n-\t},\n-\tlegacy::{\n-\t\tLegacyTransaction, LegacyTransactionMessage, TransactionAction, TransactionRecoveryId,\n-\t\tTransactionSignature,\n-\t},\n+\teip1559::EIP1559Transaction, eip2930::EIP2930Transaction, eip7702::EIP7702Transaction,\n+\tlegacy::LegacyTransaction,\n };\n use crate::enveloped::{EnvelopedDecodable, EnvelopedDecoderError, EnvelopedEncodable};\n \n@@ -328,7 +320,14 @@ pub type TransactionAny = TransactionV3;\n \n #[cfg(test)]\n mod tests {\n-\tuse super::*;\n+\tuse super::{\n+\t\teip2930::{self, AccessListItem},\n+\t\teip7702::AuthorizationListItem,\n+\t\tlegacy::{self, TransactionAction},\n+\t\tEIP1559Transaction, EIP2930Transaction, EIP7702Transaction, EnvelopedDecodable,\n+\t\tTransactionV0, TransactionV1, TransactionV2, TransactionV3,\n+\t};\n+\tuse crate::enveloped::*;\n \tuse ethereum_types::U256;\n \tuse hex_literal::hex;\n \n@@ -353,7 +352,7 @@ mod tests {\n \t\t\t),\n \t\t\tvalue: U256::from(10) * 1_000_000_000 * 1_000_000_000,\n \t\t\tinput: hex!(\"a9059cbb000000000213ed0f886efd100b67c7e4ec0a85a7d20dc971600000000000000000000015af1d78b58c4000\").into(),\n-\t\t\tsignature: TransactionSignature::new(38, hex!(\"be67e0a07db67da8d446f76add590e54b6e92cb6b8f9835aeb67540579a27717\").into(), hex!(\"2d690516512020171c1ec870f6ff45398cc8609250326be89915fb538e7bd718\").into()).unwrap(),\n+\t\t\tsignature: legacy::TransactionSignature::new(38, hex!(\"be67e0a07db67da8d446f76add590e54b6e92cb6b8f9835aeb67540579a27717\").into(), hex!(\"2d690516512020171c1ec870f6ff45398cc8609250326be89915fb538e7bd718\").into()).unwrap(),\n \t\t};\n \n \t\tassert_eq!(\n@@ -389,9 +388,12 @@ mod tests {\n \t\t\t\t\tstorage_keys: vec![],\n \t\t\t\t},\n \t\t\t],\n-\t\t\todd_y_parity: false,\n-\t\t\tr: hex!(\"36b241b061a36a32ab7fe86c7aa9eb592dd59018cd0443adc0903590c16b02b0\").into(),\n-\t\t\ts: hex!(\"5edcc541b4741c5cc6dd347c5ed9577ef293a62787b4510465fadbfe39ee4094\").into(),\n+\t\t\tsignature: eip2930::TransactionSignature::new(\n+\t\t\t\tfalse,\n+\t\t\t\thex!(\"36b241b061a36a32ab7fe86c7aa9eb592dd59018cd0443adc0903590c16b02b0\").into(),\n+\t\t\t\thex!(\"5edcc541b4741c5cc6dd347c5ed9577ef293a62787b4510465fadbfe39ee4094\").into(),\n+\t\t\t)\n+\t\t\t.unwrap(),\n \t\t});\n \n \t\tassert_eq!(\n@@ -428,9 +430,12 @@ mod tests {\n \t\t\t\t\tstorage_keys: vec![],\n \t\t\t\t},\n \t\t\t],\n-\t\t\todd_y_parity: false,\n-\t\t\tr: hex!(\"36b241b061a36a32ab7fe86c7aa9eb592dd59018cd0443adc0903590c16b02b0\").into(),\n-\t\t\ts: hex!(\"5edcc541b4741c5cc6dd347c5ed9577ef293a62787b4510465fadbfe39ee4094\").into(),\n+\t\t\tsignature: eip2930::TransactionSignature::new(\n+\t\t\t\tfalse,\n+\t\t\t\thex!(\"36b241b061a36a32ab7fe86c7aa9eb592dd59018cd0443adc0903590c16b02b0\").into(),\n+\t\t\t\thex!(\"5edcc541b4741c5cc6dd347c5ed9577ef293a62787b4510465fadbfe39ee4094\").into(),\n+\t\t\t)\n+\t\t\t.unwrap(),\n \t\t});\n \n \t\tassert_eq!(\n@@ -463,13 +468,20 @@ mod tests {\n \t\t\t\tchain_id: 5,\n \t\t\t\taddress: hex!(\"de0b295669a9fd93d5f28d9ec85e40f4cb697bae\").into(),\n \t\t\t\tnonce: 1.into(),\n-\t\t\t\ty_parity: false,\n-\t\t\t\tr: hex!(\"36b241b061a36a32ab7fe86c7aa9eb592dd59018cd0443adc0903590c16b02b0\").into(),\n-\t\t\t\ts: hex!(\"5edcc541b4741c5cc6dd347c5ed9577ef293a62787b4510465fadbfe39ee4094\").into(),\n+\t\t\t\tsignature: eip2930::MalleableTransactionSignature {\n+\t\t\t\t\todd_y_parity: false,\n+\t\t\t\t\tr: hex!(\"36b241b061a36a32ab7fe86c7aa9eb592dd59018cd0443adc0903590c16b02b0\")\n+\t\t\t\t\t\t.into(),\n+\t\t\t\t\ts: hex!(\"5edcc541b4741c5cc6dd347c5ed9577ef293a62787b4510465fadbfe39ee4094\")\n+\t\t\t\t\t\t.into(),\n+\t\t\t\t},\n \t\t\t}],\n-\t\t\todd_y_parity: false,\n-\t\t\tr: hex!(\"36b241b061a36a32ab7fe86c7aa9eb592dd59018cd0443adc0903590c16b02b0\").into(),\n-\t\t\ts: hex!(\"5edcc541b4741c5cc6dd347c5ed9577ef293a62787b4510465fadbfe39ee4094\").into(),\n+\t\t\tsignature: eip2930::TransactionSignature::new(\n+\t\t\t\tfalse,\n+\t\t\t\thex!(\"36b241b061a36a32ab7fe86c7aa9eb592dd59018cd0443adc0903590c16b02b0\").into(),\n+\t\t\t\thex!(\"5edcc541b4741c5cc6dd347c5ed9577ef293a62787b4510465fadbfe39ee4094\").into(),\n+\t\t\t)\n+\t\t\t.unwrap(),\n \t\t});\n \n \t\tassert_eq!(\n"
  },
  "CWE-190": {
    "cve": "CVE-2023-46246",
    "commit_url": "https://github.com/vim/vim/commit/9198c1f2b1ddecde22af918541e0de2a32f0f45a",
    "diff": "diff --git a/src/cmdhist.c b/src/cmdhist.c\nindex d398ca7a687b56..96a9b3e95b86f1 100644\n--- a/src/cmdhist.c\n+++ b/src/cmdhist.c\n@@ -742,7 +742,10 @@ ex_history(exarg_T *eap)\n \tend = arg;\n     if (!get_list_range(&end, &hisidx1, &hisidx2) || *end != NUL)\n     {\n-\tsemsg(_(e_trailing_characters_str), end);\n+\tif (*end != NUL)\n+\t    semsg(_(e_trailing_characters_str), end);\n+\telse\n+\t    semsg(_(e_val_too_large), arg);\n \treturn;\n     }\n \ndiff --git a/src/errors.h b/src/errors.h\nindex 79a785e1e2953c..72957d8b93bdcd 100644\n--- a/src/errors.h\n+++ b/src/errors.h\n@@ -3560,3 +3560,5 @@ EXTERN char e_xattr_e2big[]\n \tINIT(= N_(\"E1508: Size of the extended attribute value is larger than the maximum size allowed\"));\n EXTERN char e_xattr_other[]\n \tINIT(= N_(\"E1509: Error occurred when reading or writing extended attribute\"));\n+EXTERN char e_val_too_large[]\n+\tINIT(= N_(\"E1510: Value too large: %s\"));\ndiff --git a/src/ex_getln.c b/src/ex_getln.c\nindex 9683e2ebd5af50..8f0be520886bed 100644\n--- a/src/ex_getln.c\n+++ b/src/ex_getln.c\n@@ -4377,6 +4377,10 @@ get_list_range(char_u **str, int *num1, int *num2)\n     {\n \tvim_str2nr(*str, NULL, &len, 0, &num, NULL, 0, FALSE, NULL);\n \t*str += len;\n+\t// overflow\n+\tif (num > INT_MAX)\n+\t    return FAIL;\n+\n \t*num1 = (int)num;\n \tfirst = TRUE;\n     }\n@@ -4387,8 +4391,12 @@ get_list_range(char_u **str, int *num1, int *num2)\n \tvim_str2nr(*str, NULL, &len, 0, &num, NULL, 0, FALSE, NULL);\n \tif (len > 0)\n \t{\n-\t    *num2 = (int)num;\n \t    *str = skipwhite(*str + len);\n+\t    // overflow\n+\t    if (num > INT_MAX)\n+\t\treturn FAIL;\n+\n+\t    *num2 = (int)num;\n \t}\n \telse if (!first)\t\t// no number given at all\n \t    return FAIL;\ndiff --git a/src/testdir/test_history.vim b/src/testdir/test_history.vim\nindex bb6d67172585e6..482328ab4aaefc 100644\n--- a/src/testdir/test_history.vim\n+++ b/src/testdir/test_history.vim\n@@ -254,4 +254,12 @@ func Test_history_crypt_key()\n   set key& bs& ts&\n endfunc\n \n+\" The following used to overflow and causing an use-after-free\n+func Test_history_max_val()\n+\n+  set history=10\n+  call assert_fails(':history 2147483648', 'E1510:')\n+  set history&\n+endfunc\n+\n \" vim: shiftwidth=2 sts=2 expandtab\ndiff --git a/src/version.c b/src/version.c\nindex 1ae5c98714d112..0284594f7653ce 100644\n--- a/src/version.c\n+++ b/src/version.c\n@@ -704,6 +704,8 @@ static char *(features[]) =\n \n static int included_patches[] =\n {   /* Add new patch number below this line */\n+/**/\n+    2068,\n /**/\n     2067,\n /**/\n"
  },
  "CWE-354": {
    "cve": "CVE-2023-34459",
    "commit_url": "https://github.com/OpenZeppelin/openzeppelin-contracts/commit/4d2383e17186be3e8ccf5a442e9686ecc7de1c55",
    "diff": "diff --git a/.changeset/shy-crews-teach.md b/.changeset/shy-crews-teach.md\nnew file mode 100644\nindex 00000000000..8ab929bf88d\n--- /dev/null\n+++ b/.changeset/shy-crews-teach.md\n@@ -0,0 +1,5 @@\n+---\n+'openzeppelin-solidity': patch\n+---\n+\n+`MerkleProof`: Fix a bug in `processMultiProof` and `processMultiProofCalldata` that allows proving arbitrary leaves if the tree contains a node with value 0 at depth 1.\ndiff --git a/contracts/utils/cryptography/MerkleProof.sol b/contracts/utils/cryptography/MerkleProof.sol\nindex cd79e51cf77..6943d720426 100644\n--- a/contracts/utils/cryptography/MerkleProof.sol\n+++ b/contracts/utils/cryptography/MerkleProof.sol\n@@ -121,10 +121,11 @@ library MerkleProof {\n         // `hashes` array. At the end of the process, the last hash in the `hashes` array should contain the root of\n         // the merkle tree.\n         uint256 leavesLen = leaves.length;\n+        uint256 proofLen = proof.length;\n         uint256 totalHashes = proofFlags.length;\n \n         // Check proof validity.\n-        require(leavesLen + proof.length - 1 == totalHashes, \"MerkleProof: invalid multiproof\");\n+        require(leavesLen + proofLen - 1 == totalHashes, \"MerkleProof: invalid multiproof\");\n \n         // The xxxPos values are \"pointers\" to the next value to consume in each array. All accesses are done using\n         // `xxx[xxxPos++]`, which return the current value and increment the pointer, thus mimicking a queue's \"pop\".\n@@ -146,6 +147,7 @@ library MerkleProof {\n         }\n \n         if (totalHashes > 0) {\n+            require(proofPos == proofLen, \"MerkleProof: invalid multiproof\");\n             unchecked {\n                 return hashes[totalHashes - 1];\n             }\n@@ -173,10 +175,11 @@ library MerkleProof {\n         // `hashes` array. At the end of the process, the last hash in the `hashes` array should contain the root of\n         // the merkle tree.\n         uint256 leavesLen = leaves.length;\n+        uint256 proofLen = proof.length;\n         uint256 totalHashes = proofFlags.length;\n \n         // Check proof validity.\n-        require(leavesLen + proof.length - 1 == totalHashes, \"MerkleProof: invalid multiproof\");\n+        require(leavesLen + proofLen - 1 == totalHashes, \"MerkleProof: invalid multiproof\");\n \n         // The xxxPos values are \"pointers\" to the next value to consume in each array. All accesses are done using\n         // `xxx[xxxPos++]`, which return the current value and increment the pointer, thus mimicking a queue's \"pop\".\n@@ -198,6 +201,7 @@ library MerkleProof {\n         }\n \n         if (totalHashes > 0) {\n+            require(proofPos == proofLen, \"MerkleProof: invalid multiproof\");\n             unchecked {\n                 return hashes[totalHashes - 1];\n             }\ndiff --git a/test/utils/cryptography/MerkleProof.test.js b/test/utils/cryptography/MerkleProof.test.js\nindex 62157b56a84..9ed10fd0fce 100644\n--- a/test/utils/cryptography/MerkleProof.test.js\n+++ b/test/utils/cryptography/MerkleProof.test.js\n@@ -1,11 +1,8 @@\n-require('@openzeppelin/test-helpers');\n-\n const { expectRevert } = require('@openzeppelin/test-helpers');\n+const { expect } = require('chai');\n const { MerkleTree } = require('merkletreejs');\n const keccak256 = require('keccak256');\n \n-const { expect } = require('chai');\n-\n const MerkleProof = artifacts.require('$MerkleProof');\n \n contract('MerkleProof', function () {\n@@ -176,5 +173,28 @@ contract('MerkleProof', function () {\n       expect(await this.merkleProof.$multiProofVerify([root], [], root, [])).to.equal(true);\n       expect(await this.merkleProof.$multiProofVerifyCalldata([root], [], root, [])).to.equal(true);\n     });\n+\n+    it('reverts processing manipulated proofs with a zero-value node at depth 1', async function () {\n+      // Create a merkle tree that contains a zero leaf at depth 1\n+      const leaves = [keccak256('real leaf'), Buffer.alloc(32, 0)];\n+      const merkleTree = new MerkleTree(leaves, keccak256, { sortPairs: true });\n+\n+      const root = merkleTree.getRoot();\n+\n+      // Now we can pass any  ** malicious ** fake leaves as valid!\n+      const maliciousLeaves = ['some', 'malicious', 'leaves'].map(keccak256).sort(Buffer.compare);\n+      const maliciousProof = [leaves[0], leaves[0]];\n+      const maliciousProofFlags = [true, true, false];\n+\n+      await expectRevert(\n+        this.merkleProof.$multiProofVerify(maliciousProof, maliciousProofFlags, root, maliciousLeaves),\n+        'MerkleProof: invalid multiproof',\n+      );\n+\n+      await expectRevert(\n+        this.merkleProof.$multiProofVerifyCalldata(maliciousProof, maliciousProofFlags, root, maliciousLeaves),\n+        'MerkleProof: invalid multiproof',\n+      );\n+    });\n   });\n });\n"
  },
  "CWE-119": {
    "cve": "CVE-2025-11012",
    "commit_url": "https://github.com/BehaviorTree/BehaviorTree.CPP/commit/cb6c7514efa628adb8180b58b4c9ccdebbe096e3",
    "diff": "diff --git a/src/script_parser.cpp b/src/script_parser.cpp\nindex 5bbc62006..95c629fa2 100644\n--- a/src/script_parser.cpp\n+++ b/src/script_parser.cpp\n@@ -13,11 +13,12 @@ using ErrorReport = lexy_ext::_report_error<char*>;\n \n Expected<ScriptFunction> ParseScript(const std::string& script)\n {\n-  char error_msgs_buffer[2048];\n+  std::string error_msgs_buffer;  // dynamically growing error buffer\n \n   auto input = lexy::string_input<lexy::utf8_encoding>(script);\n-  auto result =\n-      lexy::parse<BT::Grammar::stmt>(input, ErrorReport().to(error_msgs_buffer));\n+\n+  auto reporter = ErrorReport().to(std::back_inserter(error_msgs_buffer));\n+  auto result = lexy::parse<BT::Grammar::stmt>(input, reporter);\n   if(result.has_value() && result.error_count() == 0)\n   {\n     try\n@@ -69,11 +70,12 @@ BT::Expected<Any> ParseScriptAndExecute(Ast::Environment& env, const std::string\n \n Result ValidateScript(const std::string& script)\n {\n-  char error_msgs_buffer[2048];\n+  std::string error_msgs_buffer;  // dynamically growing error buffer\n \n   auto input = lexy::string_input<lexy::utf8_encoding>(script);\n-  auto result =\n-      lexy::parse<BT::Grammar::stmt>(input, ErrorReport().to(error_msgs_buffer));\n+\n+  auto reporter = ErrorReport().to(std::back_inserter(error_msgs_buffer));\n+  auto result = lexy::parse<BT::Grammar::stmt>(input, reporter);\n   if(result.has_value() && result.error_count() == 0)\n   {\n     try\n"
  },
  "CWE-617": {
    "cve": "CVE-2025-8836",
    "commit_url": "https://github.com/jasper-software/jasper/commit/79185d32d7a444abae441935b20ae4676b3513d4",
    "diff": "diff --git a/data/test/other/poc_401.pnm b/data/test/other/poc_401.pnm\nnew file mode 100644\nindex 00000000..cbf12072\nBinary files /dev/null and b/data/test/other/poc_401.pnm differ\ndiff --git a/src/libjasper/jpc/jpc_enc.c b/src/libjasper/jpc/jpc_enc.c\nindex 041c030a..e910a863 100644\n--- a/src/libjasper/jpc/jpc_enc.c\n+++ b/src/libjasper/jpc/jpc_enc.c\n@@ -484,18 +484,36 @@ static jpc_enc_cp_t *cp_create(const char *optstr, jas_image_t *image)\n \t\t\tcp->tileheight = atoi(jas_tvparser_getval(tvp));\n \t\t\tbreak;\n \t\tcase OPT_PRCWIDTH:\n-\t\t\tprcwidthexpn = jpc_floorlog2(atoi(jas_tvparser_getval(tvp)));\n+\t\t\ti = atoi(jas_tvparser_getval(tvp));\n+\t\t\tif (i <= 0) {\n+\t\t\t\tjas_logerrorf(\"invalid precinct width (%d)\\n\", i);\n+\t\t\t\tgoto error;\n+\t\t\t}\n+\t\t\tprcwidthexpn = jpc_floorlog2(i);\n \t\t\tbreak;\n \t\tcase OPT_PRCHEIGHT:\n-\t\t\tprcheightexpn = jpc_floorlog2(atoi(jas_tvparser_getval(tvp)));\n+\t\t\ti = atoi(jas_tvparser_getval(tvp));\n+\t\t\tif (i <= 0) {\n+\t\t\t\tjas_logerrorf(\"invalid precinct height (%d)\\n\", i);\n+\t\t\t\tgoto error;\n+\t\t\t}\n+\t\t\tprcheightexpn = jpc_floorlog2(i);\n \t\t\tbreak;\n \t\tcase OPT_CBLKWIDTH:\n-\t\t\ttccp->cblkwidthexpn =\n-\t\t\t  jpc_floorlog2(atoi(jas_tvparser_getval(tvp)));\n+\t\t\ti = atoi(jas_tvparser_getval(tvp));\n+\t\t\tif (i <= 0) {\n+\t\t\t\tjas_logerrorf(\"invalid code block width (%d)\\n\", i);\n+\t\t\t\tgoto error;\n+\t\t\t}\n+\t\t\ttccp->cblkwidthexpn = jpc_floorlog2(i);\n \t\t\tbreak;\n \t\tcase OPT_CBLKHEIGHT:\n-\t\t\ttccp->cblkheightexpn =\n-\t\t\t  jpc_floorlog2(atoi(jas_tvparser_getval(tvp)));\n+\t\t\ti = atoi(jas_tvparser_getval(tvp));\n+\t\t\tif (i <= 0) {\n+\t\t\t\tjas_logerrorf(\"invalid code block height (%d)\\n\", i);\n+\t\t\t\tgoto error;\n+\t\t\t}\n+\t\t\ttccp->cblkheightexpn = jpc_floorlog2(i);\n \t\t\tbreak;\n \t\tcase OPT_MODE:\n \t\t\tif ((tagid = jas_taginfo_nonull(jas_taginfos_lookup(modetab,\ndiff --git a/src/libjasper/jpc/jpc_t2dec.c b/src/libjasper/jpc/jpc_t2dec.c\nindex de77623a..1eff88a0 100644\n--- a/src/libjasper/jpc/jpc_t2dec.c\n+++ b/src/libjasper/jpc/jpc_t2dec.c\n@@ -348,7 +348,8 @@ static int jpc_dec_decodepkt(jpc_dec_t *dec, jas_stream_t *pkthdrstream, jas_str\n \t\t\t\t\t\tconst unsigned n = JAS_MIN((unsigned)numnewpasses, maxpasses);\n \t\t\t\t\t\tmycounter += n;\n \t\t\t\t\t\tnumnewpasses -= n;\n-\t\t\t\t\t\tif ((len = jpc_bitstream_getbits(inb, cblk->numlenbits + jpc_floorlog2(n))) < 0) {\n+\t\t\t\t\t\tif ((len = jpc_bitstream_getbits(inb,\n+\t\t\t\t\t\t  cblk->numlenbits + jpc_floorlog2(n))) < 0) {\n \t\t\t\t\t\t\tjpc_bitstream_close(inb);\n \t\t\t\t\t\t\tjas_logerrorf(\"cannot get bits\\n\");\n \t\t\t\t\t\t\treturn -1;\n"
  },
  "CWE-476": {
    "cve": "CVE-2023-2609",
    "commit_url": "https://github.com/vim/vim/commit/d1ae8366aff286d41e7f5bc513cc0a1af5130aad",
    "diff": "diff --git a/src/register.c b/src/register.c\nindex f3df79cfd6426..e481d843c249e 100644\n--- a/src/register.c\n+++ b/src/register.c\n@@ -301,7 +301,7 @@ get_register(\n     if (copy)\n     {\n \t// If we run out of memory some or all of the lines are empty.\n-\tif (reg->y_size == 0)\n+\tif (reg->y_size == 0 || y_current->y_array == NULL)\n \t    reg->y_array = NULL;\n \telse\n \t    reg->y_array = ALLOC_MULT(char_u *, reg->y_size);\ndiff --git a/src/testdir/test_registers.vim b/src/testdir/test_registers.vim\nindex e966932478d80..33ea0f4bd3e6c 100644\n--- a/src/testdir/test_registers.vim\n+++ b/src/testdir/test_registers.vim\n@@ -835,6 +835,23 @@ func Test_end_reg_executing()\n   bwipe!\n endfunc\n \n+\" This was causing a crash because y_append was ending up being NULL\n+func Test_zero_y_append()\n+  \" Run in a separate Vim instance because changing 'encoding' may cause\n+  \" trouble for later tests.\n+  let lines =<< trim END\n+      d\n+      silent ?n\n+      next <sfile>\n+      so\n+      sil! norm 0V\u0080PS\u0003\u0011P\n+      set enc=latin1\n+      \u00a0\n+  END\n+  call writefile(lines, 'XTest_zero_y_append', 'D')\n+  call RunVim([], [], '-u NONE -i NONE -e -s -S XTest_zero_y_append -c qa\\!')\n+endfunc\n+\n \" Make sure that y_append is correctly reset\n \" and the previous register is working as expected\n func Test_register_y_append_reset()\ndiff --git a/src/version.c b/src/version.c\nindex 310803cb82938..7ee9f575f9bd9 100644\n--- a/src/version.c\n+++ b/src/version.c\n@@ -695,6 +695,8 @@ static char *(features[]) =\n \n static int included_patches[] =\n {   /* Add new patch number below this line */\n+/**/\n+    1531,\n /**/\n     1530,\n /**/\n"
  },
  "CWE-362": {
    "cve": "CVE-2025-48880",
    "commit_url": "https://github.com/freescout-help-desk/freescout/commit/3f5bb2841f7de3303bc3cb00930a28440754d122",
    "diff": "diff --git a/app/Http/Controllers/UsersController.php b/app/Http/Controllers/UsersController.php\nindex 67f57d1f9..f06ef9eb7 100644\n--- a/app/Http/Controllers/UsersController.php\n+++ b/app/Http/Controllers/UsersController.php\n@@ -528,15 +528,20 @@ public function ajax(Request $request)\n                     $response['msg'] = __('User not found');\n                 } elseif (!$auth_user->can('delete', $user)) {\n                     $response['msg'] = __('Not enough permissions');\n+                } elseif ($auth_user->id == $user->id) {\n+                    // Do not allow admin delete himself.\n+                    $response['msg'] = __('Not enough permissions');\n                 }\n \n                 // Check if the user is the only one admin\n-                if (!$response['msg'] && $user->isAdmin()) {\n-                    $admins_count = User::where('role', User::ROLE_ADMIN)->count();\n-                    if ($admins_count < 2) {\n-                        $response['msg'] = __('Administrator can not be deleted');\n-                    }\n-                }\n+                // - not needed as only admin can delete  users and\n+                // current admin can not delete himself.\n+                // if (!$response['msg'] && $user->isAdmin()) {\n+                //     $admins_count = User::where('role', User::ROLE_ADMIN)->count();\n+                //     if ($admins_count < 2) {\n+                //         $response['msg'] = __('Administrator can not be deleted');\n+                //     }\n+                // }\n \n                 if (!$response['msg']) {\n \n"
  },
  "CWE-732": {
    "cve": "CVE-2023-23939",
    "commit_url": "https://github.com/Azure/setup-kubectl/commit/d449d75495d2b9d1463555bb00ca3dca77a42ab6",
    "diff": "diff --git a/src/run.test.ts b/src/run.test.ts\nindex a25dad2a..e82e04a6 100644\n--- a/src/run.test.ts\n+++ b/src/run.test.ts\n@@ -106,7 +106,7 @@ describe('Testing all functions in run file.', () => {\n         expect(toolCache.downloadTool).toBeCalled();\n         expect(toolCache.cacheFile).toBeCalled();\n         expect(os.type).toBeCalled();\n-        expect(fs.chmodSync).toBeCalledWith(path.join('pathToCachedTool', 'kubectl.exe'), '777');\n+        expect(fs.chmodSync).toBeCalledWith(path.join('pathToCachedTool', 'kubectl.exe'), '775');\n     });\n \n     test('downloadKubectl() - throw DownloadKubectlFailed error when unable to download kubectl', async () => {\n@@ -144,7 +144,7 @@ describe('Testing all functions in run file.', () => {\n         expect(await run.downloadKubectl('v1.15.0')).toBe(path.join('pathToCachedTool', 'kubectl.exe'));\n         expect(toolCache.find).toBeCalledWith('kubectl', 'v1.15.0');\n         expect(os.type).toBeCalled();\n-        expect(fs.chmodSync).toBeCalledWith(path.join('pathToCachedTool', 'kubectl.exe'), '777');\n+        expect(fs.chmodSync).toBeCalledWith(path.join('pathToCachedTool', 'kubectl.exe'), '775');\n         expect(toolCache.downloadTool).not.toBeCalled();\n     });\n \ndiff --git a/src/run.ts b/src/run.ts\nindex 5a2d2f64..59ae0935 100644\n--- a/src/run.ts\n+++ b/src/run.ts\n@@ -57,7 +57,7 @@ export async function downloadKubectl(version: string): Promise<string> {\n     }\r\n \r\n     const kubectlPath = path.join(cachedToolpath, kubectlToolName + getExecutableExtension());\r\n-    fs.chmodSync(kubectlPath, '777');\r\n+    fs.chmodSync(kubectlPath, '775');\r\n     return kubectlPath;\r\n }\r\n \r\n"
  },
  "CWE-703": {
    "cve": "CVE-2025-54134",
    "commit_url": "https://github.com/haxtheweb/haxcms-nodejs/commit/e9773d1996233f9bafb06832b8220ec2a98bab34",
    "diff": "diff --git a/src/routes/listFiles.js b/src/routes/listFiles.js\nindex 8a5b6ab..e5cba67 100644\n--- a/src/routes/listFiles.js\n+++ b/src/routes/listFiles.js\n@@ -21,46 +21,49 @@ const mime = require('mime');\n    */\n   async function listFiles(req, res) {\n     let files = [];\n-    let site = await HAXCMS.loadSite(req.query['siteName']);\n-    if (site && site.siteDirectory) {\n-      let search = (typeof req.query['filename'] !== 'undefined') ? req.query['filename'] : '';\n-      // build files directory path\n-      let siteFilePath = path.join(site.siteDirectory, 'files');\n-      let handle;\n-      if (handle = fs.readdirSync(siteFilePath)) {\n-        handle.forEach(file => {\n-          if (\n-              file != \".\" &&\n-              file != \"..\" &&\n-              file != '.gitkeep' &&\n-              file != '.DS_Store'\n-          ) {\n-            // ensure this is a file\n+    // verify that we have params expected from frontend\n+    if (req.query && req.query['siteName']) {\n+      let site = await HAXCMS.loadSite(req.query['siteName']);\n+      if (site && site.siteDirectory) {\n+        let search = (typeof req.query['filename'] !== 'undefined') ? req.query['filename'] : '';\n+        // build files directory path\n+        let siteFilePath = path.join(site.siteDirectory, 'files');\n+        let handle;\n+        if (handle = fs.readdirSync(siteFilePath)) {\n+          handle.forEach(file => {\n             if (\n-              fs.lstatSync(siteFilePath + '/' + file).isFile()\n+                file != \".\" &&\n+                file != \"..\" &&\n+                file != '.gitkeep' &&\n+                file != '.DS_Store'\n             ) {\n-              // ensure this is a file and if we are searching for results then return only exact ones\n-              if (search == \"\" || file.indexOf(search) !== -1) {\n-                let fullUrl = '/files/' + file;\n-                // multiple sites then append the base url to site management area\n-                if (HAXCMS.operatingContext == 'multisite') {\n-                  fullUrl = HAXCMS.basePath +\n-                  HAXCMS.sitesDirectory + '/' +\n-                  site.manifest.metadata.site.name + '/files/' + file\n+              // ensure this is a file\n+              if (\n+                fs.lstatSync(siteFilePath + '/' + file).isFile()\n+              ) {\n+                // ensure this is a file and if we are searching for results then return only exact ones\n+                if (!search || search == \"\" || file.indexOf(search) !== -1) {\n+                  let fullUrl = '/files/' + file;\n+                  // multiple sites then append the base url to site management area\n+                  if (HAXCMS.operatingContext == 'multisite') {\n+                    fullUrl = HAXCMS.basePath +\n+                    HAXCMS.sitesDirectory + '/' +\n+                    site.manifest.metadata.site.name + '/files/' + file\n+                  }\n+                  files.push({\n+                    'path' : 'files/' + file,\n+                    'fullUrl' : fullUrl,\n+                    'url' : 'files/' + file,\n+                    'mimetype' : mime.getType(siteFilePath + '/' + file),\n+                    'name' : file\n+                  });\n                 }\n-                files.push({\n-                  'path' : 'files/' + file,\n-                  'fullUrl' : fullUrl,\n-                  'url' : 'files/' + file,\n-                  'mimetype' : mime.getType(siteFilePath + '/' + file),\n-                  'name' : file\n-                });\n+              } else {\n+                  // @todo maybe step into directories?\n               }\n-            } else {\n-                // @todo maybe step into directories?\n             }\n-          }\n-        });\n+          });\n+        }\n       }\n     }\n     res.send(files);\n"
  },
  "CWE-668": {
    "cve": "CVE-2024-51754",
    "commit_url": "https://github.com/twigphp/Twig/commit/2bb8c2460a2c519c498df9b643d5277117155a73",
    "diff": "diff --git a/src/Extension/SandboxExtension.php b/src/Extension/SandboxExtension.php\nindex 4e96760f7d4..c9ffe6477bd 100644\n--- a/src/Extension/SandboxExtension.php\n+++ b/src/Extension/SandboxExtension.php\n@@ -119,6 +119,14 @@ public function checkPropertyAllowed($obj, $property, int $lineno = -1, ?Source\n \n     public function ensureToStringAllowed($obj, int $lineno = -1, ?Source $source = null)\n     {\n+        if (\\is_array($obj)) {\n+            foreach ($obj as $v) {\n+                $this->ensureToStringAllowed($v, $lineno, $source);\n+            }\n+\n+            return $obj;\n+        }\n+\n         if ($this->isSandboxed($source) && $obj instanceof \\Stringable) {\n             try {\n                 $this->policy->checkMethodAllowed($obj, '__toString');\ndiff --git a/src/NodeVisitor/SandboxNodeVisitor.php b/src/NodeVisitor/SandboxNodeVisitor.php\nindex 37e184a3edc..8c15db0d6f2 100644\n--- a/src/NodeVisitor/SandboxNodeVisitor.php\n+++ b/src/NodeVisitor/SandboxNodeVisitor.php\n@@ -15,12 +15,14 @@\n use Twig\\Node\\CheckSecurityCallNode;\n use Twig\\Node\\CheckSecurityNode;\n use Twig\\Node\\CheckToStringNode;\n+use Twig\\Node\\Expression\\ArrayExpression;\n use Twig\\Node\\Expression\\Binary\\ConcatBinary;\n use Twig\\Node\\Expression\\Binary\\RangeBinary;\n use Twig\\Node\\Expression\\FilterExpression;\n use Twig\\Node\\Expression\\FunctionExpression;\n use Twig\\Node\\Expression\\GetAttrExpression;\n use Twig\\Node\\Expression\\NameExpression;\n+use Twig\\Node\\Expression\\Unary\\SpreadUnary;\n use Twig\\Node\\ModuleNode;\n use Twig\\Node\\Node;\n use Twig\\Node\\PrintNode;\n@@ -120,7 +122,18 @@ private function wrapNode(Node $node, string $name): void\n     {\n         $expr = $node->getNode($name);\n         if (($expr instanceof NameExpression || $expr instanceof GetAttrExpression) && !$expr->isGenerator()) {\n-            $node->setNode($name, new CheckToStringNode($expr));\n+            // Simplify in 4.0 as the spread attribute has been removed there\n+            $new = new CheckToStringNode($expr);\n+            if ($expr->hasAttribute('spread')) {\n+                $new->setAttribute('spread', $expr->getAttribute('spread'));\n+            }\n+            $node->setNode($name, $new);\n+        } elseif ($expr instanceof SpreadUnary) {\n+            $this->wrapNode($expr, 'node');\n+        } elseif ($expr instanceof ArrayExpression) {\n+            foreach ($expr as $name => $_) {\n+                $this->wrapNode($expr, $name);\n+            }\n         }\n     }\n \ndiff --git a/tests/Extension/SandboxTest.php b/tests/Extension/SandboxTest.php\nindex d24a06c6720..59e68f67ec2 100644\n--- a/tests/Extension/SandboxTest.php\n+++ b/tests/Extension/SandboxTest.php\n@@ -42,6 +42,7 @@ protected function setUp(): void\n             'obj' => new FooObject(),\n             'arr' => ['obj' => new FooObject()],\n             'child_obj' => new ChildClass(),\n+            'some_array' => [5, 6, 7, new FooObject()],\n         ];\n \n         self::$templates = [\n@@ -246,10 +247,10 @@ public function testSandboxUnallowedProperty()\n      */\n     public function testSandboxUnallowedToString($template)\n     {\n-        $twig = $this->getEnvironment(true, [], ['index' => $template], [], ['upper'], ['Twig\\Tests\\Extension\\FooObject' => 'getAnotherFooObject'], [], ['random']);\n+        $twig = $this->getEnvironment(true, [], ['index' => $template], [], ['upper', 'join', 'replace'], ['Twig\\Tests\\Extension\\FooObject' => 'getAnotherFooObject'], [], ['random']);\n         try {\n             $twig->load('index')->render(self::$params);\n-            $this->fail('Sandbox throws a SecurityError exception if an unallowed method (__toString()) is called in the template');\n+            $this->fail('Sandbox throws a SecurityError exception if an unallowed method \"__toString()\" method is called in the template');\n         } catch (SecurityNotAllowedMethodError $e) {\n             $this->assertEquals('Twig\\Tests\\Extension\\FooObject', $e->getClassName(), 'Exception should be raised on the \"Twig\\Tests\\Extension\\FooObject\" class');\n             $this->assertEquals('__tostring', $e->getMethodName(), 'Exception should be raised on the \"__toString\" method');\n@@ -272,6 +273,16 @@ public static function getSandboxUnallowedToStringTests()\n             'object_chain_and_function' => ['{{ random(obj.anotherFooObject) }}'],\n             'concat' => ['{{ obj ~ \"\" }}'],\n             'concat_again' => ['{{ \"\" ~ obj }}'],\n+            'object_in_arguments' => ['{{ \"__toString\"|replace({\"__toString\": obj}) }}'],\n+            'object_in_array' => ['{{ [12, \"foo\", obj]|join(\", \") }}'],\n+            'object_in_array_var' => ['{{ some_array|join(\", \") }}'],\n+            'object_in_array_nested' => ['{{ [12, \"foo\", [12, \"foo\", obj]]|join(\", \") }}'],\n+            'object_in_array_var_nested' => ['{{ [12, \"foo\", some_array]|join(\", \") }}'],\n+            'object_in_array_dynamic_key' => ['{{ {(obj): \"foo\"}|join(\", \") }}'],\n+            'object_in_array_dynamic_key_nested' => ['{{ {\"foo\": { (obj): \"foo\" }}|join(\", \") }}'],\n+            'context' => ['{{ _context|join(\", \") }}'],\n+            'spread_array_operator' => ['{{ [1, 2, ...[5, 6, 7, obj]]|join(\",\") }}'],\n+            'spread_array_operator_var' => ['{{ [1, 2, ...some_array]|join(\",\") }}'],\n         ];\n     }\n \n"
  },
  "CWE-834": {
    "cve": "CVE-2023-5632",
    "commit_url": "https://github.com/eclipse/mosquitto/commit/18bad1ff32435e523d7507e9b2ce0010124a8f2d",
    "diff": "diff --git a/lib/packet_mosq.c b/lib/packet_mosq.c\nindex ad2a3aae8e..84f93cb83a 100644\n--- a/lib/packet_mosq.c\n+++ b/lib/packet_mosq.c\n@@ -216,7 +216,9 @@ int packet__write(struct mosquitto *mosq)\n \tif(mosq->sock == INVALID_SOCKET) return MOSQ_ERR_NO_CONN;\n \n #ifdef WITH_BROKER\n-\tmux__add_out(mosq);\n+\tif (mosq->current_out_packet) {\n+\t   mux__add_out(mosq);\n+\t}\n #endif\n \n \tpthread_mutex_lock(&mosq->current_out_packet_mutex);\n"
  },
  "CWE-444": {
    "cve": "CVE-2025-49005",
    "commit_url": "https://github.com/vercel/next.js/commit/ec202eccf05820b60c6126d6411fe16766ecc066",
    "diff": "diff --git a/packages/next/src/server/base-server.ts b/packages/next/src/server/base-server.ts\nindex 80e10f84f1ccf..3cfd47c753c63 100644\n--- a/packages/next/src/server/base-server.ts\n+++ b/packages/next/src/server/base-server.ts\n@@ -104,6 +104,7 @@ import {\n   NEXT_ROUTER_SEGMENT_PREFETCH_HEADER,\n   NEXT_DID_POSTPONE_HEADER,\n   NEXT_URL,\n+  NEXT_ROUTER_STATE_TREE_HEADER,\n   NEXT_IS_PRERENDER_HEADER,\n } from '../client/components/app-router-headers'\n import type {\n@@ -1981,16 +1982,21 @@ export default abstract class Server<\n     isAppPath: boolean,\n     resolvedPathname: string\n   ): void {\n+    const baseVaryHeader = `${RSC_HEADER}, ${NEXT_ROUTER_STATE_TREE_HEADER}, ${NEXT_ROUTER_PREFETCH_HEADER}, ${NEXT_ROUTER_SEGMENT_PREFETCH_HEADER}`\n+    const isRSCRequest = getRequestMeta(req, 'isRSCRequest') ?? false\n+\n     let addedNextUrlToVary = false\n \n     if (isAppPath && this.pathCouldBeIntercepted(resolvedPathname)) {\n       // Interception route responses can vary based on the `Next-URL` header.\n       // We use the Vary header to signal this behavior to the client to properly cache the response.\n-      res.appendHeader('vary', `${NEXT_URL}`)\n+      res.appendHeader('vary', `${baseVaryHeader}, ${NEXT_URL}`)\n       addedNextUrlToVary = true\n+    } else if (isAppPath || isRSCRequest) {\n+      // We don't need to include `Next-URL` in the Vary header for non-interception routes since it won't affect the response.\n+      // We also set this header for pages to avoid caching issues when navigating between pages and app.\n+      res.appendHeader('vary', baseVaryHeader)\n     }\n-    // For other cases such as App Router requests or RSC requests we don't need to set vary header since we already\n-    // have the _rsc query with the unique hash value.\n \n     if (!addedNextUrlToVary) {\n       // Remove `Next-URL` from the request headers we determined it wasn't necessary to include in the Vary header.\ndiff --git a/test/e2e/app-dir/app/index.test.ts b/test/e2e/app-dir/app/index.test.ts\nindex 69f659f97fec6..f42ed3e642307 100644\n--- a/test/e2e/app-dir/app/index.test.ts\n+++ b/test/e2e/app-dir/app/index.test.ts\n@@ -323,6 +323,29 @@ describe('app dir - basic', () => {\n     expect(res.headers.get('Content-Type')).toBe('text/x-component')\n   })\n \n+  it('should return the `vary` header from edge runtime', async () => {\n+    const res = await next.fetch('/dashboard')\n+    expect(res.headers.get('x-edge-runtime')).toBe('1')\n+    expect(res.headers.get('vary')).toBe(\n+      'RSC, Next-Router-State-Tree, Next-Router-Prefetch, Next-Router-Segment-Prefetch'\n+    )\n+  })\n+\n+  if (!isNextDeploy) {\n+    it('should return the `vary` header from pages for flight requests', async () => {\n+      const res = await next.fetch('/', {\n+        headers: {\n+          ['RSC'.toString()]: '1',\n+        },\n+      })\n+      expect(res.headers.get('vary')).toBe(\n+        isNextDeploy\n+          ? 'RSC, Next-Router-State-Tree, Next-Router-Prefetch, Next-Router-Segment-Prefetch'\n+          : 'RSC, Next-Router-State-Tree, Next-Router-Prefetch, Next-Router-Segment-Prefetch, Accept-Encoding'\n+      )\n+    })\n+  }\n+\n   it('should pass props from getServerSideProps in root layout', async () => {\n     const $ = await next.render$('/dashboard')\n     expect($('title').first().text()).toBe('hello world')\ndiff --git a/test/e2e/vary-header/test/index.test.ts b/test/e2e/vary-header/test/index.test.ts\nindex 1fcf82bd0f38c..5919cd711b560 100644\n--- a/test/e2e/vary-header/test/index.test.ts\n+++ b/test/e2e/vary-header/test/index.test.ts\n@@ -12,16 +12,21 @@ describe('Vary Header Tests', () => {\n     expect(res.headers.get('vary')).toContain('Custom-Header')\n   })\n \n-  it('should preserve custom vary header', async () => {\n+  it('should preserve custom vary header and append RSC headers in app route handlers', async () => {\n     const res = await next.fetch('/normal')\n     const varyHeader = res.headers.get('vary')\n \n     // Custom header is preserved\n     expect(varyHeader).toContain('User-Agent')\n     expect(res.headers.get('cache-control')).toBe('s-maxage=3600')\n+\n+    // Next.js internal headers are appended\n+    expect(varyHeader).toContain('RSC')\n+    expect(varyHeader).toContain('Next-Router-State-Tree')\n+    expect(varyHeader).toContain('Next-Router-Prefetch')\n   })\n \n-  it('should preserve middleware vary header', async () => {\n+  it('should preserve middleware vary header in combination with route handlers', async () => {\n     const res = await next.fetch('/normal')\n     const varyHeader = res.headers.get('vary')\n     const customHeader = res.headers.get('my-custom-header')\n@@ -32,5 +37,10 @@ describe('Vary Header Tests', () => {\n     // Both middleware and route handler vary headers are preserved\n     expect(varyHeader).toContain('my-custom-header')\n     expect(varyHeader).toContain('User-Agent')\n+\n+    // Next.js internal headers are still present\n+    expect(varyHeader).toContain('RSC')\n+    expect(varyHeader).toContain('Next-Router-State-Tree')\n+    expect(varyHeader).toContain('Next-Router-Prefetch')\n   })\n })\n"
  },
  "CWE-522": {
    "cve": "CVE-2024-47081",
    "commit_url": "https://github.com/psf/requests/commit/96ba401c1296ab1dda74a2365ef36d88f7d144ef",
    "diff": "diff --git a/src/requests/utils.py b/src/requests/utils.py\nindex 699683e5d9..8a307ca8a0 100644\n--- a/src/requests/utils.py\n+++ b/src/requests/utils.py\n@@ -236,13 +236,7 @@ def get_netrc_auth(url, raise_errors=False):\n             return\n \n         ri = urlparse(url)\n-\n-        # Strip port numbers from netloc. This weird `if...encode`` dance is\n-        # used for Python 3.2, which doesn't support unicode literals.\n-        splitstr = b\":\"\n-        if isinstance(url, str):\n-            splitstr = splitstr.decode(\"ascii\")\n-        host = ri.netloc.split(splitstr)[0]\n+        host = ri.hostname\n \n         try:\n             _netrc = netrc(netrc_path).authenticators(host)\n"
  },
  "CWE-843": {
    "cve": "CVE-2024-6119",
    "commit_url": "https://github.com/openssl/openssl/commit/7dfcee2cd2a63b2c64b9b4b0850be64cb695b0a0",
    "diff": "diff --git a/crypto/x509/v3_utl.c b/crypto/x509/v3_utl.c\nindex 1a18174995196..a09414c972fa8 100644\n--- a/crypto/x509/v3_utl.c\n+++ b/crypto/x509/v3_utl.c\n@@ -916,36 +916,64 @@ static int do_x509_check(X509 *x, const char *chk, size_t chklen,\n             ASN1_STRING *cstr;\n \n             gen = sk_GENERAL_NAME_value(gens, i);\n-            if ((gen->type == GEN_OTHERNAME) && (check_type == GEN_EMAIL)) {\n-                if (OBJ_obj2nid(gen->d.otherName->type_id) ==\n-                    NID_id_on_SmtpUTF8Mailbox) {\n-                    san_present = 1;\n-\n-                    /*\n-                     * If it is not a UTF8String then that is unexpected and we\n-                     * treat it as no match\n-                     */\n-                    if (gen->d.otherName->value->type == V_ASN1_UTF8STRING) {\n-                        cstr = gen->d.otherName->value->value.utf8string;\n-\n-                        /* Positive on success, negative on error! */\n-                        if ((rv = do_check_string(cstr, 0, equal, flags,\n-                                                chk, chklen, peername)) != 0)\n-                            break;\n-                    }\n-                } else\n+            switch (gen->type) {\n+            default:\n+                continue;\n+            case GEN_OTHERNAME:\n+\t\tswitch (OBJ_obj2nid(gen->d.otherName->type_id)) {\n+                default:\n                     continue;\n-            } else {\n-                if ((gen->type != check_type) && (gen->type != GEN_OTHERNAME))\n+                case NID_id_on_SmtpUTF8Mailbox:\n+                    /*-\n+                     * https://datatracker.ietf.org/doc/html/rfc8398#section-3\n+                     *\n+                     *   Due to name constraint compatibility reasons described\n+                     *   in Section 6, SmtpUTF8Mailbox subjectAltName MUST NOT\n+                     *   be used unless the local-part of the email address\n+                     *   contains non-ASCII characters. When the local-part is\n+                     *   ASCII, rfc822Name subjectAltName MUST be used instead\n+                     *   of SmtpUTF8Mailbox. This is compatible with legacy\n+                     *   software that supports only rfc822Name (and not\n+                     *   SmtpUTF8Mailbox). [...]\n+                     *\n+                     *   SmtpUTF8Mailbox is encoded as UTF8String.\n+                     *\n+                     * If it is not a UTF8String then that is unexpected, and\n+                     * we ignore the invalid SAN (neither set san_present nor\n+                     * consider it a candidate for equality).  This does mean\n+                     * that the subject CN may be considered, as would be the\n+                     * case when the malformed SmtpUtf8Mailbox SAN is instead\n+                     * simply absent.\n+                     *\n+                     * When CN-ID matching is not desirable, applications can\n+                     * choose to turn it off, doing so is at this time a best\n+                     * practice.\n+                     */\n+                    if (check_type != GEN_EMAIL\n+                        || gen->d.otherName->value->type != V_ASN1_UTF8STRING)\n+                        continue;\n+                    alt_type = 0;\n+                    cstr = gen->d.otherName->value->value.utf8string;\n+                    break;\n+                }\n+                break;\n+            case GEN_EMAIL:\n+                if (check_type != GEN_EMAIL)\n                     continue;\n-            }\n-            san_present = 1;\n-            if (check_type == GEN_EMAIL)\n                 cstr = gen->d.rfc822Name;\n-            else if (check_type == GEN_DNS)\n+                break;\n+            case GEN_DNS:\n+                if (check_type != GEN_DNS)\n+                    continue;\n                 cstr = gen->d.dNSName;\n-            else\n+                break;\n+            case GEN_IPADD:\n+                if (check_type != GEN_IPADD)\n+                    continue;\n                 cstr = gen->d.iPAddress;\n+                break;\n+            }\n+            san_present = 1;\n             /* Positive on success, negative on error! */\n             if ((rv = do_check_string(cstr, alt_type, equal, flags,\n                                       chk, chklen, peername)) != 0)\ndiff --git a/test/recipes/25-test_eai_data.t b/test/recipes/25-test_eai_data.t\nindex 522982ddfb802..e18735d89aadf 100644\n--- a/test/recipes/25-test_eai_data.t\n+++ b/test/recipes/25-test_eai_data.t\n@@ -21,16 +21,18 @@ setup(\"test_eai_data\");\n #./util/wrap.pl apps/openssl verify -nameopt utf8 -no_check_time -CAfile test/recipes/25-test_eai_data/utf8_chain.pem test/recipes/25-test_eai_data/ascii_leaf.pem\n #./util/wrap.pl apps/openssl verify -nameopt utf8 -no_check_time -CAfile test/recipes/25-test_eai_data/ascii_chain.pem test/recipes/25-test_eai_data/utf8_leaf.pem\n \n-plan tests => 12;\n+plan tests => 16;\n \n require_ok(srctop_file('test','recipes','tconversion.pl'));\n my $folder = \"test/recipes/25-test_eai_data\";\n \n my $ascii_pem = srctop_file($folder, \"ascii_leaf.pem\");\n my $utf8_pem  = srctop_file($folder, \"utf8_leaf.pem\");\n+my $kdc_pem   = srctop_file($folder, \"kdc-cert.pem\");\n \n my $ascii_chain_pem = srctop_file($folder, \"ascii_chain.pem\");\n my $utf8_chain_pem  = srctop_file($folder, \"utf8_chain.pem\");\n+my $kdc_chain_pem  = srctop_file($folder, \"kdc-root-cert.pem\");\n \n my $out;\n my $outcnt = 0;\n@@ -56,10 +58,18 @@ SKIP: {\n \n ok(run(app([\"openssl\", \"verify\", \"-nameopt\", \"utf8\", \"-no_check_time\", \"-CAfile\", $ascii_chain_pem, $ascii_pem])));\n ok(run(app([\"openssl\", \"verify\", \"-nameopt\", \"utf8\", \"-no_check_time\", \"-CAfile\", $utf8_chain_pem, $utf8_pem])));\n+ok(run(app([\"openssl\", \"verify\", \"-nameopt\", \"utf8\", \"-no_check_time\", \"-CAfile\", $kdc_chain_pem, $kdc_pem])));\n \n ok(!run(app([\"openssl\", \"verify\", \"-nameopt\", \"utf8\", \"-no_check_time\", \"-CAfile\", $ascii_chain_pem, $utf8_pem])));\n ok(!run(app([\"openssl\", \"verify\", \"-nameopt\", \"utf8\", \"-no_check_time\", \"-CAfile\", $utf8_chain_pem,  $ascii_pem])));\n \n+# Check an otherName does not get misparsed as an DNS name, (should trigger ASAN errors if violated).\n+ok(run(app([\"openssl\", \"verify\", \"-nameopt\", \"utf8\", \"-no_check_time\", \"-verify_hostname\", 'mx1.example.com', \"-CAfile\", $kdc_chain_pem,  $kdc_pem])));\n+# Check an otherName does not get misparsed as an email address, (should trigger ASAN errors if violated).\n+ok(run(app([\"openssl\", \"verify\", \"-nameopt\", \"utf8\", \"-no_check_time\", \"-verify_email\", 'joe@example.com', \"-CAfile\", $kdc_chain_pem,  $kdc_pem])));\n+# We expect SmtpUTF8Mailbox to be a UTF8 String, not an IA5String.\n+ok(!run(app([\"openssl\", \"verify\", \"-nameopt\", \"utf8\", \"-no_check_time\", \"-verify_email\", 'moe@example.com', \"-CAfile\", $kdc_chain_pem,  $kdc_pem])));\n+\n #Check that we get the expected failure return code\n with({ exit_checker => sub { return shift == 2; } },\n      sub {\ndiff --git a/test/recipes/25-test_eai_data/kdc-cert.pem b/test/recipes/25-test_eai_data/kdc-cert.pem\nnew file mode 100644\nindex 0000000000000..e8a2c6f55d459\n--- /dev/null\n+++ b/test/recipes/25-test_eai_data/kdc-cert.pem\n@@ -0,0 +1,21 @@\n+-----BEGIN CERTIFICATE-----\n+MIIDbDCCAlSgAwIBAgIBAjANBgkqhkiG9w0BAQsFADAPMQ0wCwYDVQQDDARSb290\n+MCAXDTI0MDYyMDA2MTQxNVoYDzIxMjQwNjIwMDYxNDE1WjAXMRUwEwYDVQQDDAxU\n+RVNULkVYQU1QTEUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC6wfP+\n+6go79dkpo/dGLMlPZ7Gw/Q6gUYrCWZWUEgEeRVHCrqOlgUEyA+PcWas/XDPUxXry\n+BQlJHLvlqamAQn8gs4QPBARFYWKNiTVGyaRkgNA1N5gqyZdrP9UE+ZJmdqxRAAe8\n+vvpGZWSgevPhLUiSCFYDiD0Rtji2Hm3rGUrReQFBQDEw2pNGwz9zIaxUs08kQZcx\n+Yzyiplz5Oau+R/6sAgUwDlrD9xOlUxx/tA/MSDIfkK8qioU11uUZtO5VjkNQy/bT\n+7zQMmXxWgm2MIgOs1u4YN7YGOtgqHE9v9iPHHfgrkbQDtVDGQsa8AQEhkUDSCtW9\n+3VFAKx6dGNXYzFwfAgMBAAGjgcgwgcUwHQYDVR0OBBYEFFR5tZycW19DmtbL4Zqj\n+te1c2vZLMAkGA1UdIwQCMAAwCQYDVR0TBAIwADCBjQYDVR0RBIGFMIGCoD8GBisG\n+AQUCAqA1MDOgDhsMVEVTVC5FWEFNUExFoSEwH6ADAgEBoRgwFhsGa3JidGd0GwxU\n+RVNULkVYQU1QTEWgHQYIKwYBBQUHCAmgERYPbW9lQGV4YW1wbGUuY29tgQ9qb2VA\n+ZXhhbXBsZS5jb22CD214MS5leGFtcGxlLmNvbTANBgkqhkiG9w0BAQsFAAOCAQEA\n+T0xzVtVpRtaOzIhgzw7XQUdzWD5UEGSJJ1cBCOmKUWwDLTAouCYLFB4TbEE7MMUb\n+iuMy60bjmVtvfJIXorGUgSadRe5RWJ5DamJWvPA0Q9x7blnEcXqEF+9Td+ypevgU\n+UYHFmg83OYwxOsFXZ5cRuXMk3WCsDHQIBi6D1L6oDDZ2pfArs5mqm3thQKVlqyl1\n+El3XRYEdqAz/5eCOFNfwxF0ALxjxVr/Z50StUZU8I7Zfev6+kHhyrR7dqzYJImv9\n+0fTCOBEMjIETDsrA70OxAMu4V16nrWZdJdvzblS2qrt97Omkj+2kiPAJFB76RpwI\n+oDQ9fKfUOAmUFth2/R/eGA==\n+-----END CERTIFICATE-----\ndiff --git a/test/recipes/25-test_eai_data/kdc-root-cert.pem b/test/recipes/25-test_eai_data/kdc-root-cert.pem\nnew file mode 100644\nindex 0000000000000..a74c96bf31469\n--- /dev/null\n+++ b/test/recipes/25-test_eai_data/kdc-root-cert.pem\n@@ -0,0 +1,16 @@\n+-----BEGIN CERTIFICATE-----\n+MIICnDCCAYQCCQCBswYcrlZSHjANBgkqhkiG9w0BAQsFADAPMQ0wCwYDVQQDDARS\n+b290MCAXDTI0MDYyMDA2MTQxNVoYDzIxMjQwNjIwMDYxNDE1WjAPMQ0wCwYDVQQD\n+DARSb290MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAqRj8S4kBbIUj\n+61kZfi6nE35Q38U140+qt4uAiwAhKumfVHlBM0zQ98WFt5zMHIBQwIb3yjc2zj+0\n+qzUnQfwm1r/RfcMmBPEti9Ge+aEMSsds2gMXziOFM8wd2aAFPy7UVE0XpEWofsRK\n+MGi61MKVdPSbGIxBwY9VW38/7D/wf1HtJe7y0xpuecR7GB2XAs+qST59NjuF+7wS\n+dLM8Hb3TATgeYbXXWsRJgwz+SPzExg5WmLnU+7y4brZ32dHtdSmkRVSgSlaIf7Xj\n+3Tc6Zi7I+W/JYk7hy1zUexVdWCak4PHcoWrXe0gNNN/t8VfLfMExt5z/HIylXnU7\n+pGUyqZlTGQIDAQABMA0GCSqGSIb3DQEBCwUAA4IBAQAHpLF1UCRy7b6Hk0rLokxI\n+lgwiH9BU9mktigAGASvkbllpt+YbUbWnuYAvpHBGiP1qZtfX2r96UrSJaGO9BEzT\n+Gp9ThnSjoj4Srul0+s/NArU22irFLmDzbalgevAmm9gMGkdqkiIm/mXbwrPj0ncl\n+KGicevXryVpvaP62eZ8cc3C4p97frMmXxRX8sTdQpD/gRI7prdEILRSKveqT+AEW\n+7rFGM5AOevb4U8ddop8A3D/kX0wcCAIBF6jCNk3uEJ57jVcagL04kPnVfdRiedTS\n+vfq1DRNcD29d1H/9u0fHdSn1/+8Ep3X+afQ3C6//5NvOEaXcIGO4QSwkprQydfv8\n+-----END CERTIFICATE-----\ndiff --git a/test/recipes/25-test_eai_data/kdc.sh b/test/recipes/25-test_eai_data/kdc.sh\nnew file mode 100755\nindex 0000000000000..7a8dbc719fb71\n--- /dev/null\n+++ b/test/recipes/25-test_eai_data/kdc.sh\n@@ -0,0 +1,41 @@\n+#! /usr/bin/env bash\n+\n+# Create a root CA, signing a leaf cert with a KDC principal otherName SAN, and\n+# also a non-UTF8 smtpUtf8Mailbox SAN followed by an rfc822Name SAN and a DNS\n+# name SAN.  In the vulnerable EAI code, the KDC principal `otherName` should\n+# trigger ASAN errors in DNS name checks, while the non-UTF8 `smtpUtf8Mailbox`\n+# should likewise lead to ASAN issues with email name checks.\n+\n+rm -f root-key.pem root-cert.pem\n+openssl req -nodes -new -newkey rsa:2048 -keyout kdc-root-key.pem \\\n+        -x509 -subj /CN=Root -days 36524 -out kdc-root-cert.pem\n+\n+exts=$(\n+    printf \"%s\\n%s\\n%s\\n%s = \" \\\n+        \"subjectKeyIdentifier = hash\" \\\n+        \"authorityKeyIdentifier = keyid\" \\\n+        \"basicConstraints = CA:false\" \\\n+        \"subjectAltName\"\n+    printf \"%s, \" \"otherName:1.3.6.1.5.2.2;SEQUENCE:kdc_princ_name\"\n+    printf \"%s, \" \"otherName:1.3.6.1.5.5.7.8.9;IA5:moe@example.com\"\n+    printf \"%s, \" \"email:joe@example.com\"\n+    printf \"%s\\n\" \"DNS:mx1.example.com\"\n+    printf \"[kdc_princ_name]\\n\"\n+    printf \"realm = EXP:0, GeneralString:TEST.EXAMPLE\\n\"\n+    printf \"principal_name = EXP:1, SEQUENCE:kdc_principal_seq\\n\"\n+    printf \"[kdc_principal_seq]\\n\"\n+    printf \"name_type = EXP:0, INTEGER:1\\n\"\n+    printf \"name_string = EXP:1, SEQUENCE:kdc_principal_components\\n\"\n+    printf \"[kdc_principal_components]\\n\"\n+    printf \"princ1 = GeneralString:krbtgt\\n\"\n+    printf \"princ2 = GeneralString:TEST.EXAMPLE\\n\"\n+    )\n+\n+printf \"%s\\n\" \"$exts\"\n+\n+openssl req -nodes -new -newkey rsa:2048 -keyout kdc-key.pem \\\n+    -subj \"/CN=TEST.EXAMPLE\" |\n+    openssl x509 -req -out kdc-cert.pem \\\n+        -CA \"kdc-root-cert.pem\" -CAkey \"kdc-root-key.pem\" \\\n+        -set_serial 2 -days 36524 \\\n+        -extfile <(printf \"%s\\n\" \"$exts\")\n"
  },
  "CWE-835": {
    "cve": "CVE-2025-48879",
    "commit_url": "https://github.com/OctoPrint/OctoPrint/commit/c9c35c17bd820f19c6b12e6c0359fc0cfdd0c1ec",
    "diff": "diff --git a/src/octoprint/server/util/tornado.py b/src/octoprint/server/util/tornado.py\nindex cf20cb5473..fc5bf96c92 100644\n--- a/src/octoprint/server/util/tornado.py\n+++ b/src/octoprint/server/util/tornado.py\n@@ -627,9 +627,19 @@ async def _handle_method(self, *args, **kwargs):\n         body = b\"\"\n         if self.is_multipart():\n             # make sure we really processed all data in the buffer\n-            while len(self._buffer):\n+            buffer_len = len(self._buffer)\n+            while buffer_len:\n+                self._logger.debug(\"waiting for buffer to empty...\")\n                 self._process_multipart_data(self._buffer)\n \n+                if len(self._buffer) == buffer_len:\n+                    # no change between iterations, something fishy is going on, abort, abort!\n+                    raise tornado.web.HTTPError(\n+                        400,\n+                        log_message=\"Invalid multipart/form-data: no final boundary found\",\n+                    )\n+                buffer_len = len(self._buffer)\n+\n             # use rewritten body\n             body = self._new_body\n \n"
  },
  "CWE-824": {
    "cve": "CVE-2023-4508",
    "commit_url": "https://github.com/gerbv/gerbv/commit/5517e22250e935dc7f86f64ad414aeae3dbcb36a",
    "diff": "diff --git a/src/gerb_file.c b/src/gerb_file.c\nindex 9a9735cd..dbc2da17 100644\n--- a/src/gerb_file.c\n+++ b/src/gerb_file.c\n@@ -131,6 +131,9 @@ gerb_fopen(char const * filename)\n \n #endif\n \n+    dprintf(\"     Setting filename\\n\");\n+    fd->filename = g_strdup(filename);\n+\n     dprintf(\"<--- Leaving gerb_fopen\\n\");\n     return fd;\n } /* gerb_fopen */\n@@ -238,6 +241,8 @@ void\n gerb_fclose(gerb_file_t *fd)\n {\n     if (fd) {\n+        g_free(fd->filename);\n+\n #ifdef HAVE_SYS_MMAN_H\n \tif (munmap(fd->data, fd->datalen) < 0)\n \t    GERB_FATAL_ERROR(\"munmap: %s\", strerror(errno));\ndiff --git a/src/gerber.c b/src/gerber.c\nindex 50cf01cc..1a608abe 100644\n--- a/src/gerber.c\n+++ b/src/gerber.c\n@@ -1476,7 +1476,7 @@ parse_rs274x(gint levelOfRecursion, gerb_file_t *fd, gerbv_image_t *image,\n \t\t\t\t\"include file recursion which is not allowed \"\n \t\t\t\t\"by the RS-274X spec\"));\n \t\t}\n-\t\t\n+\t\tg_free (includeFilename);\n \t    }\n \t}\n \tbreak;\ndiff --git a/src/gerbv.c b/src/gerbv.c\nindex 579d0f88..db10cc92 100644\n--- a/src/gerbv.c\n+++ b/src/gerbv.c\n@@ -507,9 +507,6 @@ gerbv_open_image(gerbv_project_t *gerbvProject, gchar const* filename, int idx,\n \treturn -1;\n     }\n \n-    /* Store filename info fd for further use */\n-    fd->filename = g_strdup(filename);\n-    \n     dprintf(\"In open_image, successfully opened file.  Now check its type....\\n\");\n     /* Here's where we decide what file type we have */\n     /* Note: if the file has some invalid characters in it but still appears to\n@@ -578,7 +575,6 @@ gerbv_open_image(gerbv_project_t *gerbvProject, gchar const* filename, int idx,\n \tparsed_image = NULL;\n     }\n     \n-    g_free(fd->filename);\n     gerb_fclose(fd);\n     if (parsed_image == NULL) {\n \treturn -1;\ndiff --git a/test/golden/parse_aperture_strtok.png b/test/golden/parse_aperture_strtok.png\nnew file mode 100644\nindex 00000000..9015ba79\nBinary files /dev/null and b/test/golden/parse_aperture_strtok.png differ\ndiff --git a/test/inputs/parse_aperture_strtok.1.poc b/test/inputs/parse_aperture_strtok.1.poc\nnew file mode 100644\nindex 00000000..2dcfae5b\n--- /dev/null\n+++ b/test/inputs/parse_aperture_strtok.1.poc\n@@ -0,0 +1,11 @@\n+%FSLAX25Y25*%\n+%MOIN*%\n+\n+%ADD20C,BBBB*%\n+\n+%IFparse_aperture_strtok.2.poc*%\n+\n+D21*\n+X400000Y400000D03*\n+\n+M02*\ndiff --git a/test/inputs/parse_aperture_strtok.2.poc b/test/inputs/parse_aperture_strtok.2.poc\nnew file mode 100644\nindex 00000000..fec5092f\n--- /dev/null\n+++ b/test/inputs/parse_aperture_strtok.2.poc\n@@ -0,0 +1,2 @@\n+M09*\n+\ndiff --git a/test/run_valgrind_tests.sh b/test/run_valgrind_tests.sh\nindex 5b1b052c..0c6fa7c3 100755\n--- a/test/run_valgrind_tests.sh\n+++ b/test/run_valgrind_tests.sh\n@@ -2,4 +2,5 @@\n ./run_tests.sh --valgrind \\\n                example_cslk \\\n                out-of-bounds-drill-tool \\\n-               example_am_test\n+               example_am_test \\\n+               parse_aperture_strtok\ndiff --git a/test/tests.list b/test/tests.list\nindex 64dfa684..3c687773 100644\n--- a/test/tests.list\n+++ b/test/tests.list\n@@ -182,6 +182,8 @@ test-circular-interpolation-zero-error  |  test-circular-interpolation-zero-erro\n test-merge-a+b_temporary | test-merge-a.gbx test-merge-b.gbx | ! --export=rs274x --output=inputs/test-merge-a+b_temporary.gbx\n test-merge-a+b | test-merge-a+b_temporary.gbx\n \n+parse_aperture_strtok | parse_aperture_strtok.1.poc\n+\n # ---------------------------------------------\n # Excellon drill tests\n # ---------------------------------------------\n"
  },
  "CWE-345": {
    "cve": "CVE-2023-42816",
    "commit_url": "https://github.com/kyverno/kyverno/commit/80d139bb5d1d9d7e907abe851b97dc73821a5be2",
    "diff": "diff --git a/api/kyverno/v1/image_verification_types.go b/api/kyverno/v1/image_verification_types.go\nindex 4f281acdee5c..b30d0dfe76f8 100644\n--- a/api/kyverno/v1/image_verification_types.go\n+++ b/api/kyverno/v1/image_verification_types.go\n@@ -9,13 +9,13 @@ import (\n )\n \n // ImageVerificationType selects the type of verification algorithm\n-// +kubebuilder:validation:Enum=Cosign;NotaryV2\n+// +kubebuilder:validation:Enum=Cosign;Notary\n // +kubebuilder:default=Cosign\n type ImageVerificationType string\n \n const (\n-\tCosign   ImageVerificationType = \"Cosign\"\n-\tNotaryV2 ImageVerificationType = \"NotaryV2\"\n+\tCosign ImageVerificationType = \"Cosign\"\n+\tNotary ImageVerificationType = \"Notary\"\n )\n \n // ImageVerification validates that images that match the specified pattern\n@@ -23,7 +23,7 @@ const (\n // mutated to include the SHA digest retrieved during the registration.\n type ImageVerification struct {\n \t// Type specifies the method of signature validation. The allowed options\n-\t// are Cosign and NotaryV2. By default Cosign is used if a type is not specified.\n+\t// are Cosign and Notary. By default Cosign is used if a type is not specified.\n \t// +kubebuilder:validation:Optional\n \tType ImageVerificationType `json:\"type,omitempty\" yaml:\"type,omitempty\"`\n \n@@ -236,9 +236,14 @@ type CTLog struct {\n // OCI registry and decodes them into a list of Statements.\n type Attestation struct {\n \t// PredicateType defines the type of Predicate contained within the Statement.\n-\t// +kubebuilder:validation:Required\n+\t// Deprecated in favour of 'Type', to be removed soon\n+\t// +kubebuilder:validation:Optional\n \tPredicateType string `json:\"predicateType\" yaml:\"predicateType\"`\n \n+\t// Type defines the type of attestation contained within the Statement.\n+\t// +kubebuilder:validation:Optional\n+\tType string `json:\"type\" yaml:\"type\"`\n+\n \t// Attestors specify the required attestors (i.e. authorities)\n \t// +kubebuilder:validation:Optional\n \tAttestors []AttestorSet `json:\"attestors\" yaml:\"attestors\"`\n@@ -281,6 +286,19 @@ func (iv *ImageVerification) Validate(isAuditFailureAction bool, path *field.Pat\n \t\terrs = append(errs, attestorErrors...)\n \t}\n \n+\tif iv.Type == Notary {\n+\t\tfor _, attestorSet := range iv.Attestors {\n+\t\t\tfor _, attestor := range attestorSet.Entries {\n+\t\t\t\tif attestor.Keyless != nil {\n+\t\t\t\t\terrs = append(errs, field.Invalid(attestorsPath, iv, \"Keyless field is not allowed for type notary\"))\n+\t\t\t\t}\n+\t\t\t\tif attestor.Keys != nil {\n+\t\t\t\t\terrs = append(errs, field.Invalid(attestorsPath, iv, \"Keys field is not allowed for type notary\"))\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n \treturn errs\n }\n \ndiff --git a/api/kyverno/v2beta1/image_verification_types.go b/api/kyverno/v2beta1/image_verification_types.go\nindex 8485ccf15da6..a2577ac234fd 100644\n--- a/api/kyverno/v2beta1/image_verification_types.go\n+++ b/api/kyverno/v2beta1/image_verification_types.go\n@@ -6,13 +6,13 @@ import (\n )\n \n // ImageVerificationType selects the type of verification algorithm\n-// +kubebuilder:validation:Enum=Cosign;NotaryV2\n+// +kubebuilder:validation:Enum=Cosign;Notary\n // +kubebuilder:default=Cosign\n type ImageVerificationType string\n \n const (\n-\tCosign   ImageVerificationType = \"Cosign\"\n-\tNotaryV2 ImageVerificationType = \"NotaryV2\"\n+\tCosign ImageVerificationType = \"Cosign\"\n+\tNotary ImageVerificationType = \"Notary\"\n )\n \n // ImageVerification validates that images that match the specified pattern\n@@ -20,7 +20,7 @@ const (\n // mutated to include the SHA digest retrieved during the registration.\n type ImageVerification struct {\n \t// Type specifies the method of signature validation. The allowed options\n-\t// are Cosign and NotaryV2. By default Cosign is used if a type is not specified.\n+\t// are Cosign and Notary. By default Cosign is used if a type is not specified.\n \t// +kubebuilder:validation:Optional\n \tType ImageVerificationType `json:\"type,omitempty\" yaml:\"type,omitempty\"`\n \n@@ -86,5 +86,18 @@ func (iv *ImageVerification) Validate(isAuditFailureAction bool, path *field.Pat\n \t\terrs = append(errs, attestorErrors...)\n \t}\n \n+\tif iv.Type == Notary {\n+\t\tfor _, attestorSet := range iv.Attestors {\n+\t\t\tfor _, attestor := range attestorSet.Entries {\n+\t\t\t\tif attestor.Keyless != nil {\n+\t\t\t\t\terrs = append(errs, field.Invalid(attestorsPath, iv, \"Keyless field is not allowed for type notary\"))\n+\t\t\t\t}\n+\t\t\t\tif attestor.Keys != nil {\n+\t\t\t\t\terrs = append(errs, field.Invalid(attestorsPath, iv, \"Keys field is not allowed for type notary\"))\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n \treturn errs\n }\ndiff --git a/charts/kyverno/templates/crds/crds.yaml b/charts/kyverno/templates/crds/crds.yaml\nindex 0ba7ecab21e8..05ca055a608f 100644\n--- a/charts/kyverno/templates/crds/crds.yaml\n+++ b/charts/kyverno/templates/crds/crds.yaml\n@@ -7248,10 +7248,13 @@ spec:\n                                   type: array\n                                 predicateType:\n                                   description: PredicateType defines the type of Predicate\n+                                    contained within the Statement. Deprecated in\n+                                    favour of 'Type', to be removed soon\n+                                  type: string\n+                                type:\n+                                  description: Type defines the type of attestation\n                                     contained within the Statement.\n                                   type: string\n-                              required:\n-                              - predicateType\n                               type: object\n                             type: array\n                           attestors:\n@@ -7499,11 +7502,11 @@ spec:\n                             type: string\n                           type:\n                             description: Type specifies the method of signature validation.\n-                              The allowed options are Cosign and NotaryV2. By default\n+                              The allowed options are Cosign and Notary. By default\n                               Cosign is used if a type is not specified.\n                             enum:\n                             - Cosign\n-                            - NotaryV2\n+                            - Notary\n                             type: string\n                           verifyDigest:\n                             default: true\n@@ -11153,9 +11156,13 @@ spec:\n                                     predicateType:\n                                       description: PredicateType defines the type\n                                         of Predicate contained within the Statement.\n+                                        Deprecated in favour of 'Type', to be removed\n+                                        soon\n+                                      type: string\n+                                    type:\n+                                      description: Type defines the type of attestation\n+                                        contained within the Statement.\n                                       type: string\n-                                  required:\n-                                  - predicateType\n                                   type: object\n                                 type: array\n                               attestors:\n@@ -11412,11 +11419,11 @@ spec:\n                                 type: string\n                               type:\n                                 description: Type specifies the method of signature\n-                                  validation. The allowed options are Cosign and NotaryV2.\n+                                  validation. The allowed options are Cosign and Notary.\n                                   By default Cosign is used if a type is not specified.\n                                 enum:\n                                 - Cosign\n-                                - NotaryV2\n+                                - Notary\n                                 type: string\n                               verifyDigest:\n                                 default: true\n@@ -14748,10 +14755,13 @@ spec:\n                                   type: array\n                                 predicateType:\n                                   description: PredicateType defines the type of Predicate\n+                                    contained within the Statement. Deprecated in\n+                                    favour of 'Type', to be removed soon\n+                                  type: string\n+                                type:\n+                                  description: Type defines the type of attestation\n                                     contained within the Statement.\n                                   type: string\n-                              required:\n-                              - predicateType\n                               type: object\n                             type: array\n                           attestors:\n@@ -14974,11 +14984,11 @@ spec:\n                             type: boolean\n                           type:\n                             description: Type specifies the method of signature validation.\n-                              The allowed options are Cosign and NotaryV2. By default\n+                              The allowed options are Cosign and Notary. By default\n                               Cosign is used if a type is not specified.\n                             enum:\n                             - Cosign\n-                            - NotaryV2\n+                            - Notary\n                             type: string\n                           verifyDigest:\n                             default: true\n@@ -18628,9 +18638,13 @@ spec:\n                                     predicateType:\n                                       description: PredicateType defines the type\n                                         of Predicate contained within the Statement.\n+                                        Deprecated in favour of 'Type', to be removed\n+                                        soon\n+                                      type: string\n+                                    type:\n+                                      description: Type defines the type of attestation\n+                                        contained within the Statement.\n                                       type: string\n-                                  required:\n-                                  - predicateType\n                                   type: object\n                                 type: array\n                               attestors:\n@@ -18887,11 +18901,11 @@ spec:\n                                 type: string\n                               type:\n                                 description: Type specifies the method of signature\n-                                  validation. The allowed options are Cosign and NotaryV2.\n+                                  validation. The allowed options are Cosign and Notary.\n                                   By default Cosign is used if a type is not specified.\n                                 enum:\n                                 - Cosign\n-                                - NotaryV2\n+                                - Notary\n                                 type: string\n                               verifyDigest:\n                                 default: true\n@@ -22509,10 +22523,13 @@ spec:\n                                   type: array\n                                 predicateType:\n                                   description: PredicateType defines the type of Predicate\n+                                    contained within the Statement. Deprecated in\n+                                    favour of 'Type', to be removed soon\n+                                  type: string\n+                                type:\n+                                  description: Type defines the type of attestation\n                                     contained within the Statement.\n                                   type: string\n-                              required:\n-                              - predicateType\n                               type: object\n                             type: array\n                           attestors:\n@@ -22760,11 +22777,11 @@ spec:\n                             type: string\n                           type:\n                             description: Type specifies the method of signature validation.\n-                              The allowed options are Cosign and NotaryV2. By default\n+                              The allowed options are Cosign and Notary. By default\n                               Cosign is used if a type is not specified.\n                             enum:\n                             - Cosign\n-                            - NotaryV2\n+                            - Notary\n                             type: string\n                           verifyDigest:\n                             default: true\n@@ -26415,9 +26432,13 @@ spec:\n                                     predicateType:\n                                       description: PredicateType defines the type\n                                         of Predicate contained within the Statement.\n+                                        Deprecated in favour of 'Type', to be removed\n+                                        soon\n+                                      type: string\n+                                    type:\n+                                      description: Type defines the type of attestation\n+                                        contained within the Statement.\n                                       type: string\n-                                  required:\n-                                  - predicateType\n                                   type: object\n                                 type: array\n                               attestors:\n@@ -26674,11 +26695,11 @@ spec:\n                                 type: string\n                               type:\n                                 description: Type specifies the method of signature\n-                                  validation. The allowed options are Cosign and NotaryV2.\n+                                  validation. The allowed options are Cosign and Notary.\n                                   By default Cosign is used if a type is not specified.\n                                 enum:\n                                 - Cosign\n-                                - NotaryV2\n+                                - Notary\n                                 type: string\n                               verifyDigest:\n                                 default: true\n@@ -30011,10 +30032,13 @@ spec:\n                                   type: array\n                                 predicateType:\n                                   description: PredicateType defines the type of Predicate\n+                                    contained within the Statement. Deprecated in\n+                                    favour of 'Type', to be removed soon\n+                                  type: string\n+                                type:\n+                                  description: Type defines the type of attestation\n                                     contained within the Statement.\n                                   type: string\n-                              required:\n-                              - predicateType\n                               type: object\n                             type: array\n                           attestors:\n@@ -30237,11 +30261,11 @@ spec:\n                             type: boolean\n                           type:\n                             description: Type specifies the method of signature validation.\n-                              The allowed options are Cosign and NotaryV2. By default\n+                              The allowed options are Cosign and Notary. By default\n                               Cosign is used if a type is not specified.\n                             enum:\n                             - Cosign\n-                            - NotaryV2\n+                            - Notary\n                             type: string\n                           verifyDigest:\n                             default: true\n@@ -33891,9 +33915,13 @@ spec:\n                                     predicateType:\n                                       description: PredicateType defines the type\n                                         of Predicate contained within the Statement.\n+                                        Deprecated in favour of 'Type', to be removed\n+                                        soon\n+                                      type: string\n+                                    type:\n+                                      description: Type defines the type of attestation\n+                                        contained within the Statement.\n                                       type: string\n-                                  required:\n-                                  - predicateType\n                                   type: object\n                                 type: array\n                               attestors:\n@@ -34150,11 +34178,11 @@ spec:\n                                 type: string\n                               type:\n                                 description: Type specifies the method of signature\n-                                  validation. The allowed options are Cosign and NotaryV2.\n+                                  validation. The allowed options are Cosign and Notary.\n                                   By default Cosign is used if a type is not specified.\n                                 enum:\n                                 - Cosign\n-                                - NotaryV2\n+                                - Notary\n                                 type: string\n                               verifyDigest:\n                                 default: true\ndiff --git a/config/crds/kyverno.io_clusterpolicies.yaml b/config/crds/kyverno.io_clusterpolicies.yaml\nindex 999640c4519f..b374e3b35513 100644\n--- a/config/crds/kyverno.io_clusterpolicies.yaml\n+++ b/config/crds/kyverno.io_clusterpolicies.yaml\n@@ -3494,10 +3494,13 @@ spec:\n                                   type: array\n                                 predicateType:\n                                   description: PredicateType defines the type of Predicate\n+                                    contained within the Statement. Deprecated in\n+                                    favour of 'Type', to be removed soon\n+                                  type: string\n+                                type:\n+                                  description: Type defines the type of attestation\n                                     contained within the Statement.\n                                   type: string\n-                              required:\n-                              - predicateType\n                               type: object\n                             type: array\n                           attestors:\n@@ -3745,11 +3748,11 @@ spec:\n                             type: string\n                           type:\n                             description: Type specifies the method of signature validation.\n-                              The allowed options are Cosign and NotaryV2. By default\n+                              The allowed options are Cosign and Notary. By default\n                               Cosign is used if a type is not specified.\n                             enum:\n                             - Cosign\n-                            - NotaryV2\n+                            - Notary\n                             type: string\n                           verifyDigest:\n                             default: true\n@@ -7399,9 +7402,13 @@ spec:\n                                     predicateType:\n                                       description: PredicateType defines the type\n                                         of Predicate contained within the Statement.\n+                                        Deprecated in favour of 'Type', to be removed\n+                                        soon\n+                                      type: string\n+                                    type:\n+                                      description: Type defines the type of attestation\n+                                        contained within the Statement.\n                                       type: string\n-                                  required:\n-                                  - predicateType\n                                   type: object\n                                 type: array\n                               attestors:\n@@ -7658,11 +7665,11 @@ spec:\n                                 type: string\n                               type:\n                                 description: Type specifies the method of signature\n-                                  validation. The allowed options are Cosign and NotaryV2.\n+                                  validation. The allowed options are Cosign and Notary.\n                                   By default Cosign is used if a type is not specified.\n                                 enum:\n                                 - Cosign\n-                                - NotaryV2\n+                                - Notary\n                                 type: string\n                               verifyDigest:\n                                 default: true\n@@ -10994,10 +11001,13 @@ spec:\n                                   type: array\n                                 predicateType:\n                                   description: PredicateType defines the type of Predicate\n+                                    contained within the Statement. Deprecated in\n+                                    favour of 'Type', to be removed soon\n+                                  type: string\n+                                type:\n+                                  description: Type defines the type of attestation\n                                     contained within the Statement.\n                                   type: string\n-                              required:\n-                              - predicateType\n                               type: object\n                             type: array\n                           attestors:\n@@ -11220,11 +11230,11 @@ spec:\n                             type: boolean\n                           type:\n                             description: Type specifies the method of signature validation.\n-                              The allowed options are Cosign and NotaryV2. By default\n+                              The allowed options are Cosign and Notary. By default\n                               Cosign is used if a type is not specified.\n                             enum:\n                             - Cosign\n-                            - NotaryV2\n+                            - Notary\n                             type: string\n                           verifyDigest:\n                             default: true\n@@ -14874,9 +14884,13 @@ spec:\n                                     predicateType:\n                                       description: PredicateType defines the type\n                                         of Predicate contained within the Statement.\n+                                        Deprecated in favour of 'Type', to be removed\n+                                        soon\n+                                      type: string\n+                                    type:\n+                                      description: Type defines the type of attestation\n+                                        contained within the Statement.\n                                       type: string\n-                                  required:\n-                                  - predicateType\n                                   type: object\n                                 type: array\n                               attestors:\n@@ -15133,11 +15147,11 @@ spec:\n                                 type: string\n                               type:\n                                 description: Type specifies the method of signature\n-                                  validation. The allowed options are Cosign and NotaryV2.\n+                                  validation. The allowed options are Cosign and Notary.\n                                   By default Cosign is used if a type is not specified.\n                                 enum:\n                                 - Cosign\n-                                - NotaryV2\n+                                - Notary\n                                 type: string\n                               verifyDigest:\n                                 default: true\ndiff --git a/config/crds/kyverno.io_policies.yaml b/config/crds/kyverno.io_policies.yaml\nindex 764c0e3bcf87..4b0446f5d5f9 100644\n--- a/config/crds/kyverno.io_policies.yaml\n+++ b/config/crds/kyverno.io_policies.yaml\n@@ -3495,10 +3495,13 @@ spec:\n                                   type: array\n                                 predicateType:\n                                   description: PredicateType defines the type of Predicate\n+                                    contained within the Statement. Deprecated in\n+                                    favour of 'Type', to be removed soon\n+                                  type: string\n+                                type:\n+                                  description: Type defines the type of attestation\n                                     contained within the Statement.\n                                   type: string\n-                              required:\n-                              - predicateType\n                               type: object\n                             type: array\n                           attestors:\n@@ -3746,11 +3749,11 @@ spec:\n                             type: string\n                           type:\n                             description: Type specifies the method of signature validation.\n-                              The allowed options are Cosign and NotaryV2. By default\n+                              The allowed options are Cosign and Notary. By default\n                               Cosign is used if a type is not specified.\n                             enum:\n                             - Cosign\n-                            - NotaryV2\n+                            - Notary\n                             type: string\n                           verifyDigest:\n                             default: true\n@@ -7401,9 +7404,13 @@ spec:\n                                     predicateType:\n                                       description: PredicateType defines the type\n                                         of Predicate contained within the Statement.\n+                                        Deprecated in favour of 'Type', to be removed\n+                                        soon\n+                                      type: string\n+                                    type:\n+                                      description: Type defines the type of attestation\n+                                        contained within the Statement.\n                                       type: string\n-                                  required:\n-                                  - predicateType\n                                   type: object\n                                 type: array\n                               attestors:\n@@ -7660,11 +7667,11 @@ spec:\n                                 type: string\n                               type:\n                                 description: Type specifies the method of signature\n-                                  validation. The allowed options are Cosign and NotaryV2.\n+                                  validation. The allowed options are Cosign and Notary.\n                                   By default Cosign is used if a type is not specified.\n                                 enum:\n                                 - Cosign\n-                                - NotaryV2\n+                                - Notary\n                                 type: string\n                               verifyDigest:\n                                 default: true\n@@ -10997,10 +11004,13 @@ spec:\n                                   type: array\n                                 predicateType:\n                                   description: PredicateType defines the type of Predicate\n+                                    contained within the Statement. Deprecated in\n+                                    favour of 'Type', to be removed soon\n+                                  type: string\n+                                type:\n+                                  description: Type defines the type of attestation\n                                     contained within the Statement.\n                                   type: string\n-                              required:\n-                              - predicateType\n                               type: object\n                             type: array\n                           attestors:\n@@ -11223,11 +11233,11 @@ spec:\n                             type: boolean\n                           type:\n                             description: Type specifies the method of signature validation.\n-                              The allowed options are Cosign and NotaryV2. By default\n+                              The allowed options are Cosign and Notary. By default\n                               Cosign is used if a type is not specified.\n                             enum:\n                             - Cosign\n-                            - NotaryV2\n+                            - Notary\n                             type: string\n                           verifyDigest:\n                             default: true\n@@ -14877,9 +14887,13 @@ spec:\n                                     predicateType:\n                                       description: PredicateType defines the type\n                                         of Predicate contained within the Statement.\n+                                        Deprecated in favour of 'Type', to be removed\n+                                        soon\n+                                      type: string\n+                                    type:\n+                                      description: Type defines the type of attestation\n+                                        contained within the Statement.\n                                       type: string\n-                                  required:\n-                                  - predicateType\n                                   type: object\n                                 type: array\n                               attestors:\n@@ -15136,11 +15150,11 @@ spec:\n                                 type: string\n                               type:\n                                 description: Type specifies the method of signature\n-                                  validation. The allowed options are Cosign and NotaryV2.\n+                                  validation. The allowed options are Cosign and Notary.\n                                   By default Cosign is used if a type is not specified.\n                                 enum:\n                                 - Cosign\n-                                - NotaryV2\n+                                - Notary\n                                 type: string\n                               verifyDigest:\n                                 default: true\ndiff --git a/config/install-latest-testing.yaml b/config/install-latest-testing.yaml\nindex 88b47ddc3226..b95ed2373407 100644\n--- a/config/install-latest-testing.yaml\n+++ b/config/install-latest-testing.yaml\n@@ -7451,10 +7451,13 @@ spec:\n                                   type: array\n                                 predicateType:\n                                   description: PredicateType defines the type of Predicate\n+                                    contained within the Statement. Deprecated in\n+                                    favour of 'Type', to be removed soon\n+                                  type: string\n+                                type:\n+                                  description: Type defines the type of attestation\n                                     contained within the Statement.\n                                   type: string\n-                              required:\n-                              - predicateType\n                               type: object\n                             type: array\n                           attestors:\n@@ -7702,11 +7705,11 @@ spec:\n                             type: string\n                           type:\n                             description: Type specifies the method of signature validation.\n-                              The allowed options are Cosign and NotaryV2. By default\n+                              The allowed options are Cosign and Notary. By default\n                               Cosign is used if a type is not specified.\n                             enum:\n                             - Cosign\n-                            - NotaryV2\n+                            - Notary\n                             type: string\n                           verifyDigest:\n                             default: true\n@@ -11356,9 +11359,13 @@ spec:\n                                     predicateType:\n                                       description: PredicateType defines the type\n                                         of Predicate contained within the Statement.\n+                                        Deprecated in favour of 'Type', to be removed\n+                                        soon\n+                                      type: string\n+                                    type:\n+                                      description: Type defines the type of attestation\n+                                        contained within the Statement.\n                                       type: string\n-                                  required:\n-                                  - predicateType\n                                   type: object\n                                 type: array\n                               attestors:\n@@ -11615,11 +11622,11 @@ spec:\n                                 type: string\n                               type:\n                                 description: Type specifies the method of signature\n-                                  validation. The allowed options are Cosign and NotaryV2.\n+                                  validation. The allowed options are Cosign and Notary.\n                                   By default Cosign is used if a type is not specified.\n                                 enum:\n                                 - Cosign\n-                                - NotaryV2\n+                                - Notary\n                                 type: string\n                               verifyDigest:\n                                 default: true\n@@ -14951,10 +14958,13 @@ spec:\n                                   type: array\n                                 predicateType:\n                                   description: PredicateType defines the type of Predicate\n+                                    contained within the Statement. Deprecated in\n+                                    favour of 'Type', to be removed soon\n+                                  type: string\n+                                type:\n+                                  description: Type defines the type of attestation\n                                     contained within the Statement.\n                                   type: string\n-                              required:\n-                              - predicateType\n                               type: object\n                             type: array\n                           attestors:\n@@ -15177,11 +15187,11 @@ spec:\n                             type: boolean\n                           type:\n                             description: Type specifies the method of signature validation.\n-                              The allowed options are Cosign and NotaryV2. By default\n+                              The allowed options are Cosign and Notary. By default\n                               Cosign is used if a type is not specified.\n                             enum:\n                             - Cosign\n-                            - NotaryV2\n+                            - Notary\n                             type: string\n                           verifyDigest:\n                             default: true\n@@ -18831,9 +18841,13 @@ spec:\n                                     predicateType:\n                                       description: PredicateType defines the type\n                                         of Predicate contained within the Statement.\n+                                        Deprecated in favour of 'Type', to be removed\n+                                        soon\n+                                      type: string\n+                                    type:\n+                                      description: Type defines the type of attestation\n+                                        contained within the Statement.\n                                       type: string\n-                                  required:\n-                                  - predicateType\n                                   type: object\n                                 type: array\n                               attestors:\n@@ -19090,11 +19104,11 @@ spec:\n                                 type: string\n                               type:\n                                 description: Type specifies the method of signature\n-                                  validation. The allowed options are Cosign and NotaryV2.\n+                                  validation. The allowed options are Cosign and Notary.\n                                   By default Cosign is used if a type is not specified.\n                                 enum:\n                                 - Cosign\n-                                - NotaryV2\n+                                - Notary\n                                 type: string\n                               verifyDigest:\n                                 default: true\n@@ -22712,10 +22726,13 @@ spec:\n                                   type: array\n                                 predicateType:\n                                   description: PredicateType defines the type of Predicate\n+                                    contained within the Statement. Deprecated in\n+                                    favour of 'Type', to be removed soon\n+                                  type: string\n+                                type:\n+                                  description: Type defines the type of attestation\n                                     contained within the Statement.\n                                   type: string\n-                              required:\n-                              - predicateType\n                               type: object\n                             type: array\n                           attestors:\n@@ -22963,11 +22980,11 @@ spec:\n                             type: string\n                           type:\n                             description: Type specifies the method of signature validation.\n-                              The allowed options are Cosign and NotaryV2. By default\n+                              The allowed options are Cosign and Notary. By default\n                               Cosign is used if a type is not specified.\n                             enum:\n                             - Cosign\n-                            - NotaryV2\n+                            - Notary\n                             type: string\n                           verifyDigest:\n                             default: true\n@@ -26618,9 +26635,13 @@ spec:\n                                     predicateType:\n                                       description: PredicateType defines the type\n                                         of Predicate contained within the Statement.\n+                                        Deprecated in favour of 'Type', to be removed\n+                                        soon\n+                                      type: string\n+                                    type:\n+                                      description: Type defines the type of attestation\n+                                        contained within the Statement.\n                                       type: string\n-                                  required:\n-                                  - predicateType\n                                   type: object\n                                 type: array\n                               attestors:\n@@ -26877,11 +26898,11 @@ spec:\n                                 type: string\n                               type:\n                                 description: Type specifies the method of signature\n-                                  validation. The allowed options are Cosign and NotaryV2.\n+                                  validation. The allowed options are Cosign and Notary.\n                                   By default Cosign is used if a type is not specified.\n                                 enum:\n                                 - Cosign\n-                                - NotaryV2\n+                                - Notary\n                                 type: string\n                               verifyDigest:\n                                 default: true\n@@ -30214,10 +30235,13 @@ spec:\n                                   type: array\n                                 predicateType:\n                                   description: PredicateType defines the type of Predicate\n+                                    contained within the Statement. Deprecated in\n+                                    favour of 'Type', to be removed soon\n+                                  type: string\n+                                type:\n+                                  description: Type defines the type of attestation\n                                     contained within the Statement.\n                                   type: string\n-                              required:\n-                              - predicateType\n                               type: object\n                             type: array\n                           attestors:\n@@ -30440,11 +30464,11 @@ spec:\n                             type: boolean\n                           type:\n                             description: Type specifies the method of signature validation.\n-                              The allowed options are Cosign and NotaryV2. By default\n+                              The allowed options are Cosign and Notary. By default\n                               Cosign is used if a type is not specified.\n                             enum:\n                             - Cosign\n-                            - NotaryV2\n+                            - Notary\n                             type: string\n                           verifyDigest:\n                             default: true\n@@ -34094,9 +34118,13 @@ spec:\n                                     predicateType:\n                                       description: PredicateType defines the type\n                                         of Predicate contained within the Statement.\n+                                        Deprecated in favour of 'Type', to be removed\n+                                        soon\n+                                      type: string\n+                                    type:\n+                                      description: Type defines the type of attestation\n+                                        contained within the Statement.\n                                       type: string\n-                                  required:\n-                                  - predicateType\n                                   type: object\n                                 type: array\n                               attestors:\n@@ -34353,11 +34381,11 @@ spec:\n                                 type: string\n                               type:\n                                 description: Type specifies the method of signature\n-                                  validation. The allowed options are Cosign and NotaryV2.\n+                                  validation. The allowed options are Cosign and Notary.\n                                   By default Cosign is used if a type is not specified.\n                                 enum:\n                                 - Cosign\n-                                - NotaryV2\n+                                - Notary\n                                 type: string\n                               verifyDigest:\n                                 default: true\ndiff --git a/docs/user/crd/index.html b/docs/user/crd/index.html\nindex 220aee3cd833..5060ed34fce4 100644\n--- a/docs/user/crd/index.html\n+++ b/docs/user/crd/index.html\n@@ -715,7 +715,19 @@ <h3 id=\"kyverno.io/v1.Attestation\">Attestation\n </em>\n </td>\n <td>\n-<p>PredicateType defines the type of Predicate contained within the Statement.</p>\n+<p>PredicateType defines the type of Predicate contained within the Statement.\n+Deprecated in favour of &lsquo;Type&rsquo;, to be removed soon</p>\n+</td>\n+</tr>\n+<tr>\n+<td>\n+<code>type</code><br/>\n+<em>\n+string\n+</em>\n+</td>\n+<td>\n+<p>Type defines the type of attestation contained within the Statement.</p>\n </td>\n </tr>\n <tr>\n@@ -1972,7 +1984,7 @@ <h3 id=\"kyverno.io/v1.ImageVerification\">ImageVerification\n </td>\n <td>\n <p>Type specifies the method of signature validation. The allowed options\n-are Cosign and NotaryV2. By default Cosign is used if a type is not specified.</p>\n+are Cosign and Notary. By default Cosign is used if a type is not specified.</p>\n </td>\n </tr>\n <tr>\n@@ -6372,7 +6384,7 @@ <h3 id=\"kyverno.io/v2beta1.ImageVerification\">ImageVerification\n </td>\n <td>\n <p>Type specifies the method of signature validation. The allowed options\n-are Cosign and NotaryV2. By default Cosign is used if a type is not specified.</p>\n+are Cosign and Notary. By default Cosign is used if a type is not specified.</p>\n </td>\n </tr>\n <tr>\ndiff --git a/go.mod b/go.mod\nindex cfecfba775c5..3bf9a76f6074 100644\n--- a/go.mod\n+++ b/go.mod\n@@ -20,7 +20,7 @@ require (\n \tgithub.com/go-logr/logr v1.2.4\n \tgithub.com/go-logr/zapr v1.2.4\n \tgithub.com/google/gnostic v0.6.9\n-\tgithub.com/google/go-containerregistry v0.14.0\n+\tgithub.com/google/go-containerregistry v0.14.1-0.20230425172351-b7c6e9dc3944\n \tgithub.com/google/go-containerregistry/pkg/authn/kubernetes v0.0.0-20230403180904-b8d1c0a1df12\n \tgithub.com/in-toto/in-toto-golang v0.6.0\n \tgithub.com/jmespath/go-jmespath v0.4.0\n@@ -33,6 +33,7 @@ require (\n \tgithub.com/notaryproject/notation-go v1.0.0-rc.3\n \tgithub.com/onsi/ginkgo v1.16.5\n \tgithub.com/onsi/gomega v1.27.7\n+\tgithub.com/opencontainers/go-digest v1.0.0\n \tgithub.com/opencontainers/image-spec v1.1.0-rc2\n \tgithub.com/orcaman/concurrent-map/v2 v2.0.1\n \tgithub.com/pkg/errors v0.9.1\n@@ -76,7 +77,6 @@ require (\n \tk8s.io/kube-openapi v0.0.0-20230501164219-8b0f38b5fd1f\n \tk8s.io/pod-security-admission v0.27.2\n \tk8s.io/utils v0.0.0-20230406110748-d93618cff8a2\n-\toras.land/oras-go/v2 v2.2.0\n \tsigs.k8s.io/controller-runtime v0.15.0\n \tsigs.k8s.io/kustomize/api v0.13.4\n \tsigs.k8s.io/kustomize/kyaml v0.14.2\n@@ -84,13 +84,13 @@ require (\n )\n \n require (\n+\tcloud.google.com/go/compute v1.19.1 // indirect\n \tgithub.com/antlr/antlr4/runtime/Go/antlr v1.4.10 // indirect\n \tgithub.com/google/cel-go v0.12.6 // indirect\n \tgithub.com/stoewer/go-strcase v1.2.0 // indirect\n )\n \n require (\n-\tcloud.google.com/go/compute v1.19.0 // indirect\n \tcloud.google.com/go/compute/metadata v0.2.3 // indirect\n \tcloud.google.com/go/iam v1.0.0 // indirect\n \tcloud.google.com/go/kms v1.10.1 // indirect\n@@ -108,7 +108,7 @@ require (\n \tgithub.com/Azure/go-autorest/logger v0.2.1 // indirect\n \tgithub.com/Azure/go-autorest/tracing v0.6.0 // indirect\n \tgithub.com/Azure/go-ntlmssp v0.0.0-20221128193559-754e69321358 // indirect\n-\tgithub.com/Microsoft/go-winio v0.6.0 // indirect\n+\tgithub.com/Microsoft/go-winio v0.6.1 // indirect\n \tgithub.com/OneOfOne/xxhash v1.2.8 // indirect\n \tgithub.com/ProtonMail/go-crypto v0.0.0-20230518184743-7afd39499903 // indirect\n \tgithub.com/ThalesIgnite/crypto11 v1.2.5 // indirect\n@@ -156,9 +156,9 @@ require (\n \tgithub.com/davecgh/go-spew v1.1.1 // indirect\n \tgithub.com/dimchansky/utfbom v1.1.1 // indirect\n \tgithub.com/djherbis/times v1.5.0 // indirect\n-\tgithub.com/docker/cli v23.0.2+incompatible // indirect\n+\tgithub.com/docker/cli v23.0.4+incompatible // indirect\n \tgithub.com/docker/distribution v2.8.2+incompatible // indirect\n-\tgithub.com/docker/docker v23.0.3+incompatible // indirect\n+\tgithub.com/docker/docker v23.0.4+incompatible // indirect\n \tgithub.com/docker/docker-credential-helpers v0.7.0 // indirect\n \tgithub.com/dustin/go-humanize v1.0.1 // indirect\n \tgithub.com/emicklei/go-restful/v3 v3.10.2 // indirect\n@@ -229,7 +229,7 @@ require (\n \tgithub.com/josharian/intern v1.0.0 // indirect\n \tgithub.com/json-iterator/go v1.1.12 // indirect\n \tgithub.com/kevinburke/ssh_config v1.2.0 // indirect\n-\tgithub.com/klauspost/compress v1.16.3 // indirect\n+\tgithub.com/klauspost/compress v1.16.5 // indirect\n \tgithub.com/leodido/go-urn v1.2.2 // indirect\n \tgithub.com/letsencrypt/boulder v0.0.0-20230331213904-8c67769be400 // indirect\n \tgithub.com/liggitt/tabwriter v0.0.0-20181228230101-89fcab3d43de // indirect\n@@ -254,7 +254,6 @@ require (\n \tgithub.com/oliveagle/jsonpath v0.0.0-20180606110733-2e52cf6e6852 // indirect\n \tgithub.com/open-policy-agent/gatekeeper v0.0.0-20210824170141-dd97b8a7e966 // indirect\n \tgithub.com/open-policy-agent/opa v0.51.0 // indirect\n-\tgithub.com/opencontainers/go-digest v1.0.0 // indirect\n \tgithub.com/opentracing/opentracing-go v1.2.0 // indirect\n \tgithub.com/pelletier/go-toml/v2 v2.0.7 // indirect\n \tgithub.com/peterbourgon/diskv v2.0.1+incompatible // indirect\n@@ -313,7 +312,7 @@ require (\n \tgo.uber.org/atomic v1.10.0 // indirect\n \tgolang.org/x/mod v0.10.0 // indirect\n \tgolang.org/x/net v0.10.0 // indirect\n-\tgolang.org/x/oauth2 v0.6.0 // indirect\n+\tgolang.org/x/oauth2 v0.7.0 // indirect\n \tgolang.org/x/sync v0.2.0 // indirect\n \tgolang.org/x/sys v0.8.0 // indirect\n \tgolang.org/x/term v0.8.0 // indirect\n@@ -330,6 +329,7 @@ require (\n \tgopkg.in/warnings.v0 v0.1.2 // indirect\n \tk8s.io/component-base v0.27.2 // indirect\n \tk8s.io/kubectl v0.26.3 // indirect\n+\toras.land/oras-go/v2 v2.1.0 // indirect\n \tsigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd // indirect\n \tsigs.k8s.io/release-utils v0.7.3 // indirect\n \tsigs.k8s.io/structured-merge-diff/v4 v4.2.3 // indirect\ndiff --git a/go.sum b/go.sum\nindex c324ad23b7a3..f1c0fff6936d 100644\n--- a/go.sum\n+++ b/go.sum\n@@ -27,8 +27,8 @@ cloud.google.com/go/bigquery v1.4.0/go.mod h1:S8dzgnTigyfTmLBfrtrhyYhwRxG72rYxvf\n cloud.google.com/go/bigquery v1.5.0/go.mod h1:snEHRnqQbz117VIFhE8bmtwIDY80NLUZUMb4Nv6dBIg=\n cloud.google.com/go/bigquery v1.7.0/go.mod h1://okPTzCYNXSlb24MZs83e2Do+h+VXtc4gLoIoXIAPc=\n cloud.google.com/go/bigquery v1.8.0/go.mod h1:J5hqkt3O0uAFnINi6JXValWIb1v0goeZM77hZzJN/fQ=\n-cloud.google.com/go/compute v1.19.0 h1:+9zda3WGgW1ZSTlVppLCYFIr48Pa35q1uG2N1itbCEQ=\n-cloud.google.com/go/compute v1.19.0/go.mod h1:rikpw2y+UMidAe9tISo04EHNOIf42RLYF/q8Bs93scU=\n+cloud.google.com/go/compute v1.19.1 h1:am86mquDUgjGNWxiGn+5PGLbmgiWXlE/yNWpIpNvuXY=\n+cloud.google.com/go/compute v1.19.1/go.mod h1:6ylj3a05WF8leseCdIf77NK0g1ey+nj5IKd5/kvShxE=\n cloud.google.com/go/compute/metadata v0.2.0/go.mod h1:zFmK7XCadkQkj6TtorcaGlCW1hT1fIilQDwofLpJ20k=\n cloud.google.com/go/compute/metadata v0.2.3 h1:mg4jlk7mCAj6xXp9UJ4fjI9VUI5rubuGBW5aJ7UnBMY=\n cloud.google.com/go/compute/metadata v0.2.3/go.mod h1:VAV5nSsACxMJvgaAuX6Pk2AawlZn8kiOGuCv6gTkwuA=\n@@ -109,8 +109,8 @@ github.com/Masterminds/semver v1.5.0/go.mod h1:MB6lktGJrhw8PrUyiEoblNEGEQ+RzHPF0\n github.com/Masterminds/sprig v2.15.0+incompatible/go.mod h1:y6hNFY5UBTIWBxnzTeuNhlNS5hqE0NB0E6fgfo2Br3o=\n github.com/Masterminds/sprig v2.22.0+incompatible/go.mod h1:y6hNFY5UBTIWBxnzTeuNhlNS5hqE0NB0E6fgfo2Br3o=\n github.com/Microsoft/go-winio v0.5.2/go.mod h1:WpS1mjBmmwHBEWmogvA2mj8546UReBk4v8QkMxJ6pZY=\n-github.com/Microsoft/go-winio v0.6.0 h1:slsWYD/zyx7lCXoZVlvQrj0hPTM1HI4+v1sIda2yDvg=\n-github.com/Microsoft/go-winio v0.6.0/go.mod h1:cTAf44im0RAYeL23bpB+fzCyDH2MJiz2BO69KH/soAE=\n+github.com/Microsoft/go-winio v0.6.1 h1:9/kr64B9VUZrLm5YYwbGtUJnMgqWVOdUAXu6Migciow=\n+github.com/Microsoft/go-winio v0.6.1/go.mod h1:LRdKpFKfdobln8UmuiYcKPot9D2v6svN5+sAH+4kjUM=\n github.com/NYTimes/gziphandler v0.0.0-20170623195520-56545f4a5d46/go.mod h1:3wb06e3pkSAbeQ52E9H9iFoQsEEwGN64994WTCIhntQ=\n github.com/OneOfOne/xxhash v1.2.2/go.mod h1:HSdplMjZKSmBqAxg5vPj2TmRDmfkzw+cTzAElWljhcU=\n github.com/OneOfOne/xxhash v1.2.7/go.mod h1:eZbhyaAYD41SGSSsnmcpxVoRiQ/MPUTjUdIIOT9Um7Q=\n@@ -355,12 +355,12 @@ github.com/distribution/distribution v2.8.2+incompatible h1:k9+4DKdOG+quPFZXT/mU\n github.com/distribution/distribution v2.8.2+incompatible/go.mod h1:EgLm2NgWtdKgzF9NpMzUKgzmR7AMmb0VQi2B+ZzDRjc=\n github.com/djherbis/times v1.5.0 h1:79myA211VwPhFTqUk8xehWrsEO+zcIZj0zT8mXPVARU=\n github.com/djherbis/times v1.5.0/go.mod h1:5q7FDLvbNg1L/KaBmPcWlVR9NmoKo3+ucqUA3ijQhA0=\n-github.com/docker/cli v23.0.2+incompatible h1:Yj4wkrNtyCNLCMobKDYzEUIsbtMbfAulkHMH75/ecik=\n-github.com/docker/cli v23.0.2+incompatible/go.mod h1:JLrzqnKDaYBop7H2jaqPtU4hHvMKP+vjCwu2uszcLI8=\n+github.com/docker/cli v23.0.4+incompatible h1:xClB7PsiATttDHj8ce5qvJcikiApNy7teRR1XkoBZGs=\n+github.com/docker/cli v23.0.4+incompatible/go.mod h1:JLrzqnKDaYBop7H2jaqPtU4hHvMKP+vjCwu2uszcLI8=\n github.com/docker/distribution v2.8.2+incompatible h1:T3de5rq0dB1j30rp0sA2rER+m322EBzniBPB6ZIzuh8=\n github.com/docker/distribution v2.8.2+incompatible/go.mod h1:J2gT2udsDAN96Uj4KfcMRqY0/ypR+oyYUYmja8H+y+w=\n-github.com/docker/docker v23.0.3+incompatible h1:9GhVsShNWz1hO//9BNg/dpMnZW25KydO4wtVxWAIbho=\n-github.com/docker/docker v23.0.3+incompatible/go.mod h1:eEKB0N0r5NX/I1kEveEz05bcu8tLC/8azJZsviup8Sk=\n+github.com/docker/docker v23.0.4+incompatible h1:Kd3Bh9V/rO+XpTP/BLqM+gx8z7+Yb0AA2Ibj+nNo4ek=\n+github.com/docker/docker v23.0.4+incompatible/go.mod h1:eEKB0N0r5NX/I1kEveEz05bcu8tLC/8azJZsviup8Sk=\n github.com/docker/docker-credential-helpers v0.7.0 h1:xtCHsjxogADNZcdv1pKUHXryefjlVRqWqIhk/uXJp0A=\n github.com/docker/docker-credential-helpers v0.7.0/go.mod h1:rETQfLdHNT3foU5kuNkFR1R1V12OJRRO5lzt2D1b5X0=\n github.com/docker/spdystream v0.0.0-20160310174837-449fdfce4d96/go.mod h1:Qh8CwZgvJUkLughtfhJv5dyTYa91l1fOUCrgjqmcifM=\n@@ -669,8 +669,8 @@ github.com/google/go-cmp v0.5.6/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/\n github.com/google/go-cmp v0.5.8/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=\n github.com/google/go-cmp v0.5.9 h1:O2Tfq5qg4qc4AmwVlvv0oLiVAGB7enBSJ2x2DqQFi38=\n github.com/google/go-cmp v0.5.9/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=\n-github.com/google/go-containerregistry v0.14.0 h1:z58vMqHxuwvAsVwvKEkmVBz2TlgBgH5k6koEXBtlYkw=\n-github.com/google/go-containerregistry v0.14.0/go.mod h1:aiJ2fp/SXvkWgmYHioXnbMdlgB8eXiiYOY55gfN91Wk=\n+github.com/google/go-containerregistry v0.14.1-0.20230425172351-b7c6e9dc3944 h1:7c5khUnWebZDFMUQ7rf2vynmmnKI1VvBACrTZKKpoD4=\n+github.com/google/go-containerregistry v0.14.1-0.20230425172351-b7c6e9dc3944/go.mod h1:0JopT7wiZeP5/ATNgx85oApuNAiNnfn4mr8+WOssYNQ=\n github.com/google/go-containerregistry/pkg/authn/kubernetes v0.0.0-20230403180904-b8d1c0a1df12 h1:LLLVB/7zCZVKI27rqA7bbZHZJxH1lL2jbLxdomX1Eew=\n github.com/google/go-containerregistry/pkg/authn/kubernetes v0.0.0-20230403180904-b8d1c0a1df12/go.mod h1:CSeefFZsOfyNrYGXDafpWNkf3tUz17nKReR5INPRaMI=\n github.com/google/go-github/v45 v45.2.0 h1:5oRLszbrkvxDDqBCNj2hjDZMKmvexaZ1xw/FCD+K3FI=\n@@ -880,8 +880,8 @@ github.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO+MFFWcvkIS/tQcRk01m1F5IRFswLeQ+o\n github.com/klauspost/compress v1.10.7/go.mod h1:aoV0uJVorq1K+umq18yTdKaF57EivdYsUV+/s2qKfXs=\n github.com/klauspost/compress v1.11.0/go.mod h1:aoV0uJVorq1K+umq18yTdKaF57EivdYsUV+/s2qKfXs=\n github.com/klauspost/compress v1.13.6/go.mod h1:/3/Vjq9QcHkK5uEr5lBEmyoZ1iFhe47etQ6QUkpK6sk=\n-github.com/klauspost/compress v1.16.3 h1:XuJt9zzcnaz6a16/OU53ZjWp/v7/42WcR5t2a0PcNQY=\n-github.com/klauspost/compress v1.16.3/go.mod h1:ntbaceVETuRiXiv4DpjP66DpAtAGkEQskQzEyD//IeE=\n+github.com/klauspost/compress v1.16.5 h1:IFV2oUNUzZaz+XyusxpLzpzS8Pt5rh0Z16For/djlyI=\n+github.com/klauspost/compress v1.16.5/go.mod h1:ntbaceVETuRiXiv4DpjP66DpAtAGkEQskQzEyD//IeE=\n github.com/konsorten/go-windows-terminal-sequences v1.0.1/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\n github.com/konsorten/go-windows-terminal-sequences v1.0.2/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\n github.com/konsorten/go-windows-terminal-sequences v1.0.3/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\n@@ -1672,8 +1672,8 @@ golang.org/x/oauth2 v0.0.0-20210218202405-ba52d332ba99/go.mod h1:KelEdhl1UZF7XfJ\n golang.org/x/oauth2 v0.0.0-20210514164344-f6687ab2804c/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\n golang.org/x/oauth2 v0.0.0-20211104180415-d3ed0bb246c8/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\n golang.org/x/oauth2 v0.3.0/go.mod h1:rQrIauxkUhJ6CuwEXwymO2/eh4xz2ZWF1nBkcxS+tGk=\n-golang.org/x/oauth2 v0.6.0 h1:Lh8GPgSKBfWSwFvtuWOfeI3aAAnbXTSutYxJiOJFgIw=\n-golang.org/x/oauth2 v0.6.0/go.mod h1:ycmewcwgD4Rpr3eZJLSB4Kyyljb3qDh40vJ8STE5HKw=\n+golang.org/x/oauth2 v0.7.0 h1:qe6s0zUXlPX80/dITx3440hWZ7GwMwgDDyrSGTPJG/g=\n+golang.org/x/oauth2 v0.7.0/go.mod h1:hPLQkd9LyjfXTiRohC/41GhcFqxisoUQ99sCUOHO9x4=\n golang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n golang.org/x/sync v0.0.0-20181108010431-42b317875d0f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n golang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n@@ -2176,8 +2176,8 @@ mvdan.cc/gofumpt v0.1.1/go.mod h1:yXG1r1WqZVKWbVRtBWKWX9+CxGYfA51nSomhM0woR48=\n mvdan.cc/interfacer v0.0.0-20180901003855-c20040233aed/go.mod h1:Xkxe497xwlCKkIaQYRfC7CSLworTXY9RMqwhhCm+8Nc=\n mvdan.cc/lint v0.0.0-20170908181259-adc824a0674b/go.mod h1:2odslEg/xrtNQqCYg2/jCoyKnw3vv5biOc3JnIcYfL4=\n mvdan.cc/unparam v0.0.0-20210104141923-aac4ce9116a7/go.mod h1:hBpJkZE8H/sb+VRFvw2+rBpHNsTBcvSpk61hr8mzXZE=\n-oras.land/oras-go/v2 v2.2.0 h1:E1fqITD56Eg5neZbxBtAdZVgDHD6wBabJo6xESTcQyo=\n-oras.land/oras-go/v2 v2.2.0/go.mod h1:pXjn0+KfarspMHHNR3A56j3tgvr+mxArHuI8qVn59v8=\n+oras.land/oras-go/v2 v2.1.0 h1:1nS8BIeEP6CBVQifwxrsth2bkuD+cYfjp7Hf7smUcS8=\n+oras.land/oras-go/v2 v2.1.0/go.mod h1:v5ZSAPIMEJYnZjZ6rTGPAyaonH+rCFmbE95IAzCTeGU=\n rsc.io/binaryregexp v0.2.0/go.mod h1:qTv7/COck+e2FymRvadv62gMdZztPaShugOCi3I+8D8=\n rsc.io/quote/v3 v3.1.0/go.mod h1:yEA65RcK8LyAZtP9Kv3t0HmxON59tX3rD+tICJqUlj0=\n rsc.io/sampler v1.3.0/go.mod h1:T1hPZKmBbMNahiBKFy5HrXp6adAjACjK9JXDnKaTXpA=\ndiff --git a/pkg/cosign/cosign.go b/pkg/cosign/cosign.go\nindex 37c9b2af8cce..20a745fd1c64 100644\n--- a/pkg/cosign/cosign.go\n+++ b/pkg/cosign/cosign.go\n@@ -79,7 +79,7 @@ func (v *cosignVerifier) VerifySignature(ctx context.Context, opts images.Option\n \t}\n \n \tvar digest string\n-\tif opts.PredicateType == \"\" {\n+\tif opts.Type == \"\" {\n \t\tdigest, err = extractDigest(opts.ImageRef, payload)\n \t\tif err != nil {\n \t\t\treturn nil, err\n@@ -265,13 +265,13 @@ func (v *cosignVerifier) FetchAttestations(ctx context.Context, opts images.Opti\n \t}\n \n \tfor _, signature := range signatures {\n-\t\tmatch, predicateType, err := matchPredicateType(signature, opts.PredicateType)\n+\t\tmatch, predicateType, err := matchType(signature, opts.Type)\n \t\tif err != nil {\n \t\t\treturn nil, err\n \t\t}\n \n \t\tif !match {\n-\t\t\tlogger.V(4).Info(\"predicateType doesn't match, continue\", \"expected\", opts.PredicateType, \"received\", predicateType)\n+\t\t\tlogger.V(4).Info(\"type doesn't match, continue\", \"expected\", opts.Type, \"received\", predicateType)\n \t\t\tcontinue\n \t\t}\n \n@@ -294,15 +294,15 @@ func (v *cosignVerifier) FetchAttestations(ctx context.Context, opts images.Opti\n \treturn &images.Response{Digest: digest, Statements: inTotoStatements}, nil\n }\n \n-func matchPredicateType(sig oci.Signature, expectedPredicateType string) (bool, string, error) {\n-\tif expectedPredicateType != \"\" {\n+func matchType(sig oci.Signature, expectedType string) (bool, string, error) {\n+\tif expectedType != \"\" {\n \t\tstatement, _, err := decodeStatement(sig)\n \t\tif err != nil {\n-\t\t\treturn false, \"\", fmt.Errorf(\"failed to decode predicateType: %w\", err)\n+\t\t\treturn false, \"\", fmt.Errorf(\"failed to decode type: %w\", err)\n \t\t}\n \n-\t\tif pType, ok := statement[\"predicateType\"]; ok {\n-\t\t\tif pType.(string) == expectedPredicateType {\n+\t\tif pType, ok := statement[\"type\"]; ok {\n+\t\t\tif pType.(string) == expectedType {\n \t\t\t\treturn true, pType.(string), nil\n \t\t\t}\n \t\t}\n@@ -360,6 +360,7 @@ func decodeStatement(sig oci.Signature) (map[string]interface{}, string, error)\n \t\tif err != nil {\n \t\t\treturn nil, \"\", fmt.Errorf(\"failed to decode statement %s: %w\", string(pld), err)\n \t\t}\n+\t\tdecodedStatement[\"type\"] = decodedStatement[\"predicateType\"]\n \n \t\treturn decodedStatement, digest, nil\n \t}\n@@ -376,7 +377,7 @@ func decodePayload(payloadBase64 string) (map[string]interface{}, error) {\n \t\treturn nil, err\n \t}\n \n-\tif statement.PredicateType != attestation.CosignCustomProvenanceV01 {\n+\tif statement.Type != attestation.CosignCustomProvenanceV01 {\n \t\t// This assumes that the following statements are JSON objects:\n \t\t// - in_toto.PredicateSLSAProvenanceV01\n \t\t// - in_toto.PredicateLinkV1\n@@ -389,7 +390,7 @@ func decodePayload(payloadBase64 string) (map[string]interface{}, error) {\n }\n \n func decodeCosignCustomProvenanceV01(statement in_toto.Statement) (map[string]interface{}, error) {\n-\tif statement.PredicateType != attestation.CosignCustomProvenanceV01 {\n+\tif statement.Type != attestation.CosignCustomProvenanceV01 {\n \t\treturn nil, fmt.Errorf(\"invalid statement type %s\", attestation.CosignCustomProvenanceV01)\n \t}\n \ndiff --git a/pkg/engine/api/context.go b/pkg/engine/api/context.go\nindex d1b80b63d7c4..0091ad8c8cc0 100644\n--- a/pkg/engine/api/context.go\n+++ b/pkg/engine/api/context.go\n@@ -6,6 +6,7 @@ import (\n \t\"fmt\"\n \n \t\"github.com/go-logr/logr\"\n+\t\"github.com/google/go-containerregistry/pkg/name\"\n \tkyvernov1 \"github.com/kyverno/kyverno/api/kyverno/v1\"\n \t\"github.com/kyverno/kyverno/pkg/clients/dclient\"\n \t\"github.com/kyverno/kyverno/pkg/engine/apicall\"\n@@ -141,6 +142,13 @@ func fetchImageData(ctx context.Context, jp jmespath.Interface, rclient registry\n // FetchImageDataMap fetches image information from the remote registry.\n func fetchImageDataMap(ctx context.Context, rclient registryclient.Client, ref string) (interface{}, error) {\n \tdesc, err := rclient.FetchImageDescriptor(ctx, ref)\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"failed to fetch image descriptor: %s, error: %v\", ref, err)\n+\t}\n+\tparsedRef, err := name.ParseReference(ref)\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"failed to parse image reference: %s, error: %v\", ref, err)\n+\t}\n \tif err != nil {\n \t\treturn nil, err\n \t}\n@@ -169,10 +177,10 @@ func fetchImageDataMap(ctx context.Context, rclient registryclient.Client, ref s\n \n \tdata := map[string]interface{}{\n \t\t\"image\":         ref,\n-\t\t\"resolvedImage\": fmt.Sprintf(\"%s@%s\", desc.Ref.Context().Name(), desc.Digest.String()),\n-\t\t\"registry\":      desc.Ref.Context().RegistryStr(),\n-\t\t\"repository\":    desc.Ref.Context().RepositoryStr(),\n-\t\t\"identifier\":    desc.Ref.Identifier(),\n+\t\t\"resolvedImage\": fmt.Sprintf(\"%s@%s\", parsedRef.Context().Name(), desc.Digest.String()),\n+\t\t\"registry\":      parsedRef.Context().RegistryStr(),\n+\t\t\"repository\":    parsedRef.Context().RepositoryStr(),\n+\t\t\"identifier\":    parsedRef.Identifier(),\n \t\t\"manifest\":      manifest,\n \t\t\"configData\":    configData,\n \t}\ndiff --git a/pkg/engine/handlers/mutation/mutate_image.go b/pkg/engine/handlers/mutation/mutate_image.go\nindex 85b187ce2265..c48ca38c21a5 100644\n--- a/pkg/engine/handlers/mutation/mutate_image.go\n+++ b/pkg/engine/handlers/mutation/mutate_image.go\n@@ -112,7 +112,9 @@ func substituteVariables(rule kyvernov1.Rule, ctx enginecontext.EvalInterface, l\n \t// remove attestations as variables are not substituted in them\n \truleCopy := *rule.DeepCopy()\n \tfor i := range ruleCopy.VerifyImages {\n-\t\truleCopy.VerifyImages[i].Attestations = nil\n+\t\tfor j := range ruleCopy.VerifyImages[i].Attestations {\n+\t\t\truleCopy.VerifyImages[i].Attestations[j].Conditions = nil\n+\t\t}\n \t}\n \tvar err error\n \truleCopy, err = variables.SubstituteAllInRule(logger, ctx, ruleCopy)\n@@ -120,8 +122,10 @@ func substituteVariables(rule kyvernov1.Rule, ctx enginecontext.EvalInterface, l\n \t\treturn nil, err\n \t}\n \t// replace attestations\n-\tfor i := range rule.VerifyImages {\n-\t\truleCopy.VerifyImages[i].Attestations = rule.VerifyImages[i].Attestations\n+\tfor i := range ruleCopy.VerifyImages {\n+\t\tfor j := range ruleCopy.VerifyImages[i].Attestations {\n+\t\t\truleCopy.VerifyImages[i].Attestations[j].Conditions = rule.VerifyImages[i].Attestations[j].Conditions\n+\t\t}\n \t}\n \treturn &ruleCopy, nil\n }\ndiff --git a/pkg/engine/internal/imageverifier.go b/pkg/engine/internal/imageverifier.go\nindex f8a9bb21d81e..ffd5601c6d82 100644\n--- a/pkg/engine/internal/imageverifier.go\n+++ b/pkg/engine/internal/imageverifier.go\n@@ -15,7 +15,7 @@ import (\n \tenginecontext \"github.com/kyverno/kyverno/pkg/engine/context\"\n \t\"github.com/kyverno/kyverno/pkg/engine/variables\"\n \t\"github.com/kyverno/kyverno/pkg/images\"\n-\t\"github.com/kyverno/kyverno/pkg/notaryv2\"\n+\t\"github.com/kyverno/kyverno/pkg/notary\"\n \t\"github.com/kyverno/kyverno/pkg/registryclient\"\n \tapiutils \"github.com/kyverno/kyverno/pkg/utils/api\"\n \t\"github.com/kyverno/kyverno/pkg/utils/jsonpointer\"\n@@ -138,7 +138,7 @@ func buildStatementMap(statements []map[string]interface{}) (map[string][]map[st\n \tresults := map[string][]map[string]interface{}{}\n \tvar predicateTypes []string\n \tfor _, s := range statements {\n-\t\tpredicateType := s[\"predicateType\"].(string)\n+\t\tpredicateType := s[\"type\"].(string)\n \t\tif results[predicateType] != nil {\n \t\t\tresults[predicateType] = append(results[predicateType], s)\n \t\t} else {\n@@ -249,6 +249,11 @@ func (iv *ImageVerifier) verifyImage(\n \t\treturn nil, \"\"\n \t}\n \timage := imageInfo.String()\n+\tfor _, att := range imageVerify.Attestations {\n+\t\tif att.Type == \"\" && att.PredicateType != \"\" {\n+\t\t\tatt.Type = att.PredicateType\n+\t\t}\n+\t}\n \tiv.logger.V(2).Info(\"verifying image signatures\", \"image\", image, \"attestors\", len(imageVerify.Attestors), \"attestations\", len(imageVerify.Attestations))\n \tif err := iv.policyContext.JSONContext().AddImageInfo(imageInfo, cfg); err != nil {\n \t\tiv.logger.Error(err, \"failed to add image to context\")\n@@ -319,8 +324,13 @@ func (iv *ImageVerifier) verifyAttestations(\n \t\tvar attestationError error\n \t\tpath := fmt.Sprintf(\".attestations[%d]\", i)\n \n-\t\tif attestation.PredicateType == \"\" {\n-\t\t\treturn engineapi.RuleFail(iv.rule.Name, engineapi.ImageVerify, path+\": missing predicateType\"), \"\"\n+\t\tiv.logger.V(2).Info(fmt.Sprintf(\"attestation %+v\", attestation))\n+\t\tif attestation.Type == \"\" && attestation.PredicateType == \"\" {\n+\t\t\treturn engineapi.RuleFail(iv.rule.Name, engineapi.ImageVerify, path+\": missing type\"), \"\"\n+\t\t}\n+\n+\t\tif attestation.Type == \"\" && attestation.PredicateType != \"\" {\n+\t\t\tattestation.Type = attestation.PredicateType\n \t\t}\n \n \t\tif len(attestation.Attestors) == 0 {\n@@ -366,7 +376,7 @@ func (iv *ImageVerifier) verifyAttestations(\n \t\t\t}\n \t\t}\n \n-\t\tiv.logger.V(4).Info(\"attestation checks passed\", \"path\", path, \"image\", imageInfo.String(), \"predicateType\", attestation.PredicateType)\n+\t\tiv.logger.V(4).Info(\"attestation checks passed\", \"path\", path, \"image\", imageInfo.String(), \"type\", attestation.Type)\n \t}\n \n \tmsg := fmt.Sprintf(\"verified image attestations for %s\", image)\n@@ -432,8 +442,8 @@ func (iv *ImageVerifier) buildVerifier(\n \tattestation *kyvernov1.Attestation,\n ) (images.ImageVerifier, *images.Options, string) {\n \tswitch imageVerify.Type {\n-\tcase kyvernov1.NotaryV2:\n-\t\treturn iv.buildNotaryV2Verifier(attestor, imageVerify, image)\n+\tcase kyvernov1.Notary:\n+\t\treturn iv.buildNotaryVerifier(attestor, imageVerify, image, attestation)\n \tdefault:\n \t\treturn iv.buildCosignVerifier(attestor, imageVerify, image, attestation)\n \t}\n@@ -463,6 +473,11 @@ func (iv *ImageVerifier) buildCosignVerifier(\n \n \tif attestation != nil {\n \t\topts.PredicateType = attestation.PredicateType\n+\t\topts.Type = attestation.Type\n+\t\tif attestation.PredicateType != \"\" && attestation.Type == \"\" {\n+\t\t\tiv.logger.Info(\"predicate type has been deprecated, please use type instead\")\n+\t\t\topts.Type = attestation.PredicateType\n+\t\t}\n \t\topts.FetchAttestations = true\n \t}\n \n@@ -509,10 +524,11 @@ func (iv *ImageVerifier) buildCosignVerifier(\n \treturn cosign.NewVerifier(), opts, path\n }\n \n-func (iv *ImageVerifier) buildNotaryV2Verifier(\n+func (iv *ImageVerifier) buildNotaryVerifier(\n \tattestor kyvernov1.Attestor,\n \timageVerify kyvernov1.ImageVerification,\n \timage string,\n+\tattestation *kyvernov1.Attestation,\n ) (images.ImageVerifier, *images.Options, string) {\n \tpath := \"\"\n \topts := &images.Options{\n@@ -522,20 +538,38 @@ func (iv *ImageVerifier) buildNotaryV2Verifier(\n \t\tRegistryClient: iv.rclient,\n \t}\n \n-\treturn notaryv2.NewVerifier(), opts, path\n+\tif attestation != nil {\n+\t\topts.Type = attestation.Type\n+\t\topts.PredicateType = attestation.PredicateType\n+\t\tif attestation.PredicateType != \"\" && attestation.Type == \"\" {\n+\t\t\tiv.logger.Info(\"predicate type has been deprecated, please use type instead\")\n+\t\t\topts.Type = attestation.PredicateType\n+\t\t}\n+\t\topts.FetchAttestations = true\n+\t}\n+\n+\tif attestor.Repository != \"\" {\n+\t\topts.Repository = attestor.Repository\n+\t}\n+\n+\tif attestor.Annotations != nil {\n+\t\topts.Annotations = attestor.Annotations\n+\t}\n+\n+\treturn notary.NewVerifier(), opts, path\n }\n \n func (iv *ImageVerifier) verifyAttestation(statements []map[string]interface{}, attestation kyvernov1.Attestation, imageInfo apiutils.ImageInfo) error {\n-\tif attestation.PredicateType == \"\" {\n-\t\treturn fmt.Errorf(\"a predicateType is required\")\n+\tif attestation.Type == \"\" && attestation.PredicateType == \"\" {\n+\t\treturn fmt.Errorf(\"a type is required\")\n \t}\n \timage := imageInfo.String()\n \tstatementsByPredicate, types := buildStatementMap(statements)\n \tiv.logger.V(4).Info(\"checking attestations\", \"predicates\", types, \"image\", image)\n-\tstatements = statementsByPredicate[attestation.PredicateType]\n+\tstatements = statementsByPredicate[attestation.Type]\n \tif statements == nil {\n-\t\tiv.logger.Info(\"no attestations found for predicate\", \"type\", attestation.PredicateType, \"predicates\", types, \"image\", imageInfo.String())\n-\t\treturn fmt.Errorf(\"attestions not found for predicate type %s\", attestation.PredicateType)\n+\t\tiv.logger.Info(\"no attestations found for predicate\", \"type\", attestation.Type, \"predicates\", types, \"image\", imageInfo.String())\n+\t\treturn fmt.Errorf(\"attestions not found for predicate type %s\", attestation.Type)\n \t}\n \tfor _, s := range statements {\n \t\tiv.logger.Info(\"checking attestation\", \"predicates\", types, \"image\", imageInfo.String())\n@@ -544,7 +578,7 @@ func (iv *ImageVerifier) verifyAttestation(statements []map[string]interface{},\n \t\t\treturn fmt.Errorf(\"failed to check attestations: %w\", err)\n \t\t}\n \t\tif !val {\n-\t\t\treturn fmt.Errorf(\"attestation checks failed for %s and predicate %s: %s\", imageInfo.String(), attestation.PredicateType, msg)\n+\t\t\treturn fmt.Errorf(\"attestation checks failed for %s and predicate %s: %s\", imageInfo.String(), attestation.Type, msg)\n \t\t}\n \t}\n \treturn nil\ndiff --git a/pkg/images/verifier.go b/pkg/images/verifier.go\nindex def5d9ecc1da..51874e1c3673 100644\n--- a/pkg/images/verifier.go\n+++ b/pkg/images/verifier.go\n@@ -30,6 +30,7 @@ type Options struct {\n \tRekorURL             string\n \tSignatureAlgorithm   string\n \tPredicateType        string\n+\tType                 string\n \tIdentities           string\n }\n \ndiff --git a/pkg/notary/notary.go b/pkg/notary/notary.go\nnew file mode 100644\nindex 000000000000..993434ef75c4\n--- /dev/null\n+++ b/pkg/notary/notary.go\n@@ -0,0 +1,348 @@\n+package notary\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"encoding/json\"\n+\t\"fmt\"\n+\n+\t\"github.com/go-logr/logr\"\n+\t\"github.com/google/go-containerregistry/pkg/crane\"\n+\t\"github.com/google/go-containerregistry/pkg/name\"\n+\tv1 \"github.com/google/go-containerregistry/pkg/v1\"\n+\t\"github.com/google/go-containerregistry/pkg/v1/remote\"\n+\t\"github.com/kyverno/kyverno/pkg/images\"\n+\t\"github.com/kyverno/kyverno/pkg/logging\"\n+\t_ \"github.com/notaryproject/notation-core-go/signature/cose\"\n+\t_ \"github.com/notaryproject/notation-core-go/signature/jws\"\n+\t\"github.com/notaryproject/notation-go\"\n+\t\"github.com/notaryproject/notation-go/verifier\"\n+\t\"github.com/notaryproject/notation-go/verifier/trustpolicy\"\n+\t\"github.com/opencontainers/go-digest\"\n+\tocispec \"github.com/opencontainers/image-spec/specs-go/v1\"\n+\t\"github.com/pkg/errors\"\n+\t\"github.com/sigstore/sigstore/pkg/cryptoutils\"\n+\t\"go.uber.org/multierr\"\n+)\n+\n+func NewVerifier() images.ImageVerifier {\n+\treturn &notaryVerifier{\n+\t\tlog: logging.WithName(\"Notary\"),\n+\t}\n+}\n+\n+type notaryVerifier struct {\n+\tlog logr.Logger\n+}\n+\n+func (v *notaryVerifier) VerifySignature(ctx context.Context, opts images.Options) (*images.Response, error) {\n+\tv.log.V(2).Info(\"verifying image\", \"reference\", opts.ImageRef)\n+\n+\tcertsPEM := combineCerts(opts)\n+\tcerts, err := cryptoutils.LoadCertificatesFromPEM(bytes.NewReader([]byte(certsPEM)))\n+\tif err != nil {\n+\t\treturn nil, errors.Wrapf(err, \"failed to parse certificates\")\n+\t}\n+\n+\ttrustStore := NewTrustStore(\"kyverno\", certs)\n+\tpolicyDoc := v.buildPolicy()\n+\tnotationVerifier, err := verifier.New(policyDoc, trustStore, nil)\n+\tif err != nil {\n+\t\treturn nil, errors.Wrapf(err, \"failed to created verifier\")\n+\t}\n+\n+\tv.log.V(4).Info(\"creating notation repo\", \"reference\", opts.ImageRef)\n+\tparsedRef, err := parseReferenceCrane(ctx, opts.ImageRef, opts.RegistryClient)\n+\tif err != nil {\n+\t\treturn nil, errors.Wrapf(err, \"failed to parse image reference: %s\", opts.ImageRef)\n+\t}\n+\tv.log.V(4).Info(\"created parsedRef\", \"reference\", opts.ImageRef)\n+\n+\tref := parsedRef.Ref.Name()\n+\tremoteVerifyOptions := notation.RemoteVerifyOptions{\n+\t\tArtifactReference:    ref,\n+\t\tMaxSignatureAttempts: 10,\n+\t}\n+\n+\ttargetDesc, outcomes, err := notation.Verify(context.TODO(), notationVerifier, parsedRef.Repo, remoteVerifyOptions)\n+\tif err != nil {\n+\t\treturn nil, errors.Wrapf(err, \"failed to verify %s\", ref)\n+\t}\n+\n+\tif err := v.verifyOutcomes(outcomes); err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tv.log.V(2).Info(\"verified image\", \"type\", targetDesc.MediaType, \"digest\", targetDesc.Digest, \"size\", targetDesc.Size)\n+\n+\tresp := &images.Response{\n+\t\tDigest:     targetDesc.Digest.String(),\n+\t\tStatements: nil,\n+\t}\n+\n+\treturn resp, nil\n+}\n+\n+func combineCerts(opts images.Options) string {\n+\tcerts := opts.Cert\n+\tif opts.CertChain != \"\" {\n+\t\tif certs != \"\" {\n+\t\t\tcerts = certs + \"\\n\"\n+\t\t}\n+\n+\t\tcerts = certs + opts.CertChain\n+\t}\n+\n+\treturn certs\n+}\n+\n+func (v *notaryVerifier) buildPolicy() *trustpolicy.Document {\n+\treturn &trustpolicy.Document{\n+\t\tVersion: \"1.0\",\n+\t\tTrustPolicies: []trustpolicy.TrustPolicy{\n+\t\t\t{\n+\t\t\t\tName:                  \"kyverno\",\n+\t\t\t\tRegistryScopes:        []string{\"*\"},\n+\t\t\t\tSignatureVerification: trustpolicy.SignatureVerification{VerificationLevel: trustpolicy.LevelStrict.Name},\n+\t\t\t\tTrustStores:           []string{\"ca:kyverno\"},\n+\t\t\t\tTrustedIdentities:     []string{\"*\"},\n+\t\t\t},\n+\t\t},\n+\t}\n+}\n+\n+func (v *notaryVerifier) verifyOutcomes(outcomes []*notation.VerificationOutcome) error {\n+\tvar errs []error\n+\tfor _, outcome := range outcomes {\n+\t\tif outcome.Error != nil {\n+\t\t\terrs = append(errs, outcome.Error)\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tcontent := outcome.EnvelopeContent.Payload.Content\n+\t\tcontentType := outcome.EnvelopeContent.Payload.ContentType\n+\n+\t\tv.log.V(2).Info(\"content\", \"type\", contentType, \"data\", content)\n+\t}\n+\n+\treturn multierr.Combine(errs...)\n+}\n+\n+func (v *notaryVerifier) FetchAttestations(ctx context.Context, opts images.Options) (*images.Response, error) {\n+\tv.log.V(2).Info(\"fetching attestations\", \"reference\", opts.ImageRef, \"opts\", opts)\n+\n+\tref, err := name.ParseReference(opts.ImageRef)\n+\tif err != nil {\n+\t\treturn nil, errors.Wrapf(err, \"failed to parse image reference: %s\", opts.ImageRef)\n+\t}\n+\tauthenticator, err := getAuthenticator(ctx, opts.ImageRef, opts.RegistryClient)\n+\tif err != nil {\n+\t\treturn nil, errors.Wrapf(err, \"failed to parse authenticator: %s\", opts.ImageRef)\n+\t}\n+\tcraneOpts := crane.WithAuth(*authenticator)\n+\n+\tremoteOpts, err := getRemoteOpts(*authenticator)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tv.log.V(4).Info(\"client setup done\", \"repo\", ref)\n+\n+\trepoDesc, err := crane.Head(opts.ImageRef, craneOpts)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\tv.log.V(4).Info(\"fetched repository\", \"repoDesc\", repoDesc)\n+\n+\treferrers, err := remote.Referrers(ref.Context().Digest(repoDesc.Digest.String()), remoteOpts...)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\treferrersDescs, err := referrers.IndexManifest()\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tv.log.V(4).Info(\"fetched referrers\", \"referrers\", referrersDescs)\n+\n+\tvar statements []map[string]interface{}\n+\n+\tfor _, referrer := range referrersDescs.Manifests {\n+\t\tmatch, _, err := matchArtifactType(referrer, opts.Type)\n+\t\tif err != nil {\n+\t\t\treturn nil, err\n+\t\t}\n+\n+\t\tif !match {\n+\t\t\tv.log.V(6).Info(\"type doesn't match, continue\", \"expected\", opts.Type, \"received\", referrer.ArtifactType)\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\ttargetDesc, err := verifyAttestators(ctx, v, ref, opts, referrer)\n+\t\tif err != nil {\n+\t\t\tmsg := err.Error()\n+\t\t\tv.log.V(4).Info(msg, \"failed to verify referrer %s\", targetDesc.Digest.String())\n+\t\t\treturn nil, err\n+\t\t}\n+\n+\t\tv.log.V(4).Info(\"extracting statements\", \"desc\", referrer, \"repo\", ref)\n+\t\tstatements, err = extractStatements(ctx, ref, referrer, craneOpts)\n+\t\tif err != nil {\n+\t\t\tmsg := err.Error()\n+\t\t\tv.log.V(4).Info(\"failed to extract statements %s\", \"err\", msg)\n+\t\t\treturn nil, err\n+\t\t}\n+\n+\t\tv.log.V(4).Info(\"verified attestators\", \"digest\", targetDesc.Digest.String())\n+\n+\t\tif len(statements) == 0 {\n+\t\t\treturn nil, fmt.Errorf(\"failed to fetch attestations\")\n+\t\t}\n+\t\tv.log.V(6).Info(\"sending response\")\n+\t\treturn &images.Response{Digest: repoDesc.Digest.String(), Statements: statements}, nil\n+\t}\n+\n+\treturn nil, fmt.Errorf(\"failed to fetch attestations %s\", err)\n+}\n+\n+func verifyAttestators(ctx context.Context, v *notaryVerifier, ref name.Reference, opts images.Options, desc v1.Descriptor) (ocispec.Descriptor, error) {\n+\tv.log.V(2).Info(\"verifying attestations\", \"reference\", opts.ImageRef, \"opts\", opts)\n+\tif opts.Cert == \"\" && opts.CertChain == \"\" {\n+\t\t// skips the checks when no attestor is provided\n+\t\tv1Desc := ocispec.Descriptor{\n+\t\t\tMediaType:   string(desc.MediaType),\n+\t\t\tSize:        desc.Size,\n+\t\t\tDigest:      digest.Digest(desc.Digest.String()),\n+\t\t\tURLs:        desc.URLs,\n+\t\t\tAnnotations: desc.Annotations,\n+\t\t\tData:        desc.Data,\n+\t\t}\n+\t\treturn v1Desc, nil\n+\t}\n+\tcertsPEM := combineCerts(opts)\n+\tcerts, err := cryptoutils.LoadCertificatesFromPEM(bytes.NewReader([]byte(certsPEM)))\n+\tif err != nil {\n+\t\tv.log.V(4).Info(\"failed to parse certificates\", \"err\", err)\n+\t\treturn ocispec.Descriptor{}, errors.Wrapf(err, \"failed to parse certificates\")\n+\t}\n+\n+\tv.log.V(4).Info(\"parsed certificates\")\n+\ttrustStore := NewTrustStore(\"kyverno\", certs)\n+\tpolicyDoc := v.buildPolicy()\n+\tnotationVerifier, err := verifier.New(policyDoc, trustStore, nil)\n+\tif err != nil {\n+\t\tv.log.V(4).Info(\"failed to created verifier\", \"err\", err)\n+\t\treturn ocispec.Descriptor{}, errors.Wrapf(err, \"failed to created verifier\")\n+\t}\n+\n+\tv.log.V(4).Info(\"created verifier\")\n+\treference := ref.Context().RegistryStr() + \"/\" + ref.Context().RepositoryStr() + \"@\" + desc.Digest.String()\n+\tparsedRef, err := parseReferenceCrane(ctx, reference, opts.RegistryClient)\n+\tif err != nil {\n+\t\treturn ocispec.Descriptor{}, errors.Wrapf(err, \"failed to parse image reference: %s\", opts.ImageRef)\n+\t}\n+\tv.log.V(4).Info(\"created notation repo\", \"reference\", opts.ImageRef)\n+\n+\tremoteVerifyOptions := notation.RemoteVerifyOptions{\n+\t\tArtifactReference:    reference,\n+\t\tMaxSignatureAttempts: 10,\n+\t}\n+\n+\tv.log.V(4).Info(\"verification started\")\n+\ttargetDesc, outcomes, err := notation.Verify(context.TODO(), notationVerifier, parsedRef.Repo, remoteVerifyOptions)\n+\tif err != nil {\n+\t\tv.log.V(4).Info(\"failed to vefify attestator\", \"remoteVerifyOptions\", remoteVerifyOptions, \"repo\", parsedRef.Repo)\n+\t\treturn targetDesc, err\n+\t}\n+\tif err := v.verifyOutcomes(outcomes); err != nil {\n+\t\treturn targetDesc, err\n+\t}\n+\n+\tif targetDesc.Digest.String() != desc.Digest.String() {\n+\t\tv.log.V(4).Info(\"digest mismatch\", \"expected\", desc.Digest.String(), \"found\", targetDesc.Digest.String())\n+\t\treturn targetDesc, errors.Errorf(\"digest mismatch\")\n+\t}\n+\tv.log.V(2).Info(\"attestator verified\", \"desc\", targetDesc.Digest.String())\n+\n+\treturn targetDesc, nil\n+}\n+\n+func extractStatements(ctx context.Context, repoRef name.Reference, desc v1.Descriptor, craneOpts ...crane.Option) ([]map[string]interface{}, error) {\n+\tstatements := make([]map[string]interface{}, 0)\n+\tdata, err := extractStatement(ctx, repoRef, desc, craneOpts...)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\tstatements = append(statements, data)\n+\n+\tif len(statements) == 0 {\n+\t\treturn nil, fmt.Errorf(\"no statements found\")\n+\t}\n+\treturn statements, nil\n+}\n+\n+func extractStatement(ctx context.Context, repoRef name.Reference, desc v1.Descriptor, craneOpts ...crane.Option) (map[string]interface{}, error) {\n+\trefStr := repoRef.Context().RegistryStr() + \"/\" + repoRef.Context().RepositoryStr() + \"@\" + desc.Digest.String()\n+\tref, err := name.ParseReference(refStr)\n+\tif err != nil {\n+\t\treturn nil, errors.Wrapf(err, \"failed to parse image reference: %s\", refStr)\n+\t}\n+\n+\tmanifestBytes, err := crane.Manifest(refStr, craneOpts...)\n+\tif err != nil {\n+\t\treturn nil, fmt.Errorf(\"error in fetching statement: %w\", err)\n+\t}\n+\tvar manifest ocispec.Manifest\n+\tif err := json.Unmarshal(manifestBytes, &manifest); err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tif len(manifest.Layers) == 0 {\n+\t\treturn nil, fmt.Errorf(\"no predicate found: %+v\", manifest)\n+\t}\n+\tif len(manifest.Layers) > 1 {\n+\t\treturn nil, fmt.Errorf(\"multiple layers in predicate not supported: %+v\", manifest)\n+\t}\n+\tpredicateDesc := manifest.Layers[0]\n+\tpredicateRef := ref.Context().RegistryStr() + \"/\" + ref.Context().RepositoryStr() + \"@\" + predicateDesc.Digest.String()\n+\n+\tlayer, err := crane.PullLayer(predicateRef, craneOpts...)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\tioPredicate, err := layer.Uncompressed()\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\tpredicateBytes := new(bytes.Buffer)\n+\t_, err = predicateBytes.ReadFrom(ioPredicate)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tpredicate := make(map[string]interface{})\n+\tif err := json.Unmarshal(predicateBytes.Bytes(), &predicate); err != nil {\n+\t\treturn nil, err\n+\t}\n+\tdata := make(map[string]interface{})\n+\tif err := json.Unmarshal(manifestBytes, &data); err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tif data[\"type\"] == nil {\n+\t\tdata[\"type\"] = desc.ArtifactType\n+\t}\n+\tif data[\"predicate\"] == nil {\n+\t\tdata[\"predicate\"] = predicate\n+\t}\n+\treturn data, nil\n+}\n+\n+func matchArtifactType(ref v1.Descriptor, expectedArtifactType string) (bool, string, error) {\n+\tif expectedArtifactType != \"\" {\n+\t\tif ref.ArtifactType == expectedArtifactType {\n+\t\t\treturn true, ref.ArtifactType, nil\n+\t\t}\n+\t}\n+\treturn false, \"\", nil\n+}\ndiff --git a/pkg/notary/notary_test.go b/pkg/notary/notary_test.go\nnew file mode 100644\nindex 000000000000..dc16eea92eda\n--- /dev/null\n+++ b/pkg/notary/notary_test.go\n@@ -0,0 +1,33 @@\n+package notary\n+\n+import (\n+\t\"context\"\n+\t\"testing\"\n+\n+\t\"github.com/google/go-containerregistry/pkg/crane\"\n+\t\"github.com/google/go-containerregistry/pkg/name\"\n+\t\"github.com/google/go-containerregistry/pkg/v1/remote\"\n+\t\"gotest.tools/assert\"\n+)\n+\n+func TestExtractStatements(t *testing.T) {\n+\timageRef := \"jimnotarytest.azurecr.io/jim/net-monitor:v1\"\n+\tref, err := name.ParseReference(imageRef)\n+\tassert.NilError(t, err)\n+\trepoDesc, err := crane.Head(imageRef)\n+\tassert.NilError(t, err)\n+\treferrers, err := remote.Referrers(ref.Context().Digest(repoDesc.Digest.String()))\n+\tassert.NilError(t, err)\n+\treferrersDescs, err := referrers.IndexManifest()\n+\tassert.NilError(t, err)\n+\n+\tfor _, referrer := range referrersDescs.Manifests {\n+\t\tif referrer.ArtifactType == \"application/vnd.cncf.notary.signature\" {\n+\t\t\tstatements, err := extractStatements(context.Background(), ref, referrer)\n+\t\t\tassert.NilError(t, err)\n+\t\t\tassert.Assert(t, len(statements) == 1)\n+\t\t\tassert.Assert(t, statements[0][\"type\"] == referrer.ArtifactType)\n+\t\t\tassert.Assert(t, statements[0][\"mediaType\"] == string(referrer.MediaType))\n+\t\t}\n+\t}\n+}\ndiff --git a/pkg/notary/registry.go b/pkg/notary/registry.go\nnew file mode 100644\nindex 000000000000..6c0227dc1028\n--- /dev/null\n+++ b/pkg/notary/registry.go\n@@ -0,0 +1,133 @@\n+package notary\n+\n+import (\n+\t\"context\"\n+\t\"strings\"\n+\n+\t\"github.com/google/go-containerregistry/pkg/authn\"\n+\t\"github.com/google/go-containerregistry/pkg/crane\"\n+\t\"github.com/google/go-containerregistry/pkg/name\"\n+\tgcrremote \"github.com/google/go-containerregistry/pkg/v1/remote\"\n+\t\"github.com/kyverno/kyverno/pkg/registryclient\"\n+\tnotationregistry \"github.com/notaryproject/notation-go/registry\"\n+\tocispec \"github.com/opencontainers/image-spec/specs-go/v1\"\n+\t\"github.com/pkg/errors\"\n+)\n+\n+type parsedReference struct {\n+\tRepo       notationregistry.Repository\n+\tCraneOpts  crane.Option\n+\tRemoteOpts []gcrremote.Option\n+\tRef        name.Reference\n+\tDesc       ocispec.Descriptor\n+}\n+\n+func parseReferenceCrane(ctx context.Context, ref string, registryClient registryclient.Client) (*parsedReference, error) {\n+\tnameRef, err := name.ParseReference(ref)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tauthenticator, err := getAuthenticator(ctx, ref, registryClient)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tcraneOpts := crane.WithAuth(*authenticator)\n+\tremoteOpts, err := getRemoteOpts(*authenticator)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tdesc, err := crane.Head(ref, craneOpts)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tif !isDigestReference(ref) {\n+\t\tnameRef, err = name.ParseReference(GetReferenceFromDescriptor(v1ToOciSpecDescriptor(*desc), nameRef))\n+\t\tif err != nil {\n+\t\t\treturn nil, err\n+\t\t}\n+\t}\n+\n+\trepository := NewRepository(craneOpts, remoteOpts, nameRef)\n+\terr = resolveDigestCrane(repository, craneOpts, remoteOpts, nameRef)\n+\tif err != nil {\n+\t\treturn nil, errors.Wrapf(err, \"failed to resolve digest\")\n+\t}\n+\n+\treturn &parsedReference{\n+\t\tRepo:       repository,\n+\t\tCraneOpts:  craneOpts,\n+\t\tRemoteOpts: remoteOpts,\n+\t\tRef:        nameRef,\n+\t\tDesc:       v1ToOciSpecDescriptor(*desc),\n+\t}, nil\n+}\n+\n+type imageResource struct {\n+\tref name.Reference\n+}\n+\n+func (ir *imageResource) String() string {\n+\treturn ir.ref.Name()\n+}\n+\n+func (ir *imageResource) RegistryStr() string {\n+\treturn ir.ref.Context().RegistryStr()\n+}\n+\n+func getAuthenticator(ctx context.Context, ref string, registryClient registryclient.Client) (*authn.Authenticator, error) {\n+\tparsedRef, err := name.ParseReference(ref)\n+\tif err != nil {\n+\t\treturn nil, errors.Wrapf(err, \"failed to parse registry reference %s\", ref)\n+\t}\n+\n+\tif err := registryClient.RefreshKeychainPullSecrets(ctx); err != nil {\n+\t\treturn nil, errors.Wrapf(err, \"failed to refresh image pull secrets\")\n+\t}\n+\n+\tauthn, err := registryClient.Keychain().Resolve(&imageResource{parsedRef})\n+\tif err != nil {\n+\t\treturn nil, errors.Wrapf(err, \"failed to resolve auth for %s\", parsedRef.String())\n+\t}\n+\treturn &authn, nil\n+}\n+\n+func isDigestReference(reference string) bool {\n+\tparts := strings.SplitN(reference, \"/\", 2)\n+\tif len(parts) == 1 {\n+\t\treturn false\n+\t}\n+\n+\tindex := strings.Index(parts[1], \"@\")\n+\treturn index != -1\n+}\n+\n+func getRemoteOpts(authenticator authn.Authenticator) ([]gcrremote.Option, error) {\n+\tremoteOpts := []gcrremote.Option{}\n+\tremoteOpts = append(remoteOpts, gcrremote.WithAuth(authenticator))\n+\n+\tpusher, err := gcrremote.NewPusher(remoteOpts...)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\tremoteOpts = append(remoteOpts, gcrremote.Reuse(pusher))\n+\n+\tpuller, err := gcrremote.NewPuller(remoteOpts...)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\tremoteOpts = append(remoteOpts, gcrremote.Reuse(puller))\n+\n+\treturn remoteOpts, nil\n+}\n+\n+func resolveDigestCrane(repo notationregistry.Repository, craneOpts crane.Option, remoteOpts []gcrremote.Option, ref name.Reference) error {\n+\t_, err := repo.Resolve(context.Background(), ref.Name())\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\treturn nil\n+}\ndiff --git a/pkg/notary/repository.go b/pkg/notary/repository.go\nnew file mode 100644\nindex 000000000000..f8115f29d150\n--- /dev/null\n+++ b/pkg/notary/repository.go\n@@ -0,0 +1,127 @@\n+package notary\n+\n+import (\n+\t\"bytes\"\n+\t\"context\"\n+\t\"encoding/json\"\n+\t\"fmt\"\n+\n+\t\"github.com/google/go-containerregistry/pkg/crane\"\n+\t\"github.com/google/go-containerregistry/pkg/name\"\n+\tv1 \"github.com/google/go-containerregistry/pkg/v1\"\n+\t\"github.com/google/go-containerregistry/pkg/v1/remote\"\n+\tnotationregistry \"github.com/notaryproject/notation-go/registry\"\n+\t\"github.com/opencontainers/go-digest\"\n+\tocispec \"github.com/opencontainers/image-spec/specs-go/v1\"\n+)\n+\n+type repositoryClient struct {\n+\tref        name.Reference\n+\tcraneOpts  crane.Option\n+\tremoteOpts []remote.Option\n+}\n+\n+func NewRepository(craneOpts crane.Option, remoteOpts []remote.Option, ref name.Reference) notationregistry.Repository {\n+\treturn &repositoryClient{\n+\t\tcraneOpts:  craneOpts,\n+\t\tremoteOpts: remoteOpts,\n+\t\tref:        ref,\n+\t}\n+}\n+\n+func (c *repositoryClient) Resolve(ctx context.Context, reference string) (ocispec.Descriptor, error) {\n+\thead, err := crane.Head(reference)\n+\tif err != nil {\n+\t\treturn ocispec.Descriptor{}, nil\n+\t}\n+\tdescriptor := v1ToOciSpecDescriptor(*head)\n+\treturn descriptor, nil\n+}\n+\n+func (c *repositoryClient) ListSignatures(ctx context.Context, desc ocispec.Descriptor, fn func(signatureManifests []ocispec.Descriptor) error) error {\n+\treferrers, err := remote.Referrers(c.ref.Context().Digest(desc.Digest.String()), c.remoteOpts...)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\treferrersDescs, err := referrers.IndexManifest()\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\n+\tdescList := []ocispec.Descriptor{}\n+\tfor _, d := range referrersDescs.Manifests {\n+\t\tif d.ArtifactType == notationregistry.ArtifactTypeNotation {\n+\t\t\tdescList = append(descList, v1ToOciSpecDescriptor(d))\n+\t\t}\n+\t}\n+\n+\treturn fn(descList)\n+}\n+\n+func (c *repositoryClient) FetchSignatureBlob(ctx context.Context, desc ocispec.Descriptor) ([]byte, ocispec.Descriptor, error) {\n+\tmanifestRef := c.getReferenceFromDescriptor(desc)\n+\n+\tmanifestBytes, err := crane.Manifest(manifestRef)\n+\tif err != nil {\n+\t\treturn nil, ocispec.Descriptor{}, err\n+\t}\n+\n+\tvar manifest ocispec.Manifest\n+\tif err := json.Unmarshal(manifestBytes, &manifest); err != nil {\n+\t\treturn nil, ocispec.Descriptor{}, err\n+\t}\n+\tmanifestDesc := manifest.Layers[0]\n+\n+\tsignatureBlobRef := c.getReferenceFromDescriptor(manifestDesc)\n+\n+\tsignatureBlobLayer, err := crane.PullLayer(signatureBlobRef)\n+\tif err != nil {\n+\t\tpanic(err)\n+\t}\n+\n+\tio, err := signatureBlobLayer.Uncompressed()\n+\tif err != nil {\n+\t\tpanic(err)\n+\t}\n+\tSigBlobBuf := new(bytes.Buffer)\n+\n+\t_, err = SigBlobBuf.ReadFrom(io)\n+\tif err != nil {\n+\t\tpanic(err)\n+\t}\n+\treturn SigBlobBuf.Bytes(), manifestDesc, nil\n+}\n+\n+func (c *repositoryClient) PushSignature(ctx context.Context, mediaType string, blob []byte, subject ocispec.Descriptor, annotations map[string]string) (blobDesc, manifestDesc ocispec.Descriptor, err error) {\n+\treturn ocispec.Descriptor{}, ocispec.Descriptor{}, fmt.Errorf(\"push signature is not implemented\")\n+}\n+\n+func v1ToOciSpecDescriptor(v1desc v1.Descriptor) ocispec.Descriptor {\n+\tociDesc := ocispec.Descriptor{\n+\t\tMediaType:   string(v1desc.MediaType),\n+\t\tDigest:      digest.Digest(v1desc.Digest.String()),\n+\t\tSize:        v1desc.Size,\n+\t\tURLs:        v1desc.URLs,\n+\t\tAnnotations: v1desc.Annotations,\n+\t\tData:        v1desc.Data,\n+\n+\t\tArtifactType: v1desc.ArtifactType,\n+\t}\n+\tif v1desc.Platform != nil {\n+\t\tociDesc.Platform = &ocispec.Platform{\n+\t\t\tArchitecture: v1desc.Platform.Architecture,\n+\t\t\tOS:           v1desc.Platform.OS,\n+\t\t\tOSVersion:    v1desc.Platform.OSVersion,\n+\t\t}\n+\t}\n+\treturn ociDesc\n+}\n+\n+func (c *repositoryClient) getReferenceFromDescriptor(desc ocispec.Descriptor) string {\n+\treturn GetReferenceFromDescriptor(desc, c.ref)\n+}\n+\n+func GetReferenceFromDescriptor(desc ocispec.Descriptor, ref name.Reference) string {\n+\treturn ref.Context().RegistryStr() + \"/\" + ref.Context().RepositoryStr() + \"@\" + desc.Digest.String()\n+}\ndiff --git a/pkg/notaryv2/truststore.go b/pkg/notary/truststore.go\nsimilarity index 97%\nrename from pkg/notaryv2/truststore.go\nrename to pkg/notary/truststore.go\nindex 899d70574eb5..a5c561822aa1 100644\n--- a/pkg/notaryv2/truststore.go\n+++ b/pkg/notary/truststore.go\n@@ -1,4 +1,4 @@\n-package notaryv2\n+package notary\n \n import (\n \t\"context\"\ndiff --git a/pkg/notaryv2/notaryv2.go b/pkg/notaryv2/notaryv2.go\ndeleted file mode 100644\nindex 414efdd1720b..000000000000\n--- a/pkg/notaryv2/notaryv2.go\n+++ /dev/null\n@@ -1,132 +0,0 @@\n-package notaryv2\n-\n-import (\n-\t\"bytes\"\n-\t\"context\"\n-\n-\t\"github.com/go-logr/logr\"\n-\t\"github.com/kyverno/kyverno/pkg/images\"\n-\t\"github.com/kyverno/kyverno/pkg/logging\"\n-\t_ \"github.com/notaryproject/notation-core-go/signature/cose\"\n-\t_ \"github.com/notaryproject/notation-core-go/signature/jws\"\n-\t\"github.com/notaryproject/notation-go\"\n-\t\"github.com/notaryproject/notation-go/verifier\"\n-\t\"github.com/notaryproject/notation-go/verifier/trustpolicy\"\n-\t\"github.com/pkg/errors\"\n-\t\"github.com/sigstore/sigstore/pkg/cryptoutils\"\n-\t\"go.uber.org/multierr\"\n-)\n-\n-func NewVerifier() images.ImageVerifier {\n-\treturn &notaryV2Verifier{\n-\t\tlog: logging.WithName(\"NotaryV2\"),\n-\t}\n-}\n-\n-type notaryV2Verifier struct {\n-\tlog logr.Logger\n-}\n-\n-func (v *notaryV2Verifier) VerifySignature(ctx context.Context, opts images.Options) (*images.Response, error) {\n-\tv.log.V(2).Info(\"verifying image\", \"reference\", opts.ImageRef)\n-\n-\tcertsPEM := combineCerts(opts)\n-\tcerts, err := cryptoutils.LoadCertificatesFromPEM(bytes.NewReader([]byte(certsPEM)))\n-\tif err != nil {\n-\t\treturn nil, errors.Wrapf(err, \"failed to parse certificates\")\n-\t}\n-\n-\ttrustStore := NewTrustStore(\"kyverno\", certs)\n-\tpolicyDoc := v.buildPolicy()\n-\tnotationVerifier, err := verifier.New(policyDoc, trustStore, nil)\n-\tif err != nil {\n-\t\treturn nil, errors.Wrapf(err, \"failed to created verifier\")\n-\t}\n-\n-\trepo, parsedRef, err := parseReference(ctx, opts.ImageRef, opts.RegistryClient)\n-\tif err != nil {\n-\t\treturn nil, errors.Wrapf(err, \"failed to parse image reference: %s\", opts.ImageRef)\n-\t}\n-\n-\tdigest, err := parsedRef.Digest()\n-\tif err != nil {\n-\t\treturn nil, errors.Wrapf(err, \"failed to fetch digest\")\n-\t}\n-\n-\tref := parsedRef.String()\n-\tremoteVerifyOptions := notation.RemoteVerifyOptions{\n-\t\tArtifactReference:    ref,\n-\t\tMaxSignatureAttempts: 10,\n-\t}\n-\n-\ttargetDesc, outcomes, err := notation.Verify(context.TODO(), notationVerifier, repo, remoteVerifyOptions)\n-\tif err != nil {\n-\t\treturn nil, errors.Wrapf(err, \"failed to verify %s\", ref)\n-\t}\n-\n-\tif err := v.verifyOutcomes(outcomes); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif targetDesc.Digest != digest {\n-\t\treturn nil, errors.Errorf(\"digest mismatch\")\n-\t}\n-\n-\tv.log.V(2).Info(\"verified image\", \"type\", targetDesc.MediaType, \"digest\", targetDesc.Digest, \"size\", targetDesc.Size)\n-\n-\tresp := &images.Response{\n-\t\tDigest:     targetDesc.Digest.String(),\n-\t\tStatements: nil,\n-\t}\n-\n-\treturn resp, nil\n-}\n-\n-func combineCerts(opts images.Options) string {\n-\tcerts := opts.Cert\n-\tif opts.CertChain != \"\" {\n-\t\tif certs != \"\" {\n-\t\t\tcerts = certs + \"\\n\"\n-\t\t}\n-\n-\t\tcerts = certs + opts.CertChain\n-\t}\n-\n-\treturn certs\n-}\n-\n-func (v *notaryV2Verifier) buildPolicy() *trustpolicy.Document {\n-\treturn &trustpolicy.Document{\n-\t\tVersion: \"1.0\",\n-\t\tTrustPolicies: []trustpolicy.TrustPolicy{\n-\t\t\t{\n-\t\t\t\tName:                  \"kyverno\",\n-\t\t\t\tRegistryScopes:        []string{\"*\"},\n-\t\t\t\tSignatureVerification: trustpolicy.SignatureVerification{VerificationLevel: trustpolicy.LevelStrict.Name},\n-\t\t\t\tTrustStores:           []string{\"ca:kyverno\"},\n-\t\t\t\tTrustedIdentities:     []string{\"*\"},\n-\t\t\t},\n-\t\t},\n-\t}\n-}\n-\n-func (v *notaryV2Verifier) verifyOutcomes(outcomes []*notation.VerificationOutcome) error {\n-\tvar errs []error\n-\tfor _, outcome := range outcomes {\n-\t\tif outcome.Error != nil {\n-\t\t\terrs = append(errs, outcome.Error)\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tcontent := outcome.EnvelopeContent.Payload.Content\n-\t\tcontentType := outcome.EnvelopeContent.Payload.ContentType\n-\n-\t\tv.log.Info(\"content\", \"type\", contentType, \"data\", content)\n-\t}\n-\n-\treturn multierr.Combine(errs...)\n-}\n-\n-func (v *notaryV2Verifier) FetchAttestations(ctx context.Context, opts images.Options) (*images.Response, error) {\n-\treturn nil, errors.Errorf(\"not implemented\")\n-}\ndiff --git a/pkg/notaryv2/registry.go b/pkg/notaryv2/registry.go\ndeleted file mode 100644\nindex 71e63b94acf6..000000000000\n--- a/pkg/notaryv2/registry.go\n+++ /dev/null\n@@ -1,125 +0,0 @@\n-package notaryv2\n-\n-import (\n-\t\"context\"\n-\t\"strings\"\n-\n-\t\"github.com/kyverno/kyverno/pkg/registryclient\"\n-\tnotationregistry \"github.com/notaryproject/notation-go/registry\"\n-\tocispec \"github.com/opencontainers/image-spec/specs-go/v1\"\n-\t\"github.com/pkg/errors\"\n-\t\"oras.land/oras-go/v2/registry\"\n-\t\"oras.land/oras-go/v2/registry/remote\"\n-\t\"oras.land/oras-go/v2/registry/remote/auth\"\n-)\n-\n-func parseReference(ctx context.Context, ref string, registryClient registryclient.Client) (notationregistry.Repository, registry.Reference, error) {\n-\tparsedRef, err := registry.ParseReference(ref)\n-\tif err != nil {\n-\t\treturn nil, registry.Reference{}, errors.Wrapf(err, \"failed to parse registry reference %s\", ref)\n-\t}\n-\n-\tauthClient, plainHTTP, err := getAuthClient(ctx, parsedRef, registryClient)\n-\tif err != nil {\n-\t\treturn nil, registry.Reference{}, err\n-\t}\n-\n-\trepo, err := remote.NewRepository(ref)\n-\tif err != nil {\n-\t\treturn nil, registry.Reference{}, errors.Wrapf(err, \"failed to initialize repository\")\n-\t}\n-\n-\trepo.PlainHTTP = plainHTTP\n-\trepo.Client = authClient\n-\trepository := notationregistry.NewRepository(repo)\n-\n-\tparsedRef, err = resolveDigest(repository, parsedRef)\n-\tif err != nil {\n-\t\treturn nil, registry.Reference{}, errors.Wrapf(err, \"failed to resolve digest\")\n-\t}\n-\n-\treturn repository, parsedRef, nil\n-}\n-\n-type imageResource struct {\n-\tref registry.Reference\n-}\n-\n-func (ir *imageResource) String() string {\n-\treturn ir.ref.String()\n-}\n-\n-func (ir *imageResource) RegistryStr() string {\n-\treturn ir.ref.Registry\n-}\n-\n-func getAuthClient(ctx context.Context, ref registry.Reference, rc registryclient.Client) (*auth.Client, bool, error) {\n-\tif err := rc.RefreshKeychainPullSecrets(ctx); err != nil {\n-\t\treturn nil, false, errors.Wrapf(err, \"failed to refresh image pull secrets\")\n-\t}\n-\n-\tauthn, err := rc.Keychain().Resolve(&imageResource{ref})\n-\tif err != nil {\n-\t\treturn nil, false, errors.Wrapf(err, \"failed to resolve auth for %s\", ref.String())\n-\t}\n-\n-\tauthConfig, err := authn.Authorization()\n-\tif err != nil {\n-\t\treturn nil, false, errors.Wrapf(err, \"failed to get auth config for %s\", ref.String())\n-\t}\n-\n-\tcredentials := auth.Credential{\n-\t\tUsername:     authConfig.Username,\n-\t\tPassword:     authConfig.Password,\n-\t\tAccessToken:  authConfig.IdentityToken,\n-\t\tRefreshToken: authConfig.RegistryToken,\n-\t}\n-\n-\tauthClient := &auth.Client{\n-\t\tCredential: func(ctx context.Context, registry string) (auth.Credential, error) {\n-\t\t\tswitch registry {\n-\t\t\tdefault:\n-\t\t\t\treturn credentials, nil\n-\t\t\t}\n-\t\t},\n-\t\tCache:    auth.NewCache(),\n-\t\tClientID: \"notation\",\n-\t}\n-\n-\tauthClient.SetUserAgent(\"kyverno.io\")\n-\treturn authClient, false, nil\n-}\n-\n-func resolveDigest(repo notationregistry.Repository, ref registry.Reference) (registry.Reference, error) {\n-\tif isDigestReference(ref.String()) {\n-\t\treturn ref, nil\n-\t}\n-\n-\t// Resolve tag reference to digest reference.\n-\tmanifestDesc, err := getManifestDescriptorFromReference(repo, ref.String())\n-\tif err != nil {\n-\t\treturn registry.Reference{}, err\n-\t}\n-\n-\tref.Reference = manifestDesc.Digest.String()\n-\treturn ref, nil\n-}\n-\n-func isDigestReference(reference string) bool {\n-\tparts := strings.SplitN(reference, \"/\", 2)\n-\tif len(parts) == 1 {\n-\t\treturn false\n-\t}\n-\n-\tindex := strings.Index(parts[1], \"@\")\n-\treturn index != -1\n-}\n-\n-func getManifestDescriptorFromReference(repo notationregistry.Repository, reference string) (ocispec.Descriptor, error) {\n-\tref, err := registry.ParseReference(reference)\n-\tif err != nil {\n-\t\treturn ocispec.Descriptor{}, err\n-\t}\n-\n-\treturn repo.Resolve(context.Background(), ref.ReferenceOrDefault())\n-}\ndiff --git a/pkg/validation/policy/validate.go b/pkg/validation/policy/validate.go\nindex f7504198d7e8..0159c1290fd7 100644\n--- a/pkg/validation/policy/validate.go\n+++ b/pkg/validation/policy/validate.go\n@@ -364,6 +364,10 @@ func Validate(policy, oldPolicy kyvernov1.PolicyInterface, client dclient.Interf\n \t\t\tcheckForScaleSubresource(mutationJson, allKinds, &warnings)\n \t\t\tcheckForStatusSubresource(mutationJson, allKinds, &warnings)\n \t\t}\n+\n+\t\tif rule.HasVerifyImages() {\n+\t\t\tcheckForDeprecatedFieldsInVerifyImages(rule, &warnings)\n+\t\t}\n \t}\n \tif !mock && (spec.SchemaValidation == nil || *spec.SchemaValidation) {\n \t\tif err := openApiManager.ValidatePolicyMutation(policy); err != nil {\n@@ -1301,3 +1305,14 @@ func checkForStatusSubresource(ruleTypeJson []byte, allKinds []string, warnings\n \t\t*warnings = append(*warnings, msg)\n \t}\n }\n+\n+func checkForDeprecatedFieldsInVerifyImages(rule kyvernov1.Rule, warnings *[]string) {\n+\tfor _, imageVerify := range rule.VerifyImages {\n+\t\tfor _, attestation := range imageVerify.Attestations {\n+\t\t\tif attestation.PredicateType != \"\" {\n+\t\t\t\tmsg := fmt.Sprintf(\"predicateType has been deprecated use 'type: %s' instead of 'prediacteType: %s'\", attestation.PredicateType, attestation.PredicateType)\n+\t\t\t\t*warnings = append(*warnings, msg)\n+\t\t\t}\n+\t\t}\n+\t}\n+}\ndiff --git a/test/conformance/kuttl/flags/standard/emit-events/admission-controller-assert.yaml b/test/conformance/kuttl/flags/standard/emit-events/admission-controller-assert.yaml\nindex 4ed694ab26c5..c12a6220a38b 100644\n--- a/test/conformance/kuttl/flags/standard/emit-events/admission-controller-assert.yaml\n+++ b/test/conformance/kuttl/flags/standard/emit-events/admission-controller-assert.yaml\n@@ -1,9 +1,8 @@\n-apiVersion: kyverno.io/v1\n-kind: Policy\n+apiVersion: apps/v1\n+kind: Deployment\n metadata:\n   name: kyverno-admission-controller\n+  namespace: kyverno\n status:\n-  conditions:\n-    - reason: Succeeded\n-      status: \"True\"\n-      type: Ready\n+  readyReplicas: 1\n+  updatedReplicas: 1\n\\ No newline at end of file\ndiff --git a/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-attestation-verification/01-policy.yaml b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-attestation-verification/01-policy.yaml\nnew file mode 100644\nindex 000000000000..f3857739b00f\n--- /dev/null\n+++ b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-attestation-verification/01-policy.yaml\n@@ -0,0 +1,6 @@\n+apiVersion: kuttl.dev/v1beta1\n+kind: TestStep\n+apply:\n+- policy.yaml\n+assert:\n+- policy-ready.yaml\n\\ No newline at end of file\ndiff --git a/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-attestation-verification/02-resource.yaml b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-attestation-verification/02-resource.yaml\nnew file mode 100644\nindex 000000000000..52ffd92005b6\n--- /dev/null\n+++ b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-attestation-verification/02-resource.yaml\n@@ -0,0 +1,6 @@\n+apiVersion: kuttl.dev/v1beta1\n+kind: TestStep\n+apply:\n+- pod.yaml\n+assert:\n+- pod-assert.yaml\n\\ No newline at end of file\ndiff --git a/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-attestation-verification/README.md b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-attestation-verification/README.md\nnew file mode 100644\nindex 000000000000..b246475edad1\n--- /dev/null\n+++ b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-attestation-verification/README.md\n@@ -0,0 +1,12 @@\n+## Description\n+\n+This test verifies image attestations using notary signatures\n+\n+## Expected Behavior\n+\n+This test creates a cluster policy.\n+When a pod is created with the image reference and the signature on attestations matches, the pod creation is successful\n+\n+## Reference Issue(s)\n+\n+6142\ndiff --git a/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-attestation-verification/pod-assert.yaml b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-attestation-verification/pod-assert.yaml\nnew file mode 100644\nindex 000000000000..d18a0a10e95e\n--- /dev/null\n+++ b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-attestation-verification/pod-assert.yaml\n@@ -0,0 +1,5 @@\n+apiVersion: v1\n+kind: Pod\n+metadata:\n+  name: test\n+  namespace: notary-verify-attestation\n\\ No newline at end of file\ndiff --git a/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-attestation-verification/pod.yaml b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-attestation-verification/pod.yaml\nnew file mode 100644\nindex 000000000000..e16637872d04\n--- /dev/null\n+++ b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-attestation-verification/pod.yaml\n@@ -0,0 +1,16 @@\n+apiVersion: v1\n+kind: Pod\n+metadata:\n+  creationTimestamp: null\n+  labels:\n+    run: test\n+  name: test\n+  namespace: notary-verify-attestation\n+spec:\n+  containers:\n+  - image: ghcr.io/kyverno/test-verify-image:signed\n+    name: test\n+    resources: {}\n+  dnsPolicy: ClusterFirst\n+  restartPolicy: Always\n+status: {}\n\\ No newline at end of file\ndiff --git a/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-attestation-verification/policy-ready.yaml b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-attestation-verification/policy-ready.yaml\nnew file mode 100644\nindex 000000000000..83c51e70577c\n--- /dev/null\n+++ b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-attestation-verification/policy-ready.yaml\n@@ -0,0 +1,9 @@\n+apiVersion: kyverno.io/v2beta1\n+kind: ClusterPolicy\n+metadata:\n+  name: check-image-attestation\n+status:\n+  conditions:\n+  - reason: Succeeded\n+    status: \"True\"\n+    type: Ready\n\\ No newline at end of file\ndiff --git a/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-attestation-verification/policy.yaml b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-attestation-verification/policy.yaml\nnew file mode 100644\nindex 000000000000..25245849deb8\n--- /dev/null\n+++ b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-attestation-verification/policy.yaml\n@@ -0,0 +1,63 @@\n+apiVersion: v1\n+kind: Namespace\n+metadata:\n+  name: notary-verify-attestation\n+---\n+apiVersion: v1\n+kind: ConfigMap\n+metadata:\n+  name: keys\n+  namespace: notary-verify-attestation\n+data:\n+  certificate: |-\n+    -----BEGIN CERTIFICATE-----\n+    MIIDTTCCAjWgAwIBAgIJAPI+zAzn4s0xMA0GCSqGSIb3DQEBCwUAMEwxCzAJBgNV\n+    BAYTAlVTMQswCQYDVQQIDAJXQTEQMA4GA1UEBwwHU2VhdHRsZTEPMA0GA1UECgwG\n+    Tm90YXJ5MQ0wCwYDVQQDDAR0ZXN0MB4XDTIzMDUyMjIxMTUxOFoXDTMzMDUxOTIx\n+    MTUxOFowTDELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAldBMRAwDgYDVQQHDAdTZWF0\n+    dGxlMQ8wDQYDVQQKDAZOb3RhcnkxDTALBgNVBAMMBHRlc3QwggEiMA0GCSqGSIb3\n+    DQEBAQUAA4IBDwAwggEKAoIBAQDNhTwv+QMk7jEHufFfIFlBjn2NiJaYPgL4eBS+\n+    b+o37ve5Zn9nzRppV6kGsa161r9s2KkLXmJrojNy6vo9a6g6RtZ3F6xKiWLUmbAL\n+    hVTCfYw/2n7xNlVMjyyUpE+7e193PF8HfQrfDFxe2JnX5LHtGe+X9vdvo2l41R6m\n+    Iia04DvpMdG4+da2tKPzXIuLUz/FDb6IODO3+qsqQLwEKmmUee+KX+3yw8I6G1y0\n+    Vp0mnHfsfutlHeG8gazCDlzEsuD4QJ9BKeRf2Vrb0ywqNLkGCbcCWF2H5Q80Iq/f\n+    ETVO9z88R7WheVdEjUB8UrY7ZMLdADM14IPhY2Y+tLaSzEVZAgMBAAGjMjAwMAkG\n+    A1UdEwQCMAAwDgYDVR0PAQH/BAQDAgeAMBMGA1UdJQQMMAoGCCsGAQUFBwMDMA0G\n+    CSqGSIb3DQEBCwUAA4IBAQBX7x4Ucre8AIUmXZ5PUK/zUBVOrZZzR1YE8w86J4X9\n+    kYeTtlijf9i2LTZMfGuG0dEVFN4ae3CCpBst+ilhIndnoxTyzP+sNy4RCRQ2Y/k8\n+    Zq235KIh7uucq96PL0qsF9s2RpTKXxyOGdtp9+HO0Ty5txJE2txtLDUIVPK5WNDF\n+    ByCEQNhtHgN6V20b8KU2oLBZ9vyB8V010dQz0NRTDLhkcvJig00535/LUylECYAJ\n+    5/jn6XKt6UYCQJbVNzBg/YPGc1RF4xdsGVDBben/JXpeGEmkdmXPILTKd9tZ5TC0\n+    uOKpF5rWAruB5PCIrquamOejpXV9aQA/K2JQDuc0mcKz\n+    -----END CERTIFICATE-----\n+---\n+apiVersion: kyverno.io/v1\n+kind: ClusterPolicy\n+metadata:\n+  name: check-image-attestation\n+spec:\n+  validationFailureAction: Enforce\n+  webhookTimeoutSeconds: 30\n+  failurePolicy: Fail  \n+  rules:\n+    - name: verify-attestation-notary\n+      match:\n+        any:\n+        - resources:\n+            kinds:\n+              - Pod\n+      context:\n+      - name: keys\n+        configMap:\n+          name: keys\n+          namespace: notary-verify-attestation\n+      verifyImages:\n+      - type: Notary\n+        imageReferences:\n+          - \"ghcr.io/kyverno/test-verify-image*\"\n+        attestations:\n+          - type: sbom/cyclone-dx\n+            attestors:\n+            - entries:\n+              - certificates: \n+                  cert: \"{{ keys.data.certificate }}\"\n\\ No newline at end of file\ndiff --git a/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-image-verification/01-policy.yaml b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-image-verification/01-policy.yaml\nnew file mode 100644\nindex 000000000000..f3857739b00f\n--- /dev/null\n+++ b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-image-verification/01-policy.yaml\n@@ -0,0 +1,6 @@\n+apiVersion: kuttl.dev/v1beta1\n+kind: TestStep\n+apply:\n+- policy.yaml\n+assert:\n+- policy-ready.yaml\n\\ No newline at end of file\ndiff --git a/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-image-verification/02-resource.yaml b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-image-verification/02-resource.yaml\nnew file mode 100644\nindex 000000000000..52ffd92005b6\n--- /dev/null\n+++ b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-image-verification/02-resource.yaml\n@@ -0,0 +1,6 @@\n+apiVersion: kuttl.dev/v1beta1\n+kind: TestStep\n+apply:\n+- pod.yaml\n+assert:\n+- pod-assert.yaml\n\\ No newline at end of file\ndiff --git a/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-image-verification/README.md b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-image-verification/README.md\nnew file mode 100644\nindex 000000000000..a87ff91a4a56\n--- /dev/null\n+++ b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-image-verification/README.md\n@@ -0,0 +1,12 @@\n+## Description\n+\n+This test verifies images using notary signatures\n+\n+## Expected Behavior\n+\n+This test creates a cluster policy.\n+When a pod is created with the image reference and the signature matches, the pod creation is successful\n+\n+## Reference Issue(s)\n+\n+6142\ndiff --git a/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-image-verification/pod-assert.yaml b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-image-verification/pod-assert.yaml\nnew file mode 100644\nindex 000000000000..4bf9852a3c8a\n--- /dev/null\n+++ b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-image-verification/pod-assert.yaml\n@@ -0,0 +1,5 @@\n+apiVersion: v1\n+kind: Pod\n+metadata:\n+  name: test\n+  namespace: notary-verify-images\n\\ No newline at end of file\ndiff --git a/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-image-verification/pod.yaml b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-image-verification/pod.yaml\nnew file mode 100644\nindex 000000000000..b5ab8f795900\n--- /dev/null\n+++ b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-image-verification/pod.yaml\n@@ -0,0 +1,16 @@\n+apiVersion: v1\n+kind: Pod\n+metadata:\n+  creationTimestamp: null\n+  labels:\n+    run: test\n+  name: test\n+  namespace: notary-verify-images\n+spec:\n+  containers:\n+  - image: ghcr.io/kyverno/test-verify-image:signed\n+    name: test\n+    resources: {}\n+  dnsPolicy: ClusterFirst\n+  restartPolicy: Always\n+status: {}\n\\ No newline at end of file\ndiff --git a/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-image-verification/policy-ready.yaml b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-image-verification/policy-ready.yaml\nnew file mode 100644\nindex 000000000000..b3ad396d269f\n--- /dev/null\n+++ b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-image-verification/policy-ready.yaml\n@@ -0,0 +1,9 @@\n+apiVersion: kyverno.io/v2beta1\n+kind: ClusterPolicy\n+metadata:\n+  name: check-image-notary\n+status:\n+  conditions:\n+  - reason: Succeeded\n+    status: \"True\"\n+    type: Ready\n\\ No newline at end of file\ndiff --git a/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-image-verification/policy.yaml b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-image-verification/policy.yaml\nnew file mode 100644\nindex 000000000000..05d6d6311c87\n--- /dev/null\n+++ b/test/conformance/kuttl/verifyImages/clusterpolicy/standard/notary-image-verification/policy.yaml\n@@ -0,0 +1,62 @@\n+apiVersion: v1\n+kind: Namespace\n+metadata:\n+  name: notary-verify-images\n+---\n+apiVersion: v1\n+kind: ConfigMap\n+metadata:\n+  name: keys\n+  namespace: notary-verify-images\n+data:\n+  certificate: |-\n+    -----BEGIN CERTIFICATE-----\n+    MIIDTTCCAjWgAwIBAgIJAPI+zAzn4s0xMA0GCSqGSIb3DQEBCwUAMEwxCzAJBgNV\n+    BAYTAlVTMQswCQYDVQQIDAJXQTEQMA4GA1UEBwwHU2VhdHRsZTEPMA0GA1UECgwG\n+    Tm90YXJ5MQ0wCwYDVQQDDAR0ZXN0MB4XDTIzMDUyMjIxMTUxOFoXDTMzMDUxOTIx\n+    MTUxOFowTDELMAkGA1UEBhMCVVMxCzAJBgNVBAgMAldBMRAwDgYDVQQHDAdTZWF0\n+    dGxlMQ8wDQYDVQQKDAZOb3RhcnkxDTALBgNVBAMMBHRlc3QwggEiMA0GCSqGSIb3\n+    DQEBAQUAA4IBDwAwggEKAoIBAQDNhTwv+QMk7jEHufFfIFlBjn2NiJaYPgL4eBS+\n+    b+o37ve5Zn9nzRppV6kGsa161r9s2KkLXmJrojNy6vo9a6g6RtZ3F6xKiWLUmbAL\n+    hVTCfYw/2n7xNlVMjyyUpE+7e193PF8HfQrfDFxe2JnX5LHtGe+X9vdvo2l41R6m\n+    Iia04DvpMdG4+da2tKPzXIuLUz/FDb6IODO3+qsqQLwEKmmUee+KX+3yw8I6G1y0\n+    Vp0mnHfsfutlHeG8gazCDlzEsuD4QJ9BKeRf2Vrb0ywqNLkGCbcCWF2H5Q80Iq/f\n+    ETVO9z88R7WheVdEjUB8UrY7ZMLdADM14IPhY2Y+tLaSzEVZAgMBAAGjMjAwMAkG\n+    A1UdEwQCMAAwDgYDVR0PAQH/BAQDAgeAMBMGA1UdJQQMMAoGCCsGAQUFBwMDMA0G\n+    CSqGSIb3DQEBCwUAA4IBAQBX7x4Ucre8AIUmXZ5PUK/zUBVOrZZzR1YE8w86J4X9\n+    kYeTtlijf9i2LTZMfGuG0dEVFN4ae3CCpBst+ilhIndnoxTyzP+sNy4RCRQ2Y/k8\n+    Zq235KIh7uucq96PL0qsF9s2RpTKXxyOGdtp9+HO0Ty5txJE2txtLDUIVPK5WNDF\n+    ByCEQNhtHgN6V20b8KU2oLBZ9vyB8V010dQz0NRTDLhkcvJig00535/LUylECYAJ\n+    5/jn6XKt6UYCQJbVNzBg/YPGc1RF4xdsGVDBben/JXpeGEmkdmXPILTKd9tZ5TC0\n+    uOKpF5rWAruB5PCIrquamOejpXV9aQA/K2JQDuc0mcKz\n+    -----END CERTIFICATE-----\n+---\n+apiVersion: kyverno.io/v2beta1\n+kind: ClusterPolicy\n+metadata:\n+  name: check-image-notary\n+spec:\n+  validationFailureAction: Enforce\n+  webhookTimeoutSeconds: 30\n+  failurePolicy: Fail  \n+  rules:\n+    - name: verify-signature-notary\n+      context:\n+      - name: keys\n+        configMap:\n+          name: keys\n+          namespace: notary-verify-images\n+      match:\n+        any:\n+        - resources:\n+            kinds:\n+              - Pod\n+      verifyImages:\n+      - type: Notary\n+        imageReferences:\n+        - \"ghcr.io/kyverno/test-verify-image*\"\n+        attestors:\n+        - count: 1\n+          entries:\n+          - certificates:\n+              cert: \"{{ keys.data.certificate }}\"\n\\ No newline at end of file\n"
  },
  "CWE-401": {
    "cve": "CVE-2025-64329",
    "commit_url": "https://github.com/containerd/containerd/commit/083b53cd6f19b5de7717b0ce92c11bdf95e612df",
    "diff": "diff --git a/internal/cri/io/container_io.go b/internal/cri/io/container_io.go\nindex 9fc5545346cc..194634ec1be2 100644\n--- a/internal/cri/io/container_io.go\n+++ b/internal/cri/io/container_io.go\n@@ -17,6 +17,7 @@\n package io\n \n import (\n+\t\"context\"\n \t\"errors\"\n \t\"fmt\"\n \t\"io\"\n@@ -160,7 +161,7 @@ func (c *ContainerIO) Pipe() {\n \n // Attach attaches container stdio.\n // TODO(random-liu): Use pools.Copy in docker to reduce memory usage?\n-func (c *ContainerIO) Attach(opts AttachOptions) {\n+func (c *ContainerIO) Attach(ctx context.Context, opts AttachOptions) {\n \tvar wg sync.WaitGroup\n \tkey := util.GenerateID()\n \tstdinKey := streamKey(c.id, \"attach-\"+key, Stdin)\n@@ -201,8 +202,15 @@ func (c *ContainerIO) Attach(opts AttachOptions) {\n \t}\n \n \tattachStream := func(key string, close <-chan struct{}) {\n-\t\t<-close\n-\t\tlog.L.Infof(\"Attach stream %q closed\", key)\n+\t\tselect {\n+\t\tcase <-close:\n+\t\t\tlog.L.Infof(\"Attach stream %q closed\", key)\n+\t\tcase <-ctx.Done():\n+\t\t\tlog.L.Infof(\"Attach client of %q cancelled\", key)\n+\t\t\t// Avoid writeGroup heap up\n+\t\t\tc.stdoutGroup.Remove(key)\n+\t\t\tc.stderrGroup.Remove(key)\n+\t\t}\n \t\t// Make sure stdin gets closed.\n \t\tif stdinStreamRC != nil {\n \t\t\tstdinStreamRC.Close()\ndiff --git a/internal/cri/server/container_attach.go b/internal/cri/server/container_attach.go\nindex 01478590ca7e..f4c33227298e 100644\n--- a/internal/cri/server/container_attach.go\n+++ b/internal/cri/server/container_attach.go\n@@ -82,6 +82,6 @@ func (c *criService) attachContainer(ctx context.Context, id string, stdin io.Re\n \t\t},\n \t}\n \t// TODO(random-liu): Figure out whether we need to support historical output.\n-\tcntr.IO.Attach(opts)\n+\tcntr.IO.Attach(ctx, opts)\n \treturn nil\n }\n"
  },
  "CWE-193": {
    "cve": "CVE-2025-43971",
    "commit_url": "https://github.com/osrg/gobgp/commit/08a001e06d90e8bcc190084c66992f46f62c0986",
    "diff": "diff --git a/pkg/packet/bgp/bgp.go b/pkg/packet/bgp/bgp.go\nindex 2655f750e..17211f124 100644\n--- a/pkg/packet/bgp/bgp.go\n+++ b/pkg/packet/bgp/bgp.go\n@@ -1094,7 +1094,7 @@ func (c *CapSoftwareVersion) DecodeFromBytes(data []byte) error {\n \t\treturn NewMessageError(BGP_ERROR_OPEN_MESSAGE_ERROR, BGP_ERROR_SUB_UNSUPPORTED_CAPABILITY, nil, \"Not all CapabilitySoftwareVersion bytes allowed\")\n \t}\n \tsoftwareVersionLen := uint8(data[0])\n-\tif len(data[1:]) < int(softwareVersionLen) || softwareVersionLen > 64 {\n+\tif len(data[1:]) < int(softwareVersionLen) || softwareVersionLen > 64 || softwareVersionLen == 0 {\n \t\treturn NewMessageError(BGP_ERROR_OPEN_MESSAGE_ERROR, BGP_ERROR_SUB_UNSUPPORTED_CAPABILITY, nil, \"invalid length of software version capablity\")\n \t}\n \tc.SoftwareVersionLen = softwareVersionLen\n"
  },
  "CWE-908": {
    "cve": "CVE-2025-55198",
    "commit_url": "https://github.com/helm/helm/commit/ec5f59e2db56533d042a124f5bae54dd87b558e6",
    "diff": "diff --git a/pkg/chartutil/dependencies.go b/pkg/chartutil/dependencies.go\nindex 36a34192728..37452cec7a3 100644\n--- a/pkg/chartutil/dependencies.go\n+++ b/pkg/chartutil/dependencies.go\n@@ -16,6 +16,7 @@ limitations under the License.\n package chartutil\n \n import (\n+\t\"fmt\"\n \t\"log\"\n \t\"strings\"\n \n@@ -255,8 +256,8 @@ func processImportValues(c *chart.Chart, merge bool) error {\n \t\tfor _, riv := range r.ImportValues {\n \t\t\tswitch iv := riv.(type) {\n \t\t\tcase map[string]interface{}:\n-\t\t\t\tchild := iv[\"child\"].(string)\n-\t\t\t\tparent := iv[\"parent\"].(string)\n+\t\t\t\tchild := fmt.Sprintf(\"%v\", iv[\"child\"])\n+\t\t\t\tparent := fmt.Sprintf(\"%v\", iv[\"parent\"])\n \n \t\t\t\toutiv = append(outiv, map[string]string{\n \t\t\t\t\t\"child\":  child,\ndiff --git a/pkg/lint/rules/chartfile.go b/pkg/lint/rules/chartfile.go\nindex 910602b7df5..555ec71baf0 100644\n--- a/pkg/lint/rules/chartfile.go\n+++ b/pkg/lint/rules/chartfile.go\n@@ -151,6 +151,9 @@ func validateChartVersion(cf *chart.Metadata) error {\n \n func validateChartMaintainer(cf *chart.Metadata) error {\n \tfor _, maintainer := range cf.Maintainers {\n+\t\tif maintainer == nil {\n+\t\t\treturn errors.New(\"a maintainer entry is empty\")\n+\t\t}\n \t\tif maintainer.Name == \"\" {\n \t\t\treturn errors.New(\"each maintainer requires a name\")\n \t\t} else if maintainer.Email != \"\" && !govalidator.IsEmail(maintainer.Email) {\ndiff --git a/pkg/lint/rules/chartfile_test.go b/pkg/lint/rules/chartfile_test.go\nindex f4c836cf7f2..4740047bea3 100644\n--- a/pkg/lint/rules/chartfile_test.go\n+++ b/pkg/lint/rules/chartfile_test.go\n@@ -143,6 +143,16 @@ func TestValidateChartMaintainer(t *testing.T) {\n \t\t\tt.Errorf(\"validateChartMaintainer(%s, %s) to return no error, got %s\", test.Name, test.Email, err.Error())\n \t\t}\n \t}\n+\n+\t// Testing for an empty maintainer\n+\tbadChart.Maintainers = []*chart.Maintainer{nil}\n+\terr := validateChartMaintainer(badChart)\n+\tif err == nil {\n+\t\tt.Errorf(\"validateChartMaintainer did not return error for nil maintainer as expected\")\n+\t}\n+\tif err.Error() != \"a maintainer entry is empty\" {\n+\t\tt.Errorf(\"validateChartMaintainer returned unexpected error for nil maintainer: %s\", err.Error())\n+\t}\n }\n \n func TestValidateChartSources(t *testing.T) {\ndiff --git a/pkg/repo/index.go b/pkg/repo/index.go\nindex e1ce3c62dd2..a93314ab8bd 100644\n--- a/pkg/repo/index.go\n+++ b/pkg/repo/index.go\n@@ -357,6 +357,7 @@ func loadIndex(data []byte, source string) (*IndexFile, error) {\n \t\tfor idx := len(cvs) - 1; idx >= 0; idx-- {\n \t\t\tif cvs[idx] == nil {\n \t\t\t\tlog.Printf(\"skipping loading invalid entry for chart %q from %s: empty entry\", name, source)\n+\t\t\t\tcvs = append(cvs[:idx], cvs[idx+1:]...)\n \t\t\t\tcontinue\n \t\t\t}\n \t\t\t// When metadata section missing, initialize with no data\ndiff --git a/pkg/repo/index_test.go b/pkg/repo/index_test.go\nindex a1e3e943886..1fb30f3c0a1 100644\n--- a/pkg/repo/index_test.go\n+++ b/pkg/repo/index_test.go\n@@ -68,6 +68,7 @@ entries:\n   grafana:\n   - apiVersion: v2\n     name: grafana\n+  - null\n   foo:\n   -\n   bar:\n"
  },
  "CWE-415": {
    "cve": "CVE-2023-26545",
    "commit_url": "https://github.com/torvalds/linux/commit/fda6c89fe3d9aca073495a664e1d5aea28cd4377",
    "diff": "diff --git a/net/mpls/af_mpls.c b/net/mpls/af_mpls.c\nindex 35b5f806fdda17..dc5165d3eec4e7 100644\n--- a/net/mpls/af_mpls.c\n+++ b/net/mpls/af_mpls.c\n@@ -1428,6 +1428,7 @@ static int mpls_dev_sysctl_register(struct net_device *dev,\n free:\n \tkfree(table);\n out:\n+\tmdev->sysctl = NULL;\n \treturn -ENOBUFS;\n }\n \n@@ -1437,6 +1438,9 @@ static void mpls_dev_sysctl_unregister(struct net_device *dev,\n \tstruct net *net = dev_net(dev);\n \tstruct ctl_table *table;\n \n+\tif (!mdev->sysctl)\n+\t\treturn;\n+\n \ttable = mdev->sysctl->ctl_table_arg;\n \tunregister_net_sysctl_table(mdev->sysctl);\n \tkfree(table);\n"
  },
  "CWE-134": {
    "cve": "CVE-2025-48388",
    "commit_url": "https://github.com/freescout-help-desk/freescout/commit/eab97711027fff4bce90ccd2e189cbc184fa0370",
    "diff": "diff --git a/app/Http/Middleware/VerifyCsrfToken.php b/app/Http/Middleware/VerifyCsrfToken.php\nindex 8be274b69..0c13b8548 100644\n--- a/app/Http/Middleware/VerifyCsrfToken.php\n+++ b/app/Http/Middleware/VerifyCsrfToken.php\n@@ -14,30 +14,4 @@ class VerifyCsrfToken extends Middleware\n     protected $except = [\n         //\n     ];\n-\n-    /**\n-     * Handle an incoming request.\n-     *\n-     * @param  \\Illuminate\\Http\\Request  $request\n-     * @param  \\Closure  $next\n-     * @return mixed\n-     *\n-     * @throws \\Illuminate\\Session\\TokenMismatchException\n-     */\n-    public function handle($request, $next)\n-    {\n-        if (\n-            $this->isReading($request) ||\n-            $this->runningUnitTests() ||\n-            $this->inExceptArray($request) ||\n-            $this->tokensMatch($request)\n-        ) {\n-        \t// Do not send XSRF-TOKEN as it's not needed.\n-        \t// https://github.com/laravel/ideas/issues/873\n-            //return $this->addCookieToResponse($request, $next($request));\n-            return $next($request);\n-        }\n-\n-        throw new TokenMismatchException;\n-    }\n }\ndiff --git a/app/Misc/Helper.php b/app/Misc/Helper.php\nindex 7d2349698..db339bfa5 100644\n--- a/app/Misc/Helper.php\n+++ b/app/Misc/Helper.php\n@@ -973,6 +973,8 @@ public static function setEnvFileVar($key, $value)\n         $env_path = app()->environmentFilePath();\n         $contents = file_get_contents($env_path);\n \n+        $value = preg_replace(\"#[\\r\\n\\t]#\", '', $value);\n+\n         if (strstr($value, '\"')) {\n             // Escape quotes.\n             $value = '\"'.str_replace('\"', '\\\"', $value).'\"';\n@@ -2157,10 +2159,27 @@ public static function cspMetaTag()\n \n         $nonce = \\Helper::cspNonce();\n \n-        // default-src 'self'; img-src 'self' data:; font-src 'self' data:; style-src 'self' 'unsafe-inline'; form-action 'self'; frame-src https://recaptcha.net; connect-src https://recaptcha.net;\n+        $script_src = config('app.csp_script_src').' '.\\Eventy::filter('csp.script_src', '');\n+\n+        $script_domains = '';\n+        $scripts = explode(' ', $script_src);\n+\n+        foreach ($scripts as $url) {\n+            $url = trim($url);\n+            if (!preg_match(\"#^(http|//)#\", $url)) {\n+                $url = '//'.$url;\n+            }\n+            $parts = parse_url($url);\n+            if (!empty($parts['host'])) {\n+                $domain = preg_replace(\"#['\\\"; \\r\\n]#\", '', $parts['host']);\n+                $script_domains .= ' '.$domain;\n+            }\n+        }\n+\n+        //  frame-src https://recaptcha.net; connect-src https://recaptcha.net;\n \n-        return \"<meta http-equiv=\\\"Content-Security-Policy\\\" content=\\\"script-src 'self' 'nonce-\".$nonce.\"' \"\n-            .config('app.csp_script_src').' '.\\Eventy::filter('csp.script_src', '').\";\"\n+        return \"<meta http-equiv=\\\"Content-Security-Policy\\\" content=\\\"default-src 'self' \".$script_domains.\"; img-src 'self' data:; font-src 'self' data:; style-src 'self' 'unsafe-inline'; form-action 'self'; script-src 'self' 'nonce-\".$nonce.\"' \"\n+            .$script_src.\";\"\n             .config('app.csp_custom').\\Eventy::filter('csp.custom', '').\"\\\">\";\n     }\n \ndiff --git a/app/Option.php b/app/Option.php\nindex f4d8a6f36..1c305fe1e 100644\n--- a/app/Option.php\n+++ b/app/Option.php\n@@ -205,8 +205,16 @@ public static function remove($name)\n      */\n     public static function maybeSerialize($data)\n     {\n-        if (is_array($data) || is_object($data)) {\n-            return serialize($data);\n+        // if (is_array($data) || is_object($data)) {\n+        //     return serialize($data);\n+        // }\n+        // We don't use serialize() function as it is not safe.\n+        if (is_array($data)) {\n+            try {\n+                return \\Helper::jsonEncodeSafe($data);\n+            } catch (\\Exception $e) {\n+                // Don nothing.\n+            }\n         }\n \n         return $data;\n@@ -217,6 +225,8 @@ public static function maybeSerialize($data)\n      */\n     public static function maybeUnserialize($original)\n     {\n+        // This is kept jsut for backward compatibility.\n+        // Options are not seralized using serialize() anymore.\n         if (self::isSerialized($original)) {\n             try {\n                 $original = unserialize($original);\n@@ -227,6 +237,17 @@ public static function maybeUnserialize($original)\n             return $original;\n         }\n \n+        if (is_string($original) && !empty($original[0]) && $original[0] == '{') {\n+            try {\n+                $result = json_decode($original, true);\n+                if (is_array($result)) {\n+                    return $result;\n+                }\n+            } catch (\\Exception $e) {\n+                // Do nothing.\n+            }\n+        }\n+\n         return $original;\n     }\n \ndiff --git a/config/app.php b/config/app.php\nindex 099b89b44..020d9fd32 100644\n--- a/config/app.php\n+++ b/config/app.php\n@@ -18,7 +18,7 @@\n     | or any other location as required by the application or its packages.\n     */\n \n-    'version' => '1.8.177',\n+    'version' => '1.8.178',\n \n     /*\n     |--------------------------------------------------------------------------\ndiff --git a/config/session.php b/config/session.php\nindex 6e74ae7c6..4f85a63e7 100644\n--- a/config/session.php\n+++ b/config/session.php\n@@ -190,8 +190,11 @@\n     |\n     | Supported: \"lax\", \"strict\", \"none\"\n     |\n+    | When set to \"lax\" for example the SSO auth does not work: \n+    | https://github.com/freescout-help-desk/freescout/issues/4750\n+    |\n     */\n \n-    'same_site' => env('SESSION_SAME_SITE', 'lax'),\n+    'same_site' => env('SESSION_SAME_SITE', 'none'),\n \n ];\ndiff --git a/overrides/barryvdh/laravel-translation-manager/src/Controller.php b/overrides/barryvdh/laravel-translation-manager/src/Controller.php\nindex 5c0f7d43a..5da15ab6e 100644\n--- a/overrides/barryvdh/laravel-translation-manager/src/Controller.php\n+++ b/overrides/barryvdh/laravel-translation-manager/src/Controller.php\n@@ -213,7 +213,10 @@ public function postAddLocale(Request $request)\n     {\n         $locales = $this->manager->getLocales();\n         $newLocale = str_replace([], '-', trim($request->input('new-locale')));\n-        if (!$newLocale || in_array($newLocale, $locales)) {\n+        if (!$newLocale \n+            || in_array($newLocale, $locales)\n+            || !in_array($newLocale, array_keys(\\Helper::$locales))\n+        ) {\n             return redirect()->back();\n         }\n         $this->manager->addLocale($newLocale);\ndiff --git a/overrides/laravel/framework/src/Illuminate/Foundation/Http/Middleware/VerifyCsrfToken.php b/overrides/laravel/framework/src/Illuminate/Foundation/Http/Middleware/VerifyCsrfToken.php\nindex 89a550179..d9d281ccb 100644\n--- a/overrides/laravel/framework/src/Illuminate/Foundation/Http/Middleware/VerifyCsrfToken.php\n+++ b/overrides/laravel/framework/src/Illuminate/Foundation/Http/Middleware/VerifyCsrfToken.php\n@@ -64,7 +64,10 @@ public function handle($request, Closure $next)\n             $this->inExceptArray($request) ||\n             $this->tokensMatch($request)\n         ) {\n-            return $this->addCookieToResponse($request, $next($request));\n+            // Do not send XSRF-TOKEN as it's not needed.\n+            // https://github.com/laravel/ideas/issues/873\n+            //return $this->addCookieToResponse($request, $next($request));\n+            return $next($request);\n         }\n \n         throw new TokenMismatchException;\ndiff --git a/public/js/main.js b/public/js/main.js\nindex 4d74ad7fe..5352e0048 100755\n--- a/public/js/main.js\n+++ b/public/js/main.js\n@@ -5449,7 +5449,7 @@ function inApp(topic, token)\n \n function setCookie(name, value, props)\n {\n-    props = props || {path:\"/\", SameSite:'Lax'};\n+    props = props || {path:\"/\", SameSite:'None'};\n \n     if (!props.expires) {\n     \t// Max expiration date\ndiff --git a/public/tools.php b/public/tools.php\nindex da602f82b..040de0c6b 100644\n--- a/public/tools.php\n+++ b/public/tools.php\n@@ -89,7 +89,7 @@ function clearCache($root_dir, $php_path)\n     if (!empty($_POST['php_path'])) {\n         $php_path = trim($_POST['php_path']);\n \n-        $php_path = preg_replace(\"#[ ;\\$<>:&\\|]#\", '', $php_path);\n+        $php_path = preg_replace(\"#[ ;\\$<>:&\\|`\\t\\r\\n]#\", '', $php_path);\n         if (!$php_path) {\n             $php_path = 'php';\n         }\n"
  },
  "CWE-369": {
    "cve": "CVE-2025-55212",
    "commit_url": "https://github.com/ImageMagick/ImageMagick/commit/5f0bcf986b8b5e90567750d31a37af502b73f2af",
    "diff": "diff --git a/MagickCore/resize.c b/MagickCore/resize.c\nindex 0a0c60aabe5..6a87a6d17cb 100644\n--- a/MagickCore/resize.c\n+++ b/MagickCore/resize.c\n@@ -4613,8 +4613,6 @@ MagickExport Image *ThumbnailImage(const Image *image,const size_t columns,\n   assert(exception->signature == MagickCoreSignature);\n   if (IsEventLogging() != MagickFalse)\n     (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",image->filename);\n-  if ((columns == 0) || (rows == 0))\n-    ThrowImageException(ImageError,\"NegativeOrZeroImageSize\");\n   thumbnail_image=CloneImage(image,0,0,MagickTrue,exception);\n   if (thumbnail_image == (Image *) NULL)\n     return(thumbnail_image);\n@@ -4627,8 +4625,9 @@ MagickExport Image *ThumbnailImage(const Image *image,const size_t columns,\n         x_factor,\n         y_factor;\n \n-      x_factor=(ssize_t) image->columns/(ssize_t) columns;\n-      y_factor=(ssize_t) image->rows/(ssize_t) rows;\n+      x_factor=(ssize_t) (image->columns*MagickSafeReciprocal((double) \n+        columns));\n+      y_factor=(ssize_t) (image->rows*MagickSafeReciprocal((double) rows));\n       if ((x_factor > 4) && (y_factor > 4))\n         {\n           thumbnail_image=SampleImage(clone_image,4*columns,4*rows,exception);\n"
  },
  "CWE-276": {
    "cve": "CVE-2024-34455",
    "commit_url": "https://github.com/buildroot/buildroot/commit/0b2967e15800421efbdfe3a7a6061cf6bd84134d",
    "diff": "diff --git a/package/skeleton-init-sysv/skeleton/etc/fstab b/package/skeleton-init-sysv/skeleton/etc/fstab\nindex 169054b74f2d..06c20fe9d528 100644\n--- a/package/skeleton-init-sysv/skeleton/etc/fstab\n+++ b/package/skeleton-init-sysv/skeleton/etc/fstab\n@@ -2,7 +2,7 @@\n /dev/root\t/\t\text2\trw,noauto\t0\t1\n proc\t\t/proc\t\tproc\tdefaults\t0\t0\n devpts\t\t/dev/pts\tdevpts\tdefaults,gid=5,mode=620,ptmxmode=0666\t0\t0\n-tmpfs\t\t/dev/shm\ttmpfs\tmode=0777\t0\t0\n+tmpfs\t\t/dev/shm\ttmpfs\tmode=1777\t0\t0\n tmpfs\t\t/tmp\t\ttmpfs\tmode=1777\t0\t0\n tmpfs\t\t/run\t\ttmpfs\tmode=0755,nosuid,nodev\t0\t0\n sysfs\t\t/sys\t\tsysfs\tdefaults\t0\t0\n"
  },
  "CWE-295": {
    "cve": "CVE-2025-62371",
    "commit_url": "https://github.com/opensearch-project/data-prepper/commit/98fcf0d0ff9c18f1f7501e11dbed918814724b99",
    "diff": "diff --git a/data-prepper-plugins/opensearch/build.gradle b/data-prepper-plugins/opensearch/build.gradle\nindex 85409b9995..3f85dc33b0 100644\n--- a/data-prepper-plugins/opensearch/build.gradle\n+++ b/data-prepper-plugins/opensearch/build.gradle\n@@ -47,6 +47,7 @@ dependencies {\n     testImplementation 'net.bytebuddy:byte-buddy-agent:1.17.6'\n     testImplementation testLibs.slf4j.simple\n     testImplementation project(path: ':data-prepper-test:test-common')\n+    testImplementation 'org.wiremock:wiremock:3.10.0'\n }\n \n sourceSets {\ndiff --git a/data-prepper-plugins/opensearch/src/integrationTest/java/org/opensearch/dataprepper/plugins/sink/opensearch/OpenSearchIT.java b/data-prepper-plugins/opensearch/src/integrationTest/java/org/opensearch/dataprepper/plugins/sink/opensearch/OpenSearchIT.java\nindex 23fbc4c915..34c66e23c3 100644\n--- a/data-prepper-plugins/opensearch/src/integrationTest/java/org/opensearch/dataprepper/plugins/sink/opensearch/OpenSearchIT.java\n+++ b/data-prepper-plugins/opensearch/src/integrationTest/java/org/opensearch/dataprepper/plugins/sink/opensearch/OpenSearchIT.java\n@@ -32,6 +32,7 @@ public void testOpenSearchConnection() throws IOException {\n             builder.withUsername(user);\n             builder.withPassword(password);\n         }\n+        builder.withInsecure(true);\n         final AwsCredentialsSupplier awsCredentialsSupplier = mock(AwsCredentialsSupplier.class);\n         final RestHighLevelClient client = builder.build().createClient(awsCredentialsSupplier);\n \ndiff --git a/data-prepper-plugins/opensearch/src/integrationTest/java/org/opensearch/dataprepper/plugins/sink/opensearch/OpenSearchSinkIT.java b/data-prepper-plugins/opensearch/src/integrationTest/java/org/opensearch/dataprepper/plugins/sink/opensearch/OpenSearchSinkIT.java\nindex 2146b6ac20..564ed55fce 100644\n--- a/data-prepper-plugins/opensearch/src/integrationTest/java/org/opensearch/dataprepper/plugins/sink/opensearch/OpenSearchSinkIT.java\n+++ b/data-prepper-plugins/opensearch/src/integrationTest/java/org/opensearch/dataprepper/plugins/sink/opensearch/OpenSearchSinkIT.java\n@@ -1691,6 +1691,7 @@ private Map<String, Object> initializeConfigurationMetadata(final String indexTy\n         metadata.put(IndexConfiguration.INDEX_ALIAS, indexAlias);\n         metadata.put(IndexConfiguration.TEMPLATE_FILE, templateFilePath);\n         metadata.put(IndexConfiguration.FLUSH_TIMEOUT, -1);\n+        metadata.put(\"insecure\", true);\n         final String user = System.getProperty(\"tests.opensearch.user\");\n         final String password = System.getProperty(\"tests.opensearch.password\");\n         if (user != null) {\ndiff --git a/data-prepper-plugins/opensearch/src/main/java/org/opensearch/dataprepper/plugins/sink/opensearch/ConnectionConfiguration.java b/data-prepper-plugins/opensearch/src/main/java/org/opensearch/dataprepper/plugins/sink/opensearch/ConnectionConfiguration.java\nindex f33ed06be8..8461bc37c8 100644\n--- a/data-prepper-plugins/opensearch/src/main/java/org/opensearch/dataprepper/plugins/sink/opensearch/ConnectionConfiguration.java\n+++ b/data-prepper-plugins/opensearch/src/main/java/org/opensearch/dataprepper/plugins/sink/opensearch/ConnectionConfiguration.java\n@@ -384,8 +384,18 @@ private void checkProxyPort(final int port) {\n   }\n \n   private void attachSSLContext(final HttpAsyncClientBuilder httpClientBuilder) {\n-    final SSLContext sslContext = certPath != null ? getCAStrategy(certPath) : getTrustAllStrategy();\n-    httpClientBuilder.setSSLContext(sslContext);\n+    final SSLContext sslContext;\n+    if(certPath != null) {\n+      sslContext = getCAStrategy(certPath);\n+    } else if(this.insecure) {\n+      sslContext = getTrustAllStrategy();\n+    } else {\n+      sslContext = null;\n+    }\n+    if(sslContext != null) {\n+      httpClientBuilder.setSSLContext(sslContext);\n+    }\n+\n     if (this.insecure) {\n       httpClientBuilder.setSSLHostnameVerifier(NoopHostnameVerifier.INSTANCE);\n     }\n@@ -439,7 +449,7 @@ private OpenSearchTransport createOpenSearchTransport(final RestHighLevelClient\n         transportOptions.setRequestCompressionSize(Integer.MAX_VALUE);\n       }\n \n-      return new AwsSdk2Transport(createSdkHttpClient(), HttpHost.create(hosts.get(0)).getHostName(),\n+      return new AwsSdk2Transport(createSdkHttpClient(), HttpHost.create(hosts.get(0)).toHostString(),\n               serviceName, Region.of(awsRegion), transportOptions.build());\n     } else {\n       return new RestClientTransport(\n@@ -461,11 +471,13 @@ private SdkHttpClient createSdkHttpClient() {\n   }\n \n   private void attachSSLContext(final ApacheHttpClient.Builder apacheHttpClientBuilder) {\n-    TrustManager[] trustManagers = createTrustManagers(certPath);\n-    apacheHttpClientBuilder.tlsTrustManagersProvider(() -> trustManagers);\n+    TrustManager[] trustManagers = createTrustManagers(certPath, insecure);\n+    if(trustManagers.length > 0) {\n+      apacheHttpClientBuilder.tlsTrustManagersProvider(() -> trustManagers);\n+    }\n   }\n \n-  private static TrustManager[] createTrustManagers(final Path certPath) {\n+  private static TrustManager[] createTrustManagers(final Path certPath, final boolean insecure) {\n     if (certPath != null) {\n       LOG.info(\"Using the cert provided in the config.\");\n       try (InputStream certificateInputStream = Files.newInputStream(certPath)) {\n@@ -481,8 +493,11 @@ private static TrustManager[] createTrustManagers(final Path certPath) {\n       } catch (Exception ex) {\n         throw new RuntimeException(ex.getMessage(), ex);\n       }\n-    } else {\n+    } else if(insecure) {\n+      LOG.info(\"Using the trust all strategy\");\n       return new TrustManager[] { new X509TrustAllManager() };\n+    } else {\n+      return new TrustManager[0];\n     }\n   }\n \ndiff --git a/data-prepper-plugins/opensearch/src/main/java/org/opensearch/dataprepper/plugins/source/opensearch/worker/client/OpenSearchClientFactory.java b/data-prepper-plugins/opensearch/src/main/java/org/opensearch/dataprepper/plugins/source/opensearch/worker/client/OpenSearchClientFactory.java\nindex 2ee6c59b87..06e2ab3f07 100644\n--- a/data-prepper-plugins/opensearch/src/main/java/org/opensearch/dataprepper/plugins/source/opensearch/worker/client/OpenSearchClientFactory.java\n+++ b/data-prepper-plugins/opensearch/src/main/java/org/opensearch/dataprepper/plugins/source/opensearch/worker/client/OpenSearchClientFactory.java\n@@ -271,7 +271,9 @@ private void setConnectAndSocketTimeout(final org.elasticsearch.client.RestClien\n \n     private void attachSSLContext(final NettyNioAsyncHttpClient.Builder asyncClientBuilder, final OpenSearchSourceConfiguration openSearchSourceConfiguration) {\n         TrustManager[] trustManagers = createTrustManagers(openSearchSourceConfiguration.getConnectionConfiguration());\n-        asyncClientBuilder.tlsTrustManagersProvider(() -> trustManagers);\n+        if (trustManagers.length > 0) {\n+            asyncClientBuilder.tlsTrustManagersProvider(() -> trustManagers);\n+        }\n     }\n \n     private void attachSSLContext(final HttpAsyncClientBuilder httpClientBuilder, final OpenSearchSourceConfiguration openSearchSourceConfiguration) {\n@@ -287,31 +289,37 @@ private void attachSSLContext(final HttpAsyncClientBuilder httpClientBuilder, fi\n \n     private TrustManager[] createTrustManagers(final ConnectionConfiguration connectionConfiguration) {\n         final Path certPath = connectionConfiguration.getCertPath();\n-        if (Objects.nonNull(certPath)) {\n+        final String certificate = connectionConfiguration.getCertificate();\n+        if (certPath != null) {\n             return TrustStoreProvider.createTrustManager(certPath);\n-        } else if (Objects.nonNull(connectionConfiguration.getCertificate())) {\n-            if (PemObjectValidator.isPemObject(connectionConfiguration.getCertificate())) {\n-                return TrustStoreProvider.createTrustManager(connectionConfiguration.getCertificate());\n+        } else if (certificate != null) {\n+            if (PemObjectValidator.isPemObject(certificate)) {\n+                return TrustStoreProvider.createTrustManager(certificate);\n             } else {\n-                return TrustStoreProvider.createTrustManager(Path.of(connectionConfiguration.getCertificate()));\n-            }\n-        } else {\n+                return TrustStoreProvider.createTrustManager(Path.of(certificate));}\n+        } else if (connectionConfiguration.isInsecure()) {\n             return TrustStoreProvider.createTrustAllManager();\n+\n+        } else {\n+            return new TrustManager[0];\n         }\n     }\n \n     private SSLContext getCAStrategy(final ConnectionConfiguration connectionConfiguration) {\n         final Path certPath = connectionConfiguration.getCertPath();\n-        if (Objects.nonNull(certPath)) {\n+        final String certificate = connectionConfiguration.getCertificate();\n+        if (certPath != null) {\n             return TrustStoreProvider.createSSLContext(certPath);\n-        } else if (Objects.nonNull(connectionConfiguration.getCertificate())) {\n-            if (PemObjectValidator.isPemObject(connectionConfiguration.getCertificate())) {\n-                return TrustStoreProvider.createSSLContext(connectionConfiguration.getCertificate());\n+        } else if (certificate != null) {\n+            if (PemObjectValidator.isPemObject(certificate)) {\n+                return TrustStoreProvider.createSSLContext(certificate);\n             } else {\n                 return TrustStoreProvider.createSSLContext(Path.of(connectionConfiguration.getCertificate()));\n             }\n+        } else if (connectionConfiguration.isInsecure()) {\n+                return TrustStoreProvider.createSSLContextWithTrustAllStrategy();\n         } else {\n-            return TrustStoreProvider.createSSLContextWithTrustAllStrategy();\n+            return null;\n         }\n     }\n }\ndiff --git a/data-prepper-plugins/opensearch/src/test/java/org/opensearch/dataprepper/plugins/sink/opensearch/ConnectionConfigurationTests.java b/data-prepper-plugins/opensearch/src/test/java/org/opensearch/dataprepper/plugins/sink/opensearch/ConnectionConfigurationTests.java\nindex 9a4cf44b3e..8c891564b3 100644\n--- a/data-prepper-plugins/opensearch/src/test/java/org/opensearch/dataprepper/plugins/sink/opensearch/ConnectionConfigurationTests.java\n+++ b/data-prepper-plugins/opensearch/src/test/java/org/opensearch/dataprepper/plugins/sink/opensearch/ConnectionConfigurationTests.java\n@@ -150,7 +150,6 @@ void testCreateOpenSearchClientAwsServerlessDefault() throws IOException {\n         when(awsCredentialsSupplier.getProvider(any())).thenReturn(awsCredentialsProvider);\n \n         final RestHighLevelClient client = connectionConfiguration.createClient(awsCredentialsSupplier);\n-        when(apacheHttpClientBuilder.tlsTrustManagersProvider(any())).thenReturn(apacheHttpClientBuilder);\n         when(apacheHttpClientBuilder.build()).thenReturn(apacheHttpClient);\n         final OpenSearchClient openSearchClient;\n         try (final MockedStatic<ApacheHttpClient> apacheHttpClientMockedStatic = mockStatic(ApacheHttpClient.class)) {\n@@ -160,7 +159,6 @@ void testCreateOpenSearchClientAwsServerlessDefault() throws IOException {\n         assertNotNull(openSearchClient);\n         assertThat(openSearchClient._transport(), instanceOf(AwsSdk2Transport.class));\n         assertThat(openSearchClient._transport().jsonpMapper(), instanceOf(PreSerializedJsonpMapper.class));\n-        verify(apacheHttpClientBuilder).tlsTrustManagersProvider(any());\n         verify(apacheHttpClientBuilder).build();\n         openSearchClient.shutdown();\n         client.close();\ndiff --git a/data-prepper-plugins/opensearch/src/test/java/org/opensearch/dataprepper/plugins/sink/opensearch/ConnectionConfiguration_ServerTest.java b/data-prepper-plugins/opensearch/src/test/java/org/opensearch/dataprepper/plugins/sink/opensearch/ConnectionConfiguration_ServerTest.java\nnew file mode 100644\nindex 0000000000..115e1afcec\n--- /dev/null\n+++ b/data-prepper-plugins/opensearch/src/test/java/org/opensearch/dataprepper/plugins/sink/opensearch/ConnectionConfiguration_ServerTest.java\n@@ -0,0 +1,186 @@\n+/*\n+ * Copyright OpenSearch Contributors\n+ * SPDX-License-Identifier: Apache-2.0\n+ */\n+\n+package org.opensearch.dataprepper.plugins.sink.opensearch;\n+\n+import com.github.tomakehurst.wiremock.WireMockServer;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Nested;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.mockito.Mock;\n+import org.mockito.junit.jupiter.MockitoExtension;\n+import org.opensearch.client.RequestOptions;\n+import org.opensearch.client.RestHighLevelClient;\n+import org.opensearch.client.core.MainResponse;\n+import org.opensearch.client.opensearch.OpenSearchClient;\n+import org.opensearch.client.opensearch.core.InfoResponse;\n+import org.opensearch.dataprepper.aws.api.AwsCredentialsSupplier;\n+import software.amazon.awssdk.auth.credentials.AnonymousCredentialsProvider;\n+\n+import javax.net.ssl.SSLHandshakeException;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.Map;\n+import java.util.UUID;\n+\n+import static com.github.tomakehurst.wiremock.client.WireMock.get;\n+import static com.github.tomakehurst.wiremock.client.WireMock.jsonResponse;\n+import static com.github.tomakehurst.wiremock.core.WireMockConfiguration.options;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.CoreMatchers.notNullValue;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.when;\n+\n+@ExtendWith(MockitoExtension.class)\n+class ConnectionConfiguration_ServerTest {\n+    private static WireMockServer wireMockServer;\n+\n+    @Mock\n+    private AwsCredentialsSupplier awsCredentialsSupplier;\n+\n+    private String host;\n+\n+    private String clusterUuid;\n+\n+    @BeforeAll\n+    static void setUpAll() {\n+        wireMockServer = new WireMockServer(options()\n+                .httpDisabled(true)\n+                .dynamicHttpsPort()\n+                .keystorePath(\"src/test/resources/test_keystore.jks\")\n+                .keystorePassword(\"password\")\n+                .keyManagerPassword(\"password\")\n+        );\n+\n+        wireMockServer.start();\n+    }\n+\n+    @AfterAll\n+    static void tearDownAll() {\n+        wireMockServer.stop();\n+    }\n+\n+    @BeforeEach\n+    void setUp() {\n+        host = \"https://localhost:\" + wireMockServer.httpsPort();\n+\n+        clusterUuid = UUID.randomUUID().toString();\n+        final Map<String, Object> responseBody = Map.of(\n+                \"name\", \"opensearch\",\n+                \"cluster_name\", \"opensearch\",\n+                \"cluster_uuid\", clusterUuid,\n+                \"version\", Map.of(\n+                        \"number\", \"2.10.0\",\n+                        \"build_hash\", \"abcdefg\",\n+                        \"build_date\", \"20241212\",\n+                        \"build_type\", \"testing\",\n+                        \"distribution\", \"datapreppertesting\",\n+                        \"build_snapshot\", \"false\",\n+                        \"lucene_version\", \"8\",\n+                        \"minimum_wire_compatibility_version\", \"2.10.0\",\n+                        \"minimum_index_compatibility_version\", \"2.10.0\"\n+                ),\n+                \"tagline\", \"You Know, for Search\"\n+        );\n+        wireMockServer.stubFor(get(\"/\").willReturn(jsonResponse(responseBody, 200)));\n+    }\n+\n+    @Nested\n+    class DefaultConfiguration {\n+        private ConnectionConfiguration createObjectUnderTest() {\n+            return new ConnectionConfiguration.Builder(Collections.singletonList(host))\n+                    .build();\n+        }\n+\n+        @Test\n+        void createClient_will_not_trust_self_signed_certificates_by_default() {\n+            final RestHighLevelClient client = createObjectUnderTest().createClient(awsCredentialsSupplier);\n+            assertThat(client, notNullValue());\n+\n+            assertThrows(SSLHandshakeException.class, () -> client.info(RequestOptions.DEFAULT));\n+        }\n+\n+        @Test\n+        void createOpenSearchClient_will_not_trust_self_signed_certificates_by_default() {\n+            final ConnectionConfiguration objectUnderTest = createObjectUnderTest();\n+            final OpenSearchClient openSearchClient = objectUnderTest.createOpenSearchClient(objectUnderTest.createClient(awsCredentialsSupplier), awsCredentialsSupplier);\n+            assertThat(openSearchClient, notNullValue());\n+\n+            assertThrows(SSLHandshakeException.class, openSearchClient::info);\n+        }\n+    }\n+\n+    @Nested\n+    class DefaultSigV4Configuration {\n+        @BeforeEach\n+        void setUp() {\n+            when(awsCredentialsSupplier.getProvider(any())).thenReturn(AnonymousCredentialsProvider.create());\n+        }\n+\n+        private ConnectionConfiguration createObjectUnderTest() {\n+            return new ConnectionConfiguration.Builder(Collections.singletonList(host))\n+                    .withAwsSigv4(true)\n+                    .withAwsRegion(\"us-east-1\")\n+                    .build();\n+        }\n+\n+        @Test\n+        void createClient_will_not_trust_self_signed_certificates_by_default() {\n+            final RestHighLevelClient client = createObjectUnderTest().createClient(awsCredentialsSupplier);\n+            assertThat(client, notNullValue());\n+\n+            assertThrows(SSLHandshakeException.class, () -> client.info(RequestOptions.DEFAULT));\n+        }\n+\n+        @Test\n+        void createOpenSearchClient_will_not_trust_self_signed_certificates_by_default() {\n+            final ConnectionConfiguration objectUnderTest = createObjectUnderTest();\n+            final OpenSearchClient openSearchClient = objectUnderTest.createOpenSearchClient(objectUnderTest.createClient(awsCredentialsSupplier), awsCredentialsSupplier);\n+            assertThat(openSearchClient, notNullValue());\n+\n+            assertThrows(SSLHandshakeException.class, openSearchClient::info);\n+        }\n+    }\n+\n+    @Nested\n+    class InsecureConfiguration {\n+        private ConnectionConfiguration createObjectUnderTest() {\n+            return new ConnectionConfiguration.Builder(Collections.singletonList(host))\n+                    .withInsecure(true)\n+                    .build();\n+        }\n+\n+        @Test\n+        void createClient_will_trust_self_signed_certificates_if_insecure() throws IOException {\n+            final RestHighLevelClient client = createObjectUnderTest().createClient(awsCredentialsSupplier);\n+            assertThat(client, notNullValue());\n+\n+            final MainResponse infoResponse = client.info(RequestOptions.DEFAULT);\n+\n+            assertThat(infoResponse, notNullValue());\n+            assertThat(infoResponse.getClusterName(), equalTo(\"opensearch\"));\n+            assertThat(infoResponse.getClusterUuid(), equalTo(clusterUuid));\n+        }\n+\n+\n+        @Test\n+        void createOpenSearchClient_will_trust_self_signed_certificates_if_insecure() throws IOException {\n+            final ConnectionConfiguration objectUnderTest = createObjectUnderTest();\n+            final OpenSearchClient openSearchClient = objectUnderTest.createOpenSearchClient(objectUnderTest.createClient(awsCredentialsSupplier), awsCredentialsSupplier);\n+            assertThat(openSearchClient, notNullValue());\n+\n+            final InfoResponse infoResponse = openSearchClient.info();\n+\n+            assertThat(infoResponse, notNullValue());\n+            assertThat(infoResponse.clusterName(), equalTo(\"opensearch\"));\n+            assertThat(infoResponse.clusterUuid(), equalTo(clusterUuid));\n+        }\n+    }\n+}\n\\ No newline at end of file\ndiff --git a/data-prepper-plugins/opensearch/src/test/java/org/opensearch/dataprepper/plugins/source/opensearch/worker/client/OpenSearchClientFactoryTest.java b/data-prepper-plugins/opensearch/src/test/java/org/opensearch/dataprepper/plugins/source/opensearch/worker/client/OpenSearchClientFactoryTest.java\nindex 647dd09227..ecc7d54787 100644\n--- a/data-prepper-plugins/opensearch/src/test/java/org/opensearch/dataprepper/plugins/source/opensearch/worker/client/OpenSearchClientFactoryTest.java\n+++ b/data-prepper-plugins/opensearch/src/test/java/org/opensearch/dataprepper/plugins/source/opensearch/worker/client/OpenSearchClientFactoryTest.java\n@@ -27,6 +27,8 @@\n import software.amazon.awssdk.regions.Region;\n \n import javax.net.ssl.SSLContext;\n+import javax.net.ssl.TrustManager;\n+\n import java.nio.file.Path;\n import java.time.Duration;\n import java.util.Collections;\n@@ -41,6 +43,7 @@\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.mockStatic;\n import static org.mockito.Mockito.never;\n+import static org.mockito.Mockito.times;\n import static org.mockito.Mockito.verify;\n import static org.mockito.Mockito.verifyNoInteractions;\n import static org.mockito.Mockito.when;\n@@ -409,9 +412,40 @@ void createSdkAsyncHttpClient_with_self_signed_certificate() {\n         lenient().when(openSearchSourceConfiguration.getConnectionConfiguration()).thenReturn(connectionConfiguration);\n         lenient().when(connectionConfiguration.getCertPath()).thenReturn(path);\n         try (MockedStatic<TrustStoreProvider> trustStoreProviderMockedStatic = mockStatic(TrustStoreProvider.class)) {\n+            TrustManager[] mockTrustManagers = new TrustManager[] { mock(TrustManager.class) };\n+            trustStoreProviderMockedStatic.when(() -> TrustStoreProvider.createTrustManager(path)).thenReturn(mockTrustManagers);\n             final SdkAsyncHttpClient sdkAsyncHttpClient = createObjectUnderTest().createSdkAsyncHttpClient(openSearchSourceConfiguration);\n             assertThat(sdkAsyncHttpClient, notNullValue());\n             trustStoreProviderMockedStatic.verify(() -> TrustStoreProvider.createTrustManager(path));\n         }\n     }\n+    @Test\n+    void createSdkAsyncHttpClient_with_secure_configuration_and_no_cert_path_does_not_trust_all_managers() {\n+        when(connectionConfiguration.getCertPath()).thenReturn(null);\n+        when(connectionConfiguration.isInsecure()).thenReturn(false);\n+        when(connectionConfiguration.getConnectTimeout()).thenReturn(Duration.ofSeconds(30));\n+        try (MockedStatic<TrustStoreProvider> trustStoreProviderMockedStatic = mockStatic(TrustStoreProvider.class)) {\n+            final SdkAsyncHttpClient sdkAsyncHttpClient = createObjectUnderTest().createSdkAsyncHttpClient(openSearchSourceConfiguration);\n+            assertThat(sdkAsyncHttpClient, notNullValue());\n+            trustStoreProviderMockedStatic.verify(() -> TrustStoreProvider.createTrustAllManager(), never());\n+            trustStoreProviderMockedStatic.verify(() -> TrustStoreProvider.createTrustManager(any(Path.class)), never());\n+        }\n+    }\n+    \n+    @Test\n+    void createSdkAsyncHttpClient_with_insecure_configuration_and_no_cert_path_trusts_all_managers() {\n+        when(connectionConfiguration.getCertPath()).thenReturn(null);\n+        when(connectionConfiguration.isInsecure()).thenReturn(true);\n+        when(connectionConfiguration.getConnectTimeout()).thenReturn(Duration.ofSeconds(30));\n+        try (MockedStatic<TrustStoreProvider> trustStoreProviderMockedStatic = mockStatic(TrustStoreProvider.class)) {\n+            TrustManager[] mockTrustManagers = new TrustManager[] { mock(TrustManager.class) };\n+            trustStoreProviderMockedStatic.when(() -> TrustStoreProvider.createTrustAllManager())\n+                    .thenReturn(mockTrustManagers);\n+            final SdkAsyncHttpClient sdkAsyncHttpClient = createObjectUnderTest().createSdkAsyncHttpClient(openSearchSourceConfiguration);\n+            assertThat(sdkAsyncHttpClient, notNullValue());\n+            trustStoreProviderMockedStatic.verify(() -> TrustStoreProvider.createTrustAllManager(), times(1));\n+            trustStoreProviderMockedStatic.verify(() -> TrustStoreProvider.createTrustManager(any(Path.class)), never());\n+        }\n+    }\n+\n }\ndiff --git a/data-prepper-plugins/opensearch/src/test/resources/test_keystore.jks b/data-prepper-plugins/opensearch/src/test/resources/test_keystore.jks\nnew file mode 100644\nindex 0000000000..77002f6532\nBinary files /dev/null and b/data-prepper-plugins/opensearch/src/test/resources/test_keystore.jks differ\ndiff --git a/e2e-test/log/src/integrationTest/java/org/opensearch/dataprepper/integration/log/EndToEndBasicLogTest.java b/e2e-test/log/src/integrationTest/java/org/opensearch/dataprepper/integration/log/EndToEndBasicLogTest.java\nindex 294af39288..2571aceaa9 100644\n--- a/e2e-test/log/src/integrationTest/java/org/opensearch/dataprepper/integration/log/EndToEndBasicLogTest.java\n+++ b/e2e-test/log/src/integrationTest/java/org/opensearch/dataprepper/integration/log/EndToEndBasicLogTest.java\n@@ -132,6 +132,7 @@ private RestHighLevelClient prepareOpenSearchRestHighLevelClient() {\n                 Collections.singletonList(\"https://127.0.0.1:9200\"));\n         builder.withUsername(\"admin\");\n         builder.withPassword(\"admin\");\n+        builder.withInsecure(true);\n         return builder.build().createClient(null);\n     }\n \ndiff --git a/e2e-test/log/src/integrationTest/java/org/opensearch/dataprepper/integration/log/ParallelGrokStringSubstituteLogTest.java b/e2e-test/log/src/integrationTest/java/org/opensearch/dataprepper/integration/log/ParallelGrokStringSubstituteLogTest.java\nindex e4f7b87f8e..203597d7db 100644\n--- a/e2e-test/log/src/integrationTest/java/org/opensearch/dataprepper/integration/log/ParallelGrokStringSubstituteLogTest.java\n+++ b/e2e-test/log/src/integrationTest/java/org/opensearch/dataprepper/integration/log/ParallelGrokStringSubstituteLogTest.java\n@@ -100,6 +100,7 @@ private RestHighLevelClient prepareOpenSearchRestHighLevelClient() {\n                 Collections.singletonList(\"https://127.0.0.1:9200\"));\n         builder.withUsername(\"admin\");\n         builder.withPassword(\"admin\");\n+        builder.withInsecure(true);\n         final AwsCredentialsSupplier awsCredentialsSupplier = mock(AwsCredentialsSupplier.class);\n         return builder.build().createClient(awsCredentialsSupplier);\n     }\ndiff --git a/e2e-test/log/src/integrationTest/resources/basic-grok-e2e-pipeline-date-pattern-index.yml b/e2e-test/log/src/integrationTest/resources/basic-grok-e2e-pipeline-date-pattern-index.yml\nindex 7d1ecb150d..16026e682f 100644\n--- a/e2e-test/log/src/integrationTest/resources/basic-grok-e2e-pipeline-date-pattern-index.yml\n+++ b/e2e-test/log/src/integrationTest/resources/basic-grok-e2e-pipeline-date-pattern-index.yml\n@@ -11,5 +11,6 @@ grok-pipeline:\n         hosts: [ \"https://node-0.example.com:9200\" ]\n         username: \"admin\"\n         password: \"admin\"\n+        insecure: true\n         index: \"test-grok-index-%{yyyy.MM.dd}\"\n         flush_timeout: 5000\ndiff --git a/e2e-test/log/src/integrationTest/resources/basic-grok-e2e-pipeline-with-aws-secrets.yml b/e2e-test/log/src/integrationTest/resources/basic-grok-e2e-pipeline-with-aws-secrets.yml\nindex b05b774386..2246db5af5 100644\n--- a/e2e-test/log/src/integrationTest/resources/basic-grok-e2e-pipeline-with-aws-secrets.yml\n+++ b/e2e-test/log/src/integrationTest/resources/basic-grok-e2e-pipeline-with-aws-secrets.yml\n@@ -17,5 +17,6 @@ grok-pipeline:\n         hosts: [ \"https://node-0.example.com:9200\" ]\n         username: \"${{aws_secrets:opensearch-sink:username}}\"\n         password: \"${{aws_secrets:opensearch-sink:password}}\"\n+        insecure: true\n         index: \"test-grok-index\"\n         flush_timeout: 5000\n\\ No newline at end of file\ndiff --git a/e2e-test/log/src/integrationTest/resources/basic-grok-e2e-pipeline.yml b/e2e-test/log/src/integrationTest/resources/basic-grok-e2e-pipeline.yml\nindex f9bbc0506c..1eee55f52e 100644\n--- a/e2e-test/log/src/integrationTest/resources/basic-grok-e2e-pipeline.yml\n+++ b/e2e-test/log/src/integrationTest/resources/basic-grok-e2e-pipeline.yml\n@@ -12,5 +12,6 @@ grok-pipeline:\n         hosts: [ \"https://node-0.example.com:9200\" ]\n         username: \"admin\"\n         password: \"admin\"\n+        insecure: true\n         index: \"test-grok-index\"\n         flush_timeout: 5000\ndiff --git a/e2e-test/log/src/integrationTest/resources/parallel-grok-substitute-e2e-pipeline.yml b/e2e-test/log/src/integrationTest/resources/parallel-grok-substitute-e2e-pipeline.yml\nindex 0d4ef4260e..3488e684ab 100644\n--- a/e2e-test/log/src/integrationTest/resources/parallel-grok-substitute-e2e-pipeline.yml\n+++ b/e2e-test/log/src/integrationTest/resources/parallel-grok-substitute-e2e-pipeline.yml\n@@ -22,6 +22,7 @@ pipeline2:\n         hosts: [ \"https://node-0.example.com:9200\" ]\n         username: \"admin\"\n         password: \"admin\"\n+        insecure: true\n         index: \"test-substitute-index\"\n         flush_timeout: 5000\n \n@@ -38,5 +39,6 @@ pipeline3:\n         hosts: [ \"https://node-0.example.com:9200\" ]\n         username: \"admin\"\n         password: \"admin\"\n+        insecure: true\n         index: \"test-grok-index\"\n         flush_timeout: 5000\ndiff --git a/e2e-test/peerforwarder/src/integrationTest/java/org/opensearch/dataprepper/integration/peerforwarder/EndToEndLogMetricsTest.java b/e2e-test/peerforwarder/src/integrationTest/java/org/opensearch/dataprepper/integration/peerforwarder/EndToEndLogMetricsTest.java\nindex 397b0280de..f324d75980 100644\n--- a/e2e-test/peerforwarder/src/integrationTest/java/org/opensearch/dataprepper/integration/peerforwarder/EndToEndLogMetricsTest.java\n+++ b/e2e-test/peerforwarder/src/integrationTest/java/org/opensearch/dataprepper/integration/peerforwarder/EndToEndLogMetricsTest.java\n@@ -181,6 +181,7 @@ private RestHighLevelClient prepareOpenSearchRestHighLevelClient() {\n                 Collections.singletonList(\"https://127.0.0.1:9200\"));\n         builder.withUsername(\"admin\");\n         builder.withPassword(\"admin\");\n+        builder.withInsecure(true);\n         final AwsCredentialsSupplier awsCredentialsSupplier = mock(AwsCredentialsSupplier.class);\n         return builder.build().createClient(awsCredentialsSupplier);\n     }\ndiff --git a/e2e-test/peerforwarder/src/integrationTest/java/org/opensearch/dataprepper/integration/peerforwarder/EndToEndPeerForwarderTest.java b/e2e-test/peerforwarder/src/integrationTest/java/org/opensearch/dataprepper/integration/peerforwarder/EndToEndPeerForwarderTest.java\nindex ad24b3889b..a39a41cc5c 100644\n--- a/e2e-test/peerforwarder/src/integrationTest/java/org/opensearch/dataprepper/integration/peerforwarder/EndToEndPeerForwarderTest.java\n+++ b/e2e-test/peerforwarder/src/integrationTest/java/org/opensearch/dataprepper/integration/peerforwarder/EndToEndPeerForwarderTest.java\n@@ -117,6 +117,7 @@ private RestHighLevelClient prepareOpenSearchRestHighLevelClient() {\n                 Collections.singletonList(\"https://127.0.0.1:9200\"));\n         builder.withUsername(\"admin\");\n         builder.withPassword(\"admin\");\n+        builder.withInsecure(true);\n         final AwsCredentialsSupplier awsCredentialsSupplier = mock(AwsCredentialsSupplier.class);\n         return builder.build().createClient(awsCredentialsSupplier);\n     }\ndiff --git a/e2e-test/peerforwarder/src/integrationTest/resources/aggregate-e2e-pipeline.yml b/e2e-test/peerforwarder/src/integrationTest/resources/aggregate-e2e-pipeline.yml\nindex 476340ddd8..1c863d6241 100644\n--- a/e2e-test/peerforwarder/src/integrationTest/resources/aggregate-e2e-pipeline.yml\n+++ b/e2e-test/peerforwarder/src/integrationTest/resources/aggregate-e2e-pipeline.yml\n@@ -12,5 +12,6 @@ aggregate-pipeline:\n         hosts: [ \"https://node-0.example.com:9200\" ]\n         username: \"admin\"\n         password: \"admin\"\n+        insecure: true\n         index: \"test-peer-forwarder-index\"\n         flush_timeout: 5000\n\\ No newline at end of file\ndiff --git a/e2e-test/peerforwarder/src/integrationTest/resources/log-metrics-pipeline.yml b/e2e-test/peerforwarder/src/integrationTest/resources/log-metrics-pipeline.yml\nindex f7a77414fc..b9dac3ddc3 100644\n--- a/e2e-test/peerforwarder/src/integrationTest/resources/log-metrics-pipeline.yml\n+++ b/e2e-test/peerforwarder/src/integrationTest/resources/log-metrics-pipeline.yml\n@@ -16,5 +16,6 @@ aggregate-pipeline:\n         hosts: [ \"https://node-0.example.com:9200\" ]\n         username: \"admin\"\n         password: \"admin\"\n+        insecure: true\n         index: \"test-log-metrics-index\"\n         flush_timeout: 5000\ndiff --git a/e2e-test/trace/src/integrationTest/java/org/opensearch/dataprepper/integration/trace/EndToEndRawSpanTest.java b/e2e-test/trace/src/integrationTest/java/org/opensearch/dataprepper/integration/trace/EndToEndRawSpanTest.java\nindex 6a8d033572..f7c956417a 100644\n--- a/e2e-test/trace/src/integrationTest/java/org/opensearch/dataprepper/integration/trace/EndToEndRawSpanTest.java\n+++ b/e2e-test/trace/src/integrationTest/java/org/opensearch/dataprepper/integration/trace/EndToEndRawSpanTest.java\n@@ -115,6 +115,7 @@ public void testPipelineEndToEnd() {\n                 Collections.singletonList(\"https://127.0.0.1:9200\"));\n         builder.withUsername(\"admin\");\n         builder.withPassword(\"admin\");\n+        builder.withInsecure(true);\n         final RestHighLevelClient restHighLevelClient = builder.build().createClient(null);\n         // Wait for data to flow through pipeline and be indexed by ES\n         await().atLeast(3, TimeUnit.SECONDS).atMost(20, TimeUnit.SECONDS).untilAsserted(\ndiff --git a/e2e-test/trace/src/integrationTest/java/org/opensearch/dataprepper/integration/trace/EndToEndServiceMapTest.java b/e2e-test/trace/src/integrationTest/java/org/opensearch/dataprepper/integration/trace/EndToEndServiceMapTest.java\nindex aead979e6c..ef9396d040 100644\n--- a/e2e-test/trace/src/integrationTest/java/org/opensearch/dataprepper/integration/trace/EndToEndServiceMapTest.java\n+++ b/e2e-test/trace/src/integrationTest/java/org/opensearch/dataprepper/integration/trace/EndToEndServiceMapTest.java\n@@ -81,6 +81,7 @@ public void testPipelineEndToEnd() {\n                 Collections.singletonList(\"https://127.0.0.1:9200\"));\n         builder.withUsername(\"admin\");\n         builder.withPassword(\"admin\");\n+        builder.withInsecure(true);\n         final AwsCredentialsSupplier awsCredentialsSupplier = mock(AwsCredentialsSupplier.class);\n         final RestHighLevelClient restHighLevelClient = builder.build().createClient(awsCredentialsSupplier);\n \ndiff --git a/e2e-test/trace/src/integrationTest/resources/raw-span-e2e-pipeline-from-build.yml b/e2e-test/trace/src/integrationTest/resources/raw-span-e2e-pipeline-from-build.yml\nindex 55e7ac7423..c869a38a16 100644\n--- a/e2e-test/trace/src/integrationTest/resources/raw-span-e2e-pipeline-from-build.yml\n+++ b/e2e-test/trace/src/integrationTest/resources/raw-span-e2e-pipeline-from-build.yml\n@@ -18,5 +18,6 @@ raw-pipeline:\n         hosts: [ \"https://node-0.example.com:9200\" ]\n         username: \"admin\"\n         password: \"admin\"\n+        insecure: true\n         index_type: trace-analytics-raw\n         flush_timeout: 5000\n\\ No newline at end of file\ndiff --git a/e2e-test/trace/src/integrationTest/resources/raw-span-e2e-pipeline-latest-release.yml b/e2e-test/trace/src/integrationTest/resources/raw-span-e2e-pipeline-latest-release.yml\nindex d09631885c..4bbbab2a83 100644\n--- a/e2e-test/trace/src/integrationTest/resources/raw-span-e2e-pipeline-latest-release.yml\n+++ b/e2e-test/trace/src/integrationTest/resources/raw-span-e2e-pipeline-latest-release.yml\n@@ -18,5 +18,6 @@ raw-pipeline:\n         hosts: [ \"https://node-0.example.com:9200\" ]\n         username: \"admin\"\n         password: \"admin\"\n+        insecure: true\n         index_type: trace-analytics-raw\n         flush_timeout: 5000\ndiff --git a/e2e-test/trace/src/integrationTest/resources/raw-span-e2e-pipeline.yml b/e2e-test/trace/src/integrationTest/resources/raw-span-e2e-pipeline.yml\nindex 2f036a7208..35d8d060d2 100644\n--- a/e2e-test/trace/src/integrationTest/resources/raw-span-e2e-pipeline.yml\n+++ b/e2e-test/trace/src/integrationTest/resources/raw-span-e2e-pipeline.yml\n@@ -16,10 +16,12 @@ raw-pipeline:\n         hosts: [ \"https://node-0.example.com:9200\" ]\n         username: \"admin\"\n         password: \"admin\"\n+        insecure: true\n   sink:\n     - opensearch:\n         hosts: [ \"https://node-0.example.com:9200\" ]\n         username: \"admin\"\n         password: \"admin\"\n+        insecure: true\n         index_type: trace-analytics-raw\n         flush_timeout: 5000\ndiff --git a/e2e-test/trace/src/integrationTest/resources/service-map-e2e-pipeline.yml b/e2e-test/trace/src/integrationTest/resources/service-map-e2e-pipeline.yml\nindex 5d934e0e95..326ee02cc8 100644\n--- a/e2e-test/trace/src/integrationTest/resources/service-map-e2e-pipeline.yml\n+++ b/e2e-test/trace/src/integrationTest/resources/service-map-e2e-pipeline.yml\n@@ -18,5 +18,6 @@ service-map-pipeline:\n         hosts: [\"https://node-0.example.com:9200\"]\n         username: \"admin\"\n         password: \"admin\"\n+        insecure: true\n         index_type: trace-analytics-service-map\n         flush_timeout: 5000\n"
  },
  "CWE-772": {
    "cve": "CVE-2024-52303",
    "commit_url": "https://github.com/aio-libs/aiohttp/commit/bc15db61615079d1b6327ba42c682f758fa96936",
    "diff": "diff --git a/CHANGES/9852.bugfix.rst b/CHANGES/9852.bugfix.rst\nnew file mode 100644\nindex 00000000000..b459d08478b\n--- /dev/null\n+++ b/CHANGES/9852.bugfix.rst\n@@ -0,0 +1 @@\n+Fixed system routes polluting the middleware cache -- by :user:`bdraco`.\ndiff --git a/aiohttp/web_app.py b/aiohttp/web_app.py\nindex 78b1a67bacc..81a84833532 100644\n--- a/aiohttp/web_app.py\n+++ b/aiohttp/web_app.py\n@@ -54,6 +54,7 @@\n     MaskDomain,\n     MatchedSubAppResource,\n     PrefixedSubAppResource,\n+    SystemRoute,\n     UrlDispatcher,\n )\n \n@@ -79,7 +80,6 @@\n _Resource = TypeVar(\"_Resource\", bound=AbstractResource)\n \n \n-@lru_cache(None)\n def _build_middlewares(\n     handler: Handler, apps: Tuple[\"Application\", ...]\n ) -> Callable[[Request], Awaitable[StreamResponse]]:\n@@ -90,6 +90,9 @@ def _build_middlewares(\n     return handler\n \n \n+_cached_build_middleware = lru_cache(maxsize=1024)(_build_middlewares)\n+\n+\n class Application(MutableMapping[Union[str, AppKey[Any]], Any]):\n     ATTRS = frozenset(\n         [\n@@ -544,8 +547,13 @@ async def _handle(self, request: Request) -> StreamResponse:\n         handler = match_info.handler\n \n         if self._run_middlewares:\n-            if not self._has_legacy_middlewares:\n-                handler = _build_middlewares(handler, match_info.apps)\n+            # If its a SystemRoute, don't cache building the middlewares since\n+            # they are constructed for every MatchInfoError as a new handler\n+            # is made each time.\n+            if not self._has_legacy_middlewares and not isinstance(\n+                match_info.route, SystemRoute\n+            ):\n+                handler = _cached_build_middleware(handler, match_info.apps)\n             else:\n                 for app in match_info.apps[::-1]:\n                     for m, new_style in app._middlewares_handlers:  # type: ignore[union-attr]\ndiff --git a/tests/test_web_middleware.py b/tests/test_web_middleware.py\nindex 9c4462be409..13acc589da9 100644\n--- a/tests/test_web_middleware.py\n+++ b/tests/test_web_middleware.py\n@@ -1,10 +1,11 @@\n import re\n-from typing import Any\n+from typing import Any, NoReturn\n \n import pytest\n from yarl import URL\n \n-from aiohttp import web\n+from aiohttp import web, web_app\n+from aiohttp.pytest_plugin import AiohttpClient\n from aiohttp.typedefs import Handler\n \n \n@@ -520,3 +521,27 @@ async def call(self, request, handler: Handler):\n     assert 201 == resp.status\n     txt = await resp.text()\n     assert \"OK[new style middleware]\" == txt\n+\n+\n+async def test_middleware_does_not_leak(aiohttp_client: AiohttpClient) -> None:\n+    async def any_handler(request: web.Request) -> NoReturn:\n+        assert False\n+\n+    class Middleware:\n+        @web.middleware\n+        async def call(\n+            self, request: web.Request, handler: Handler\n+        ) -> web.StreamResponse:\n+            return await handler(request)\n+\n+    app = web.Application()\n+    app.router.add_route(\"POST\", \"/any\", any_handler)\n+    app.middlewares.append(Middleware().call)\n+\n+    client = await aiohttp_client(app)\n+\n+    web_app._cached_build_middleware.cache_clear()\n+    for _ in range(10):\n+        resp = await client.get(\"/any\")\n+        assert resp.status == 405\n+    assert web_app._cached_build_middleware.cache_info().currsize < 10\n"
  },
  "CWE-288": {
    "cve": "CVE-2024-31463",
    "commit_url": "https://github.com/metal3-io/ironic-image/commit/48e40bd30d49aefabac6fc80204a8650b13d10b4",
    "diff": "diff --git a/scripts/ironic-common.sh b/scripts/ironic-common.sh\nindex ee7e420f7..f157dc3ec 100644\n--- a/scripts/ironic-common.sh\n+++ b/scripts/ironic-common.sh\n@@ -93,8 +93,8 @@ run_ironic_dbsync()\n }\n \n # Use the special value \"unix\" for unix sockets\n-export IRONIC_PRIVATE_PORT=${IRONIC_PRIVATE_PORT:-6388}\n-export IRONIC_INSPECTOR_PRIVATE_PORT=${IRONIC_INSPECTOR_PRIVATE_PORT:-5049}\n+export IRONIC_PRIVATE_PORT=${IRONIC_PRIVATE_PORT:-unix}\n+export IRONIC_INSPECTOR_PRIVATE_PORT=${IRONIC_INSPECTOR_PRIVATE_PORT:-unix}\n \n export IRONIC_ACCESS_PORT=${IRONIC_ACCESS_PORT:-6385}\n export IRONIC_LISTEN_PORT=${IRONIC_LISTEN_PORT:-$IRONIC_ACCESS_PORT}\n"
  },
  "CWE-909": {
    "cve": "CVE-2024-53845",
    "commit_url": "https://github.com/espressif/esp-idf/commit/4f85a2726e04b737c8646d865b44ddd837b703db",
    "diff": "diff --git a/components/esp_wifi/lib b/components/esp_wifi/lib\nindex 3c0c961eaa92..225cc5d2dc05 160000\n--- a/components/esp_wifi/lib\n+++ b/components/esp_wifi/lib\n@@ -1 +1 @@\n-Subproject commit 3c0c961eaa925626666400b451028f6512e2bad9\n+Subproject commit 225cc5d2dc0509c4d348343736427cb1679a60d4\ndiff --git a/docs/en/api-reference/network/esp_smartconfig.rst b/docs/en/api-reference/network/esp_smartconfig.rst\nindex 9d95a68fc0fe..3f62b3da0ca4 100644\n--- a/docs/en/api-reference/network/esp_smartconfig.rst\n+++ b/docs/en/api-reference/network/esp_smartconfig.rst\n@@ -1,10 +1,19 @@\n SmartConfig\n ===========\n \n-The SmartConfig\\ :sup:`TM` is a provisioning technology developed by TI to connect a new Wi-Fi device to a Wi-Fi network. It uses a mobile app to broadcast the network credentials from a smartphone, or a tablet, to an un-provisioned Wi-Fi device.\n+:link_to_translation:`zh_CN:[\u4e2d\u6587]`\n+\n+Introduction\n+------------\n+\n+The SmartConfig\\ :sup:`TM` is a provisioning technology developed by TI to connect a new Wi-Fi device to a Wi-Fi network. It uses a mobile application to broadcast the network credentials from a smartphone, or a tablet, to an un-provisioned Wi-Fi device.\n \n The advantage of this technology is that the device does not need to directly know SSID or password of an Access Point (AP). This information is provided using the smartphone. This is particularly important to headless device and systems, due to their lack of a user interface.\n \n+Currently, {IDF_TARGET_NAME} support three types of SmartConfig: Airkiss, ESPTouch, and ESPTouch v2. ESPTouch v2 has been supported since SmartConfig v3.0 (the version of SmartConfig can be get from :cpp:func:`esp_smartconfig_get_version()`), and it employs a completely different algorithm compared to ESPTouch, resulting in faster setup times. Additionally, ESPTouch v2 introduces AES encryption and custom data fields.\n+\n+Starting from SmartConfig v3.0.2, ESPTouch v2 introduces support for random IV in AES encryption. On the application side, when the option for random IV is disabled, the default IV is set to 0, maintaining consistency with previous versions. When the random IV option is enabled, the IV will be a random value. It is important to note that when AES encryption is enabled with a random IV, the provision time will be extended due to the need of transmitting the IV to the provisioning device. On the provisioning device side, the device will identify whether the random IV for AES is enabled based on the flag in the provisioning packet.\n+\n If you are looking for other options to provision your {IDF_TARGET_NAME} devices, check :doc:`../provisioning/index`.\n \n \ndiff --git a/docs/zh_CN/api-reference/network/esp_smartconfig.rst b/docs/zh_CN/api-reference/network/esp_smartconfig.rst\nindex 00a8e6f6603d..4acfe91f2eb3 100644\n--- a/docs/zh_CN/api-reference/network/esp_smartconfig.rst\n+++ b/docs/zh_CN/api-reference/network/esp_smartconfig.rst\n@@ -1 +1,28 @@\n-.. include:: ../../../en/api-reference/network/esp_smartconfig.rst\n+SmartConfig\n+\n+:link_to_translation:`en:[English]`\n+\n+\u6982\u8ff0\n+-----\n+\n+SmartConfig\\ :sup:`TM` \u662f\u7531 TI \u5f00\u53d1\u7684\u914d\u7f51\u6280\u672f\uff0c\u7528\u4e8e\u5c06\u65b0\u7684 Wi-Fi \u8bbe\u5907\u8fde\u63a5\u5230 Wi-Fi \u7f51\u7edc\u3002\u5b83\u4f7f\u7528\u79fb\u52a8\u5e94\u7528\u7a0b\u5e8f\u5c06\u65e0\u7ebf\u7f51\u51ed\u636e\u4ece\u667a\u80fd\u624b\u673a\u6216\u5e73\u677f\u7535\u8111\u7aef\u5e7f\u64ad\u7ed9\u672a\u914d\u7f51\u7684 Wi-Fi \u8bbe\u5907\u3002\n+\n+\u8fd9\u9879\u6280\u672f\u7684\u4f18\u52bf\u5728\u4e8e\uff0c\u8bbe\u5907\u65e0\u9700\u76f4\u63a5\u83b7\u77e5 AP \u7684 SSID \u6216\u5bc6\u7801\uff0c\u800c\u662f\u901a\u8fc7\u667a\u80fd\u624b\u673a\u83b7\u53d6\u3002\u8fd9\u5bf9\u4e8e\u6ca1\u6709\u7528\u6237\u754c\u9762\u7684\u65e0\u5934\u8bbe\u5907\u548c\u7cfb\u7edf\u800c\u8a00\u5341\u5206\u91cd\u8981\u3002\n+\n+\u76ee\u524d\uff0c {IDF_TARGET_NAME} \u652f\u6301\u4e09\u79cd\u7c7b\u578b\u7684 SmartConfig \u914d\u7f51\uff1a Airkiss\u3001ESPTouch \u548c ESPTouch v2\u3002ESPTouch v2 \u81ea SmartConfig v3.0 \uff08SmartConfig \u7684\u7248\u672c\u53ef\u4ee5\u4ece :cpp:func:`esp_smartconfig_get_version()` \u83b7\u53d6\uff09\u8d77\u5f00\u59cb\u652f\u6301\uff0cESPTouch v2 \u548c vESPTouch \u91c7\u7528\u5b8c\u5168\u4e0d\u540c\u7684\u914d\u7f51\u7b97\u6cd5\uff0c\u56e0\u6b64\u914d\u7f51\u901f\u5ea6\u66f4\u5feb\u3002\u6b64\u5916\uff0cESPTouch v2 \u8fd8\u589e\u52a0\u4e86 AES \u52a0\u5bc6\u529f\u80fd\u548c\u81ea\u5b9a\u4e49\u6570\u636e\u5b57\u6bb5\u3002\n+\n+\u4ece SmartConfig v3.0.2 \u5f00\u59cb\uff0cESPTouch v2 \u7684 AES \u52a0\u5bc6\u652f\u6301\u968f\u673a IV\u3002\u5728\u5e94\u7528\u7a0b\u5e8f\u7aef\uff0c\u5f53\u968f\u673a IV \u7684\u9009\u9879\u5173\u95ed\u7684\u65f6\u5019\uff0c\u9ed8\u8ba4\u7684 IV \u4e3a 0\uff0c\u4e0e\u65e7\u7248\u672c\u4fdd\u6301\u4e00\u81f4\uff0c\u5f53\u968f\u673a IV \u7684\u9009\u9879\u6253\u5f00\u7684\u65f6\u5019\uff0cIV \u4e3a\u968f\u673a\u503c\u3002\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5f53\u542f\u7528 AES \u52a0\u5bc6\u4e14 IV \u4e3a\u968f\u673a\u503c\u65f6\uff0c\u914d\u7f51\u65f6\u95f4\u4f1a\u5ef6\u957f\uff0c\u56e0\u4e3a\u9700\u8981\u5c06 IV \u4f20\u8f93\u5230\u914d\u7f51\u8bbe\u5907\u3002\u5728\u914d\u7f51\u8bbe\u5907\u7aef\uff0c\u8bbe\u5907\u4f1a\u6839\u636e\u914d\u7f51\u5305\u4e2d\u7684 flag \u6765\u8bc6\u522b AES \u7684\u968f\u673a IV \u662f\u5426\u5f00\u542f\u3002\n+\n+\u5982\u9700\u901a\u8fc7\u5176\u4ed6\u65b9\u5f0f\u4e3a {IDF_TARGET_NAME} \u8bbe\u5907\u914d\u7f51\uff0c\u8bf7\u53c2\u9605 :doc:`../provisioning/index`\u3002\n+\n+\n+\u5e94\u7528\u793a\u4f8b\n+------------\n+\n+\u524d\u5f80 :example:`wifi/smart_config`\uff0c\u67e5\u770b\u4f7f\u7528 SmartConfig \u5c06 {IDF_TARGET_NAME} \u8fde\u63a5\u5230\u76ee\u6807 AP \u7684\u5e94\u7528\u793a\u4f8b\u3002\n+\n+\n+API \u53c2\u8003\n+----------\n+\n+.. include-build-file:: inc/esp_smartconfig.inc\n"
  },
  "CWE-924": {
    "cve": "CVE-2024-52288",
    "commit_url": "https://github.com/goToMain/libosdp/commit/298576d9214b48214092eebdd892ec77be085e5a",
    "diff": "diff --git a/src/osdp_cp.c b/src/osdp_cp.c\nindex f7a6ec0c..faf8d903 100644\n--- a/src/osdp_cp.c\n+++ b/src/osdp_cp.c\n@@ -639,6 +639,10 @@ static int cp_decode_response(struct osdp_pd *pd, uint8_t *buf, int len)\n \t\tret = osdp_file_cmd_stat_decode(pd, buf + pos, len);\n \t\tbreak;\n \tcase REPLY_CCRYPT:\n+\t\tif (sc_is_active(pd) || pd->cmd_id != CMD_CHLNG) {\n+\t\t\tLOG_EM(\"Out of order REPLY_CCRYPT; has PD gone rogue?\");\n+\t\t\tbreak;\n+\t\t}\n \t\tif (len != REPLY_CCRYPT_DATA_LEN) {\n \t\t\tbreak;\n \t\t}\n@@ -654,6 +658,10 @@ static int cp_decode_response(struct osdp_pd *pd, uint8_t *buf, int len)\n \t\tret = OSDP_CP_ERR_NONE;\n \t\tbreak;\n \tcase REPLY_RMAC_I:\n+\t\tif (sc_is_active(pd) || pd->cmd_id != CMD_SCRYPT) {\n+\t\t\tLOG_EM(\"Out of order REPLY_RMAC_I; has PD gone rogue?\");\n+\t\t\tbreak;\n+\t\t}\n \t\tif (len != REPLY_RMAC_I_DATA_LEN) {\n \t\t\tbreak;\n \t\t}\ndiff --git a/src/osdp_pd.c b/src/osdp_pd.c\nindex 779a2906..b1e31cb4 100644\n--- a/src/osdp_pd.c\n+++ b/src/osdp_pd.c\n@@ -621,7 +621,7 @@ static int pd_decode_command(struct osdp_pd *pd, uint8_t *buf, int len)\n \t\tif (sc_is_active(pd)) {\n \t\t\tpd->reply_id = REPLY_NAK;\n \t\t\tpd->ephemeral_data[0] = OSDP_PD_NAK_SC_COND;\n-\t\t\tLOG_WRN(\"Out of order CMD_SCRYPT; has CP gone rogue?\");\n+\t\t\tLOG_EM(\"Out of order CMD_SCRYPT; has CP gone rogue?\");\n \t\t\tbreak;\n \t\t}\n \t\tmemcpy(pd->sc.cp_cryptogram, buf + pos, CMD_SCRYPT_DATA_LEN);\n"
  },
  "CWE-212": {
    "cve": "CVE-2025-53886",
    "commit_url": "https://github.com/directus/directus/commit/22be460c76957708d67fdd52846a9ad1cbb083fb",
    "diff": "diff --git a/.changeset/seven-flies-exist.md b/.changeset/seven-flies-exist.md\nnew file mode 100644\nindex 0000000000000..29509aed32b2a\n--- /dev/null\n+++ b/.changeset/seven-flies-exist.md\n@@ -0,0 +1,5 @@\n+---\n+'@directus/api': major\n+---\n+\n+Fixed manual flows to only trigger with appropriate permissions\ndiff --git a/api/src/flows.ts b/api/src/flows.ts\nindex 791608cecbe6f..2ae7513294c42 100644\n--- a/api/src/flows.ts\n+++ b/api/src/flows.ts\n@@ -12,6 +12,8 @@ import { useBus } from './bus/index.js';\n import getDatabase from './database/index.js';\n import emitter from './emitter.js';\n import { useLogger } from './logger/index.js';\n+import { fetchPermissions } from './permissions/lib/fetch-permissions.js';\n+import { fetchPolicies } from './permissions/lib/fetch-policies.js';\n import { ActivityService } from './services/activity.js';\n import { FlowsService } from './services/flows.js';\n import * as services from './services/index.js';\n@@ -19,6 +21,7 @@ import { RevisionsService } from './services/revisions.js';\n import type { EventHandler } from './types/index.js';\n import { constructFlowTree } from './utils/construct-flow-tree.js';\n import { getSchema } from './utils/get-schema.js';\n+import { getService } from './utils/get-service.js';\n import { JobQueue } from './utils/job-queue.js';\n import { redactObject } from './utils/redact-object.js';\n import { scheduleSynchronizedJob, validateCron } from './utils/schedule.js';\n@@ -120,7 +123,7 @@ class FlowManager {\n \tpublic async runWebhookFlow(\n \t\tid: string,\n \t\tdata: unknown,\n-\t\tcontext: Record<string, unknown>,\n+\t\tcontext: { schema: SchemaOverview; accountability: Accountability | undefined } & Record<string, unknown>,\n \t): Promise<{ result: unknown; cacheEnabled?: boolean }> {\n \t\tconst logger = useLogger();\n \n@@ -248,7 +251,9 @@ class FlowManager {\n \t\t\t} else if (flow.trigger === 'manual') {\n \t\t\t\tconst handler = async (data: unknown, context: Record<string, unknown>) => {\n \t\t\t\t\tconst enabledCollections = flow.options?.['collections'] ?? [];\n+\t\t\t\t\tconst requireSelection = flow.options?.['requireSelection'] ?? true;\n \t\t\t\t\tconst targetCollection = (data as Record<string, any>)?.['body'].collection;\n+\t\t\t\t\tconst targetKeys = (data as Record<string, any>)?.['body'].keys;\n \n \t\t\t\t\tif (!targetCollection) {\n \t\t\t\t\t\tlogger.warn(`Manual trigger requires \"collection\" to be specified in the payload`);\n@@ -265,6 +270,63 @@ class FlowManager {\n \t\t\t\t\t\tthrow new ForbiddenError();\n \t\t\t\t\t}\n \n+\t\t\t\t\tif (!targetKeys || !Array.isArray(targetKeys)) {\n+\t\t\t\t\t\tlogger.warn(`Manual trigger requires \"keys\" to be specified in the payload`);\n+\t\t\t\t\t\tthrow new ForbiddenError();\n+\t\t\t\t\t}\n+\n+\t\t\t\t\tif (requireSelection && targetKeys.length === 0) {\n+\t\t\t\t\t\tlogger.warn(`Manual trigger requires at least one key to be specified in the payload`);\n+\t\t\t\t\t\tthrow new ForbiddenError();\n+\t\t\t\t\t}\n+\n+\t\t\t\t\tconst accountability = context?.['accountability'] as Accountability | undefined;\n+\n+\t\t\t\t\tif (!accountability) {\n+\t\t\t\t\t\tlogger.warn(`Manual flows are only triggerable when authenticated`);\n+\t\t\t\t\t\tthrow new ForbiddenError();\n+\t\t\t\t\t}\n+\n+\t\t\t\t\tif (accountability.admin === false) {\n+\t\t\t\t\t\tconst database = (context['database'] as Knex) ?? getDatabase();\n+\t\t\t\t\t\tconst schema = (context['schema'] as SchemaOverview) ?? (await getSchema({ database }));\n+\n+\t\t\t\t\t\tconst policies = await fetchPolicies(accountability, { schema, knex: database });\n+\n+\t\t\t\t\t\tconst permissions = await fetchPermissions(\n+\t\t\t\t\t\t\t{\n+\t\t\t\t\t\t\t\tpolicies,\n+\t\t\t\t\t\t\t\taccountability,\n+\t\t\t\t\t\t\t\taction: 'read',\n+\t\t\t\t\t\t\t\tcollections: [targetCollection],\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t\t{ schema, knex: database },\n+\t\t\t\t\t\t);\n+\n+\t\t\t\t\t\tif (permissions.length === 0) {\n+\t\t\t\t\t\t\tlogger.warn(`Triggering ${targetCollection} is not allowed`);\n+\t\t\t\t\t\t\tthrow new ForbiddenError();\n+\t\t\t\t\t\t}\n+\n+\t\t\t\t\t\tconst service = getService(targetCollection, { schema, accountability, knex: database });\n+\t\t\t\t\t\tconst primaryField = schema.collections[targetCollection]!.primary;\n+\n+\t\t\t\t\t\tlet keys = await service.readMany(\n+\t\t\t\t\t\t\ttargetKeys,\n+\t\t\t\t\t\t\t{ fields: [primaryField] },\n+\t\t\t\t\t\t\t{\n+\t\t\t\t\t\t\t\temitEvents: false,\n+\t\t\t\t\t\t\t},\n+\t\t\t\t\t\t);\n+\n+\t\t\t\t\t\tkeys = keys.map((key) => key[primaryField]);\n+\n+\t\t\t\t\t\tif (targetKeys.some((key) => !keys.includes(key))) {\n+\t\t\t\t\t\t\tlogger.warn(`Triggering keys ${targetKeys} is not allowed`);\n+\t\t\t\t\t\t\tthrow new ForbiddenError();\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\n \t\t\t\t\tif (flow.options['async']) {\n \t\t\t\t\t\tthis.executeFlow(flow, data, context);\n \t\t\t\t\t\treturn { result: undefined };\n"
  },
  "CWE-189": {
    "cve": "CVE-2025-9688",
    "commit_url": "https://github.com/mupen64plus/mupen64plus-core/commit/3984137fc0c44110f1ef876adb008885b05a6e18",
    "diff": "diff --git a/src/device/cart/is_viewer.c b/src/device/cart/is_viewer.c\nindex d3522401d..c8f442e65 100644\n--- a/src/device/cart/is_viewer.c\n+++ b/src/device/cart/is_viewer.c\n@@ -55,13 +55,15 @@ void write_is_viewer(void* opaque, uint32_t address, uint32_t value, uint32_t ma\n     {\n         if (word > 0)\n         {\n-            /* make sure we don't overflow the buffer */\n-            if (is_viewer->buffer_pos + word > IS_BUFFER_SIZE)\n+            /* make sure we don't overflow the integer or the buffer  */\n+            if (is_viewer->buffer_pos > IS_BUFFER_SIZE \n+                || word > IS_BUFFER_SIZE\n+                || is_viewer->buffer_pos + word > IS_BUFFER_SIZE )\n             {\n                 /* reset buffer */\n                 memset(is_viewer->output_buffer, 0, IS_BUFFER_SIZE);\n                 is_viewer->buffer_pos = 0;\n-                DebugMessage(M64MSG_WARNING, \"IS64: prevented buffer overflow, cleared buffer\");\n+                DebugMessage(M64MSG_WARNING, \"IS64: prevented integer overflow, cleared buffer\");\n                 return;\n             }\n \n"
  }
}